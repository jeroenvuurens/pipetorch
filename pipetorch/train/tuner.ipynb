{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tuner.py\n",
    "from __future__ import print_function, with_statement, division\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "#from .train_metrics import loss\n",
    "from .helper import *\n",
    "from math import log, exp\n",
    "import statistics\n",
    "from functools import partial\n",
    "\n",
    "def frange(start, end, steps):\n",
    "    incr = (end - start) / (steps)\n",
    "    return (start + x * incr for x in range(steps))\n",
    "\n",
    "def exprange(start, end, steps, **kwargs):\n",
    "    return (exp(x) for x in frange(log(start), log(end), steps))\n",
    "\n",
    "def arange(start, end, steps, **kwargs):\n",
    "    return np.arange(start, end, steps)\n",
    "\n",
    "def set_dropouts(dropouts):\n",
    "    def change(value):\n",
    "        for d in dropouts:\n",
    "            d.p = value\n",
    "    return change\n",
    "\n",
    "class tuner:\n",
    "    def __init__(self, trainer, lrvalues, lrupdate=None, xlabel='parameter', smooth=0.05, diverge=10, max_validation_mem=None, **kwargs):\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "        self.xlabel = xlabel\n",
    "        self.trainer = trainer\n",
    "        self.lrvalues = list(lrvalues)\n",
    "        self.lrupdate = lrupdate if lrupdate else trainer.set_lr\n",
    "        self.smooth = smooth\n",
    "        self.diverge = diverge\n",
    "        self.max_validation_mem = max_validation_mem\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.trainer.commit('tuner')\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.trainer.revert('tuner')\n",
    "\n",
    "    def next_train(self):\n",
    "        try:\n",
    "            return next(self.train_Xy)\n",
    "        except (StopIteration, AttributeError):\n",
    "            self.train_iterator = iter(self.trainer.train_Xy)\n",
    "            return next(self.train_iterator)\n",
    "\n",
    "    def run( self, cache_valid=True ):\n",
    "        graphx = []\n",
    "        sloss = []\n",
    "        validation_set = []\n",
    "        mem_validation = 0\n",
    "        self.trainer.model\n",
    "        \n",
    "        if cache_valid:\n",
    "            for batch in self.trainer.valid_Xy:\n",
    "                validation_set.append(batch)\n",
    "                mem_validation += sum([sys.getsizeof(x.storage()) for x in batch])\n",
    "                #print(mem_validation)\n",
    "                if self.max_validation_mem and mem_validation > self.max_validation_mem:\n",
    "                    print('warning: validation set is too large for memory')\n",
    "                    break\n",
    "        else:\n",
    "            validation_set = self.trainer.valid_Xy\n",
    "        with plt_notebook():\n",
    "            with Plot(xscale='log', xlabel=self.xlabel) as p:\n",
    "                with self.trainer.train_mode:\n",
    "                    for i, lr in enumerate(tqdm(self.lrvalues, leave=False)):\n",
    "                        graphx.append(lr)\n",
    "                        self.lrupdate(lr)\n",
    "                        *X, y = self.next_train()\n",
    "                        loss, pred_y = self.trainer.train_batch(*X, y=y)\n",
    "                        loss = self.trainer.validate_loss(validation_set)\n",
    "                        try:\n",
    "                            loss = self.smooth * loss + (1 - self.smooth) * sloss[-1]\n",
    "                        except: pass\n",
    "                        sloss.append(loss)\n",
    "\n",
    "                        try:\n",
    "                            if i > len(self.lrvalues) / 4 and loss > self.diverge * min_loss:\n",
    "                                #print(\"Stopping early, the loss has diverged\")\n",
    "                                break\n",
    "                            min_loss = min(min_loss, loss)\n",
    "                        except:\n",
    "                            min_loss = loss\n",
    "                        p.replot( graphx, sloss )\n",
    "\n",
    "    def run_multi( self, param2_values, param2_update ):\n",
    "        param2_values = list(param2_values)\n",
    "        for p in param2_values:\n",
    "            param2_update(p)\n",
    "            self.trainer.commit(f'param2_{p:.2E}')\n",
    "        x = []\n",
    "        sloss = { f'{p:.2E}':[] for p in param2_values }\n",
    "        with plt_notebook():\n",
    "            with Plot(xscale='log', xlabel=self.xlabel) as plot:\n",
    "\n",
    "                dropped_param2_values = []\n",
    "                for lr in tqdm(self.lrvalues, leave=False):\n",
    "                    with self.trainer.train_mode:\n",
    "                        x.append(lr)\n",
    "                        *X, y = self.next_train()\n",
    "                        for p in param2_values:\n",
    "                            self.trainer.checkout(f'param2_{p:.2E}')\n",
    "                            param2_update(p)\n",
    "                            self.lrupdate(lr)\n",
    "                            loss, pred_y = self.trainer.train_batch(*X, y=y)\n",
    "                            loss = self.trainer.validate_loss()\n",
    "                            try:\n",
    "                                loss = smooth * loss + (1 - smooth) * sloss[f'{p:.2E}'][-1]\n",
    "                            except: pass\n",
    "                            sloss[f'{p:.2E}'].append(loss)\n",
    "                            #print(self.trainer.optimizer.param_groups[0]['weight_decay'])\n",
    "                            #print(f'param2_{p:.2E} {loss}')\n",
    "\n",
    "                            try:\n",
    "                                if loss > diverge * min_loss:\n",
    "                                    dropped_param2_values.append(p)\n",
    "                                min_loss = min(min_loss, loss)\n",
    "                            except:\n",
    "                                min_loss = loss\n",
    "                        for p in param2_values:\n",
    "                            self.trainer.commit(f'param2_{p:.2E}')\n",
    "                        plot.multiplot( x, sloss )\n",
    "\n",
    "        for p in param2_values:\n",
    "            self.trainer.remove_checkpoint(f'param2_{p:.2E}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
