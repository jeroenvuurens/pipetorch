{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer.py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import timeit\n",
    "import sys\n",
    "import copy\n",
    "import inspect\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from ..evaluate.evaluate import Evaluator\n",
    "from torch.optim.lr_scheduler import OneCycleLR, LambdaLR\n",
    "from .tuner import *\n",
    "from functools import partial\n",
    "import os\n",
    "try:\n",
    "    GPU = int(os.environ['GPU'])\n",
    "    GPU = 0\n",
    "except:\n",
    "    GPU = -1\n",
    "    \n",
    "def last_container(last):\n",
    "    try:\n",
    "        l = last_container(last.children())\n",
    "        if l is not None:\n",
    "            return l\n",
    "    except: pass\n",
    "    try:\n",
    "        if len(last._modules) > 0 and next(reversed(last._modules.values())).out_features > 0:\n",
    "            return last\n",
    "    except: pass\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "class DLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def set_last_linear(self, out_features):\n",
    "        container = self.last_container()\n",
    "        name, last = container._modules.popitem()\n",
    "        container.add_module(name, nn.Linear(last.in_features, out_features))\n",
    "\n",
    "    def last_container(self):\n",
    "        return last_container(self)\n",
    "\n",
    "def UniformLR(*args, **kwargs):\n",
    "    class Uniform_Scheduler:\n",
    "        def step(self):\n",
    "            pass\n",
    "    return Uniform_Scheduler()\n",
    "\n",
    "def onecycle(optimizer, lr, steps):\n",
    "    return OneCycleLR(optimizer, lr[1], total_steps=steps)\n",
    "\n",
    "class ordered_dl:\n",
    "    def __init__(self, dl):\n",
    "        self.dl = dl\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.oldsampler = self.dl.batch_sampler.sampler\n",
    "        self.newsampler = torch.utils.data.sampler.SequentialSampler(self.oldsampler.data_source)\n",
    "        self.dl.batch_sampler.sampler = self.newsampler\n",
    "        return self.dl\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        self.dl.batch_sampler.sampler = self.oldsampler\n",
    "        if exc_type is not None:\n",
    "            return False\n",
    "\n",
    "class trainer:\n",
    "    def __init__(self, model, loss, *data, report_frequency=1, report_phases=['train','valid'], metrics = [], optimizer=AdamW, optimizerparams=dict(), out_features=None, random_state=None, cycle_epochs=1.0, scheduler=None, weight_decay=None, momentum=None, device=None, gpu=None, evaluator=None, **kwargs):\n",
    "        self.report_frequency = report_frequency\n",
    "        self.report_phases = report_phases\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.cycle_epochs = cycle_epochs\n",
    "        if gpu is not None:\n",
    "            if gpu == -1:\n",
    "                device = torch.device('cpu')\n",
    "            else:\n",
    "                device = torch.device(f'cuda:{gpu}')\n",
    "        self.device = device\n",
    "        self.set_data(*data)\n",
    "        self._model = model\n",
    "        try:\n",
    "            self.post_forward = model.post_forward\n",
    "        except: pass\n",
    "        if out_features is not None:\n",
    "            self._out_features = out_features\n",
    "        self._optimizerclass = optimizer\n",
    "        self._optimizerparams = optimizerparams\n",
    "        self.scheduler = scheduler\n",
    "        if self.random_state is not None:\n",
    "            torch.backends.cudnn.deterministic=True\n",
    "            torch.manual_seed(self.random_state)\n",
    "        self._commit = {}\n",
    "        self.epochid = 0\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.lowest_score=None\n",
    "        self.highest_score=None\n",
    "        if evaluator is not None:\n",
    "            assert len(metrics) == 0, 'When you assign an evaluator, you cannot assign different metrics to a trainer'\n",
    "            self._evaluator = evaluator\n",
    "            self.metrics = evaluator.metrics\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "\n",
    "    def set_data(self, *data):\n",
    "        assert len(data) > 0, 'You have to specify a data source. Either a databunch or a set of dataloaders'\n",
    "        if len(data) == 1:\n",
    "            db = data[0]\n",
    "            self.data = db\n",
    "        elif len(data) < 4:\n",
    "            try:\n",
    "                _ = iter(data[0])\n",
    "                self.train_dl = data[0]\n",
    "            except TypeError:\n",
    "                raise TypeError('The first data source must be iterable, preferably a DataLoader that provide an X and y')\n",
    "            try:\n",
    "                _ = iter(data[1])\n",
    "                self.valid_dl = data[1]\n",
    "            except TypeError:\n",
    "                raise TypeError('The second data source must be iterable, preferably a DataLoader that provide an X and y')\n",
    "            if len(data) > 2:\n",
    "                try:\n",
    "                    _ = iter(data[2])\n",
    "                    self.test_dl = data[2]\n",
    "                except TypeError:\n",
    "                    raise TypeError('The third data source must be iterable, preferably a DataLoader that provide an X and y')\n",
    "\n",
    "    @property\n",
    "    def evaluator(self):\n",
    "        try:\n",
    "            return self._evaluator\n",
    "        except:\n",
    "            try:\n",
    "                self._evaluator = self.db.to_evaluator( *self.metrics )\n",
    "            except:\n",
    "                self._evaluator = Evaluator(self, *self.metrics)\n",
    "            return self._evaluator\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return 'Trainer( ' + self.model + ')'\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "\n",
    "    def cpu(self):\n",
    "        self.to(torch.device('cpu'))\n",
    "\n",
    "    def gpu(self):\n",
    "        self.to(torch.device('cuda:0'))\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self._metrics\n",
    "    \n",
    "    @metrics.setter\n",
    "    def metrics(self, value):\n",
    "        try:\n",
    "            iter(value)\n",
    "            self._metrics = value\n",
    "        except:\n",
    "            self._metrics = [] if value is None else [value] \n",
    "        \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, db):\n",
    "        assert hasattr(db, 'train_dl'), 'A single data source must be an object with a train_dl property (like a databunch)'\n",
    "        assert hasattr(db, 'valid_dl'), 'A single data source must be an object with a valid_dl property (like a databunch)'\n",
    "        self._data = db\n",
    "        self.train_dl = self.data.train_dl\n",
    "        self.valid_dl = self.data.valid_dl\n",
    "        try:\n",
    "            self.test_dl = self.data.test_dl\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        \"\"\"\n",
    "        return: the learning rate that was set, could be an interval\n",
    "        \"\"\"\n",
    "        return self._lr\n",
    "        \n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        \"\"\"\n",
    "        Sets the learning rate that is used for training. You can either use a single value\n",
    "        for a fixed lr, a tuple with an interval of two values for a linear annealing \n",
    "        scheduler, or a tuple with an interval of two values for a OneCyleLR scheduler.\n",
    "        The allocation of a scheduler can be overruled by setting a scheduler manually.\n",
    "        \n",
    "        If the lr did not change, nothing happens, otherwise a new optimizer is created\n",
    "        when needed.\n",
    "        \"\"\"\n",
    "        if type(lr) is tuple:\n",
    "            lr = tuple(sorted(lr))\n",
    "        elif type(lr) is list:\n",
    "            lr = sorted(lr)\n",
    "        try:\n",
    "            if self.lr == lr:\n",
    "                return\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "        self._lr = lr\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        \"\"\"\n",
    "        sets the learning rate without changing the learning rate settings\n",
    "        the scheduler or optimizer. is used by tuners like find_lr.\n",
    "        \"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    @property\n",
    "    def min_lr(self):\n",
    "        \"\"\"\n",
    "        the learning rate or lowest of an interval of learning rates\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.lr[0]\n",
    "        except:\n",
    "            try:\n",
    "                return self.lr\n",
    "            except:\n",
    "                return 1e-2\n",
    "\n",
    "    @property\n",
    "    def max_lr(self):\n",
    "        \"\"\"\n",
    "        the learning rate or highest of an interval of learning rates\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.lr[1]\n",
    "        except: pass\n",
    "        try:\n",
    "            return self.lr[0]\n",
    "        except: pass\n",
    "        return self.lr\n",
    "\n",
    "    def set_optimizer_param(self, key, value):\n",
    "        if value is not None:\n",
    "            self._optimizerparams[key] = value\n",
    "        else:\n",
    "            try:\n",
    "                del self._optimizerparams[key]\n",
    "            except: pass\n",
    "        try:\n",
    "            del self._optimizer\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def weight_decay(self):\n",
    "        return self.optimizer.param_groups[0]['weight_decay']\n",
    "\n",
    "    @weight_decay.setter\n",
    "    def weight_decay(self, value):\n",
    "        self.set_optimizer_param('weight_decay', value)\n",
    "\n",
    "    @property\n",
    "    def momentum(self):\n",
    "        return self.optimizer.param_groups[0]['betas']\n",
    "\n",
    "    @momentum.setter\n",
    "    def momentum(self, value):\n",
    "        self.set_optimizer_param('betas', value)\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        try:\n",
    "            return self._optimizer\n",
    "        except:\n",
    "            self.set_optimizer_param('lr', self.min_lr)\n",
    "            self._optimizer = self._optimizerclass(self.model.parameters(), **self._optimizerparams)\n",
    "            return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, value):\n",
    "        self._optimizerclass = value\n",
    "        try:\n",
    "            del self._optimizer\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "\n",
    "    def del_optimizer(self):\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        \"\"\"\n",
    "        Returns: scheduler that is used to adapt the learning rate\n",
    "\n",
    "        You can either provide a (partial) function that accepts (optimizer, lr, epochs, batch_steps)\n",
    "        that returns a new scheduler. If you have not specified a function, one of three standard\n",
    "        schedulers is used based on the value of the learning rate. If the learning rate is a\n",
    "        single value, the learning rate is fixed. If the learning rate is an interval of two values\n",
    "        in a list, a linear annealing scheduler is used. If the learning rate is an interval \n",
    "        of two values in a tuple, a OneCyleLR scheduler is used.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._scheduler\n",
    "        except:\n",
    "            try:\n",
    "                #steps = int(round((len(self.train_dl) * self.cycle_epochs)))\n",
    "                if self.schedulertype is None:\n",
    "                    try:\n",
    "                        self.lr[1]\n",
    "                        if type(self.lr) == tuple:\n",
    "                            schedulertype = OneCycleLR\n",
    "                        elif type(self.lr) == list:\n",
    "                            schedulertype = LambdaLR\n",
    "                        else:\n",
    "                            raise NotImplementedError(f'Provide either an single value learning rate for a Uniform scheduler, list [low, high] for a Linear Annealing, or tuple (low, high) for a OneCycleLR scheduler')\n",
    "                    except:\n",
    "                        schedulertype = UniformLR\n",
    "                else:\n",
    "                    schedulertype = self.schedulertype\n",
    "                if schedulertype == LambdaLR:\n",
    "                    if self.cycle_epochs < 2:\n",
    "                        lr = lambda epoch: self.lr[1]\n",
    "                    else:\n",
    "                        lr = lambda epoch: self.lr[1] - (self.lr[1] - self.lr[0]) * epoch / (self.cycle_epochs - 1)\n",
    "                    self._scheduler = LambdaLR(self.optimizer, lr_lambda = lr)\n",
    "                elif schedulertype == OneCycleLR:\n",
    "                    self._scheduler = OneCycleLR(self.optimizer, self.lr[1], epochs=int(self.cycle_epochs), steps_per_epoch=len(self.train_dl))\n",
    "                else:\n",
    "                    self._scheduler = schedulertype(self.optimizer, self.lr, self.cycle_epochs, len(self.train_dl))\n",
    "            except:\n",
    "                raise NotImplementedError(f'The provided function does not work with (optim, {self.lr}, {self.cycle_epochs}, {len(self.train_dl)}) to instantiate a scheduler')\n",
    "            return self._scheduler\n",
    "    @scheduler.setter\n",
    "    def scheduler(self, value):\n",
    "        try:\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "        self.schedulertype = value\n",
    "\n",
    "    @property\n",
    "    def out_features(self):\n",
    "        try:\n",
    "            return self._out_features\n",
    "        except: pass\n",
    "        try:\n",
    "            self._out_features = last_container(self.model).out_features\n",
    "            return self._out_features\n",
    "        except:\n",
    "            print('cannot infer out_features from the model, please specify it in the constructor of the trainer')\n",
    "            raise\n",
    "\n",
    "    @property\n",
    "    def in_features(self):\n",
    "        first = next(iter(self._model.modules()))\n",
    "        while type(first) is nn.Sequential:\n",
    "            first = next(iter(first.modules()))\n",
    "        return first.in_features\n",
    "    \n",
    "    @property\n",
    "    def valid_ds(self):\n",
    "        return self.valid_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def train_ds(self):\n",
    "        return self.train_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def test_ds(self):\n",
    "        return self.test_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def train_Xy(self):\n",
    "        for batch in self.train_dl:\n",
    "            yield [ t.to(self.model.device) for t in batch ]\n",
    "    \n",
    "    @property\n",
    "    def valid_Xy(self):\n",
    "        for batch in self.valid_dl:\n",
    "            yield [ t.to(self.model.device) for t in batch ]\n",
    "    \n",
    "    @property\n",
    "    def test_Xy(self):\n",
    "        for batch in self.test_dl:\n",
    "            yield [ t.to(self.model.device) for t in batch ]\n",
    "    \n",
    "    @property\n",
    "    def valid_tensors(self):\n",
    "        return self.valid_dl.dataset.tensors\n",
    "\n",
    "    @property\n",
    "    def train_tensors(self):\n",
    "        return self.train_dl.dataset.tensors\n",
    "\n",
    "    @property\n",
    "    def test_tensors(self):\n",
    "        return self.test_dl.dataset.tensors\n",
    "\n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.train_tensors[0]\n",
    "\n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train_tensors[-1]\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.valid_tensors[0]\n",
    "\n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid_tensors[-1]\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test_tensors[0]\n",
    "\n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test_tensors[-1]\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        try:\n",
    "            if self.device is not self._model.device:\n",
    "                self._model.device = self.device\n",
    "                self._model.to(self.device)\n",
    "                try:\n",
    "                    del self._optimizer\n",
    "                except: pass\n",
    "        except:\n",
    "            try:\n",
    "                self._model.device = self.device\n",
    "                self._model.to(self.device)\n",
    "                #print('change device')\n",
    "                try:\n",
    "                    del self._optimizer\n",
    "                except: pass\n",
    "            except: pass\n",
    "        return self._model\n",
    "\n",
    "    def parameters(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data)\n",
    "\n",
    "    def predict(self, *X):\n",
    "        with self.eval_mode:\n",
    "            X = [ x.to(self.model.device) for x in X ]\n",
    "            return self.post_forward(self.model(*X))\n",
    "\n",
    "    def predict_numpy(self, *X):\n",
    "        \"\"\"\n",
    "        This will only work when instantiated with a PipeTorch Databunch.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.data\n",
    "        \n",
    "        with self.eval_mode:\n",
    "            X = [ x.to(self.model.device) for x in X ]\n",
    "            return self.post_forward(self.model(*X))\n",
    "\n",
    "\n",
    "    def post_forward(self, y):\n",
    "        return y\n",
    "\n",
    "    def list_commits(self):\n",
    "        return self._commit.keys()\n",
    "\n",
    "    def commit(self, label):\n",
    "        \"save the model and optimzer state, allowing to revert to a previous state\"\n",
    "        model_state = copy.deepcopy(self.model.state_dict())\n",
    "        optimizer_state = copy.deepcopy(self.optimizer.state_dict())\n",
    "        self._commit[label] = (model_state, optimizer_state, self._optimizerparams)\n",
    "\n",
    "    def revert(self, label):\n",
    "        \"revert the model and optimizer to a previously commited state, deletes the commit point\"\n",
    "        if label in self._commit:\n",
    "            model_state, optimizer_state, self._optimizerparams = self._commit.pop(label)\n",
    "            self.model.load_state_dict(model_state)\n",
    "            self.del_optimizer()            \n",
    "            self.optimizer.load_state_dict(optimizer_state)\n",
    "        else:\n",
    "            print('commit point {label} not found')\n",
    "    \n",
    "    def checkout(self, label):\n",
    "        \"switches the model and optimizer to a previously commited state, keeps the commit point\"\n",
    "        if label in self._commit:\n",
    "            model_state, optimizer_state, self._optimizerparams = self._commit[label]\n",
    "            self.model.load_state_dict(model_state)\n",
    "            self.del_optimizer()            \n",
    "            self.optimizer.load_state_dict(optimizer_state)  \n",
    "        else:\n",
    "            print('commit point {label} not found')\n",
    "\n",
    "    def remove_checkpoint(self, label):\n",
    "        self._commit.pop(label)\n",
    "\n",
    "    def purge(self, label):\n",
    "        \"switches the model and optimizer to a previously commited state, keeps only the commit point\"\n",
    "        if label in self._commit:\n",
    "            self.checkout(label)\n",
    "            self._commit = { l:s for l, s in self._commit.items() if l == label }\n",
    "        else:\n",
    "            print(f'commit point {label} not found')\n",
    "\n",
    "    def validate_loss(self, dl=None):\n",
    "        if not dl:\n",
    "            dl = self.valid_Xy\n",
    "        with self.eval_mode:\n",
    "            losses = []\n",
    "            for *X, y in dl:\n",
    "                losses.append((self.loss_xy(*X, y=y)[0].item() * len(y), len(y)))\n",
    "            sums = [ sum(x) for x in zip(*losses) ]\n",
    "            return sums[0] / sums[1]\n",
    "\n",
    "    @property\n",
    "    def eval_mode(self):\n",
    "        class CM(object):\n",
    "            def __init__(self, trainer):\n",
    "                self.trainer = trainer\n",
    "            def __enter__(self):\n",
    "                self.trainer.model.eval()\n",
    "                self.prev = torch.is_grad_enabled()\n",
    "                torch.set_grad_enabled(False)\n",
    "                return self.trainer.model\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                torch.set_grad_enabled(self.prev)\n",
    "                self.trainer.model.train()\n",
    "        return CM(self)\n",
    "\n",
    "    @property\n",
    "    def train_mode(self):\n",
    "        class CM(object):\n",
    "            def __init__(self, trainer):\n",
    "                self.trainer = trainer\n",
    "            def __enter__(self):\n",
    "                self.trainer.model.train()\n",
    "                self.prev = torch.is_grad_enabled()\n",
    "                torch.set_grad_enabled(True)\n",
    "                return self.trainer.model\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                torch.set_grad_enabled(self.prev)\n",
    "                self.trainer.model.eval()\n",
    "        return CM(self)\n",
    "\n",
    "    def validate(self, pbar=None, log={}):\n",
    "        epochloss = 0\n",
    "        n = 0\n",
    "        epoch_y_pred = []\n",
    "        epoch_y = []\n",
    "\n",
    "        with self.eval_mode:\n",
    "            for *X, y in self.valid_Xy:\n",
    "                loss, y_pred = self.loss_xy(*X, y=y)\n",
    "                epochloss += loss.item() * len(y_pred)\n",
    "                n += len(y_pred)\n",
    "                epoch_y_pred.append(to_numpy(y_pred))\n",
    "                epoch_y.append(to_numpy(y))\n",
    "                if pbar is not None:\n",
    "                    pbar.update(self.valid_dl.batch_size)\n",
    "            epochloss /= n\n",
    "            epoch_y = np.concatenate(epoch_y, axis=0)\n",
    "            epoch_y_pred = np.concatenate(epoch_y_pred, axis=0)\n",
    "            self.evaluator._store(epoch_y, epoch_y_pred, loss=epochloss, phase='valid', epoch=self.epochid, **log)\n",
    "        return epochloss\n",
    "            \n",
    "    def loss_xy(self, *X, y=None):\n",
    "        assert y is not None, 'Call loss_xy with y=y'\n",
    "        y_pred = self.model(*X)\n",
    "        return self.loss(y_pred, y), self.post_forward(y_pred)\n",
    "\n",
    "    def train_batch(self, *X, y=None):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss, y_pred = self.loss_xy(*X, y=y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss, y_pred\n",
    "        \n",
    "    def _time(self):\n",
    "        try:\n",
    "            t = self._start_time\n",
    "        except:\n",
    "            t = timeit.default_timer()\n",
    "        self._start_time = timeit.default_timer()\n",
    "        return timeit.default_timer() - t\n",
    "    \n",
    "    def train(self, epochs, lr=None, report_frequency=None, save=None, optimizer=None, scheduler=False, weight_decay=None, momentum=None, save_lowest=None, save_highest=None, log={}):\n",
    "        self.del_optimizer()\n",
    "        self.lr = lr or self.lr\n",
    "        if weight_decay is not None and self.weight_decay != weight_decay:\n",
    "            self.weight_decay = weight_decay\n",
    "        if momentum is not None and self.momentum != momentum:\n",
    "            self.momentum = momentum\n",
    "        if optimizer and self._optimizerclass != optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        if scheduler is not False:\n",
    "            self.scheduler = scheduler\n",
    "        self.report_frequency = report_frequency or self.report_frequency\n",
    "        model = self.model\n",
    "        torch.set_grad_enabled(False)\n",
    "        reports = math.ceil(epochs / self.report_frequency)\n",
    "        maxepoch = self.epochid + epochs\n",
    "        epochspaces = int(math.log(maxepoch)/math.log(10)) + 1\n",
    "        batches = len(self.train_dl) * self.train_dl.batch_size * epochs + len(self.valid_dl) * self.valid_dl.batch_size * reports\n",
    "        pbar = tqdm(range(batches), desc='Total', leave=False)\n",
    "        self._time()\n",
    "        for i in range(epochs):\n",
    "            self.epochid += 1\n",
    "            epochloss = 0\n",
    "            n = 0\n",
    "            epoch_y_pred = []\n",
    "            epoch_y = []\n",
    "            try:\n",
    "                del self._scheduler\n",
    "            except: pass\n",
    "            self.scheduler\n",
    "            report = (((i + 1) % self.report_frequency) == 0 or i == epochs - 1)\n",
    "            with self.train_mode:\n",
    "                for *X, y in self.train_Xy:\n",
    "                    loss, y_pred = self.train_batch(*X, y=y)\n",
    "                    self.scheduler.step()\n",
    "                    try:\n",
    "                        # TODO naam aanpassen\n",
    "                        y_pred = model.post_forward(y_pred)\n",
    "                    except: pass\n",
    "                    if report:\n",
    "                        epochloss += loss.item() * len(y_pred)\n",
    "                        n += len(y_pred)\n",
    "                        epoch_y_pred.append(to_numpy(y_pred))\n",
    "                        epoch_y.append(to_numpy(y))\n",
    "\n",
    "                    pbar.update(self.train_dl.batch_size)\n",
    "            if report:\n",
    "                epochloss /= n\n",
    "                epoch_y = np.concatenate(epoch_y, axis=0)\n",
    "                epoch_y_pred = np.concatenate(epoch_y_pred, axis=0)\n",
    "                self.evaluator._store(epoch_y, epoch_y_pred, loss=epochloss, phase='train', epoch=self.epochid, **log)\n",
    "                validloss = self.validate(pbar = pbar, log=log)\n",
    "                metric = ''\n",
    "                v = self.evaluator.valid.iloc[-1]\n",
    "                for m in self.metrics:\n",
    "                    m = m.__name__\n",
    "                    value = v[m]\n",
    "                    metric += f'{m}={value:.5f} '\n",
    "                print(f'{self.epochid:>{epochspaces}} {self._time():.2f}s trainloss={epochloss:.5f} validloss={validloss:.5f} {metric}')\n",
    "                if save is not None:\n",
    "                    self.commit(f'{save}-{self.epochid}')\n",
    "                if save_lowest is not None:\n",
    "                    if self.lowest_score is None or validloss < self.lowest_score:\n",
    "                        self.lowest_score = validloss\n",
    "                        self.commit('lowest')\n",
    "                if save_highest is not None:\n",
    "                    if self.highest_score is None or validloss > self.highest_score:\n",
    "                        self.highest_score = validloss\n",
    "                        self.commit('highest')\n",
    "    \n",
    "    def lowest(self):\n",
    "        self.checkout('lowest')\n",
    "\n",
    "    def highest(self):\n",
    "        self.checkout('highest')\n",
    "\n",
    "    def learning_curve(self, y='loss', series='phase', select=None, xlabel = None, ylabel = None, title=None, label_prefix='', **kwargs):\n",
    "        return self.evaluator.line_metric(x='epoch', series=series, select=select, y=y, xlabel = xlabel, ylabel = ylabel, title=title, label_prefix=label_prefix, **kwargs)\n",
    "        \n",
    "    def validation_curve(self, y=None, x='epoch', series='phase', select=None, xlabel = None, ylabel = None, title=None, label_prefix='', **kwargs):\n",
    "        if y is not None and type(y) != str:\n",
    "            y = y.__name__\n",
    "        return self.evaluator.line_metric(x=x, series=series, select=select, y=y, xlabel = xlabel, ylabel = ylabel, title=title, label_prefix=label_prefix, **kwargs)\n",
    "       \n",
    "    def freeze(self, last=-1):\n",
    "        for c in list(self.model.children())[:last]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for c in list(self.model.children()):\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad=True\n",
    "\n",
    "    def tune(self, params,setter, lr=[1e-6, 1e-2], steps=40, smooth=0.05, label=None, **kwargs):\n",
    "        lr_values = exprange(*lr, steps)\n",
    "        if label is None:\n",
    "            label = str(setter)\n",
    "        if len(params) == 2:\n",
    "            params = range3(*params)\n",
    "        with tuner(self, lr_values, self.set_lr, smooth=0.05, label=label) as t:\n",
    "            t.run_multi(params, setter)\n",
    "\n",
    "    def tune_weight_decay(self, lr=[1e-6,1e-4], params=[1e-6, 1], steps=40, smooth=0.05, yscale='log', **kwargs):\n",
    "        self.tune( params, partial(self.set_optimizer_param, 'weight_decay'), lr=lr, steps=steps, smooth=smooth, label='weight decay', yscale=yscale, **kwargs)\n",
    "\n",
    "    def lr_find(self, lr=[1e-6, 10], steps=40, smooth=0.05, cache_valid=True, **kwargs):\n",
    "        with tuner(self, exprange(lr[0], lr[1], steps), self.set_lr, label='lr', yscale='log', smooth=smooth, cache_valid=cache_valid, **kwargs) as t:\n",
    "            t.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
