{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer.py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import timeit\n",
    "import sys\n",
    "import copy\n",
    "import inspect\n",
    "import numpy as np\n",
    "import math\n",
    "#from tqdm.notebook import tqdm\n",
    "from ..evaluate.evaluate import Evaluator\n",
    "from torch.optim.lr_scheduler import OneCycleLR, ConstantLR\n",
    "from .tuner import *\n",
    "from .helper import nonondict, tqdm_trainer\n",
    "from functools import partial\n",
    "import os\n",
    "    \n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "def UniformLR(*args, **kwargs):\n",
    "    class Uniform_Scheduler:\n",
    "        def step(self):\n",
    "            pass\n",
    "    return Uniform_Scheduler()\n",
    "\n",
    "def onecycle(optimizer, lr, steps):\n",
    "    return OneCycleLR(optimizer, lr[1], total_steps=steps)\n",
    "\n",
    "class ordered_dl:\n",
    "    def __init__(self, dl):\n",
    "        self.dl = dl\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.oldsampler = self.dl.batch_sampler.sampler\n",
    "        self.newsampler = torch.utils.data.sampler.SequentialSampler(self.oldsampler.data_source)\n",
    "        self.dl.batch_sampler.sampler = self.newsampler\n",
    "        return self.dl\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        self.dl.batch_sampler.sampler = self.oldsampler\n",
    "        if exc_type is not None:\n",
    "            return False\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    A general purpose trainer for PyTorch.\n",
    "    \n",
    "    Arguments:\n",
    "        model: nn.Module\n",
    "            a PyTorch Module that will be trained\n",
    "            \n",
    "        loss: callable\n",
    "            a PyTorch or custom loss function\n",
    "            \n",
    "        data: databunch or a list of iterables (DataLoaders)\n",
    "            a databunch is an object that has a train_dl, valid_dl,\n",
    "            and optionally test_dl property.\n",
    "            otherwise, a list of iterables can also be given. \n",
    "            Most often, these iterables are PyTorch DataLoaders that \n",
    "            are used to iterate over the respective datasets\n",
    "            for training and validation.\n",
    "            \n",
    "        metrics: callable or list of callable\n",
    "            One or more functions that can be called with (y, y_pred)\n",
    "            to compute an evaluation metric. This will automatically be\n",
    "            done during training, for both the train and valid sets.\n",
    "            Typically, the callable is a function from SKLearn.metrics\n",
    "            like mean_squared_error or recall_score.\n",
    "            \n",
    "        optimizer: PyTorch Optimizer (AdamW)\n",
    "            The PyTorch or custom optimizer class that is used during training\n",
    "            \n",
    "        optimizer_params: dict (None)\n",
    "            the parameters that are passed (along with the model parameters)\n",
    "            to initialize an optimizer. A 'nonondict' is used, meaning that\n",
    "            when a None value is set, the key is removed, so that the default\n",
    "            value is used instead.\n",
    "            \n",
    "        scheduler: None, OneCycleLR, ConstantLR\n",
    "            used to adapt the learning rate: \n",
    "            - None will use a constant learning rate\n",
    "            - OneCycleLR will will use a cyclic annealing learning rate\n",
    "              between an upper and lower bound.\n",
    "            - ConstantLR will use a linear decaying learning rate between\n",
    "              an upper bound and lower bound. You can optionally use\n",
    "              'cycle' when calling 'train' to restart ConstantLR \n",
    "              every 'cycle' epochs.\n",
    "              \n",
    "        scheduler_params: dict (None)\n",
    "            additional parameters that are passed when initializing the scheduler\n",
    "\n",
    "        weight_decay: float\n",
    "            Apply weight_decay regularization with the AdamW optimizer\n",
    "            \n",
    "        momentum: float\n",
    "            Apply momentum with the AdamW optimizer\n",
    "            \n",
    "        random_state: int\n",
    "            used to set a random state for reproducible results\n",
    "            \n",
    "        gpu: bool, int or torch.device\n",
    "            The device to train on:\n",
    "                False or -1: cpu\n",
    "                True: cuda:0, this is probably what you want to train on gpu\n",
    "                int: cuda:gpu\n",
    "            Setting the device will automatically move the model and data to\n",
    "            the given device. Note that the model is not automatically\n",
    "            transfered back to cpu afterwards.\n",
    "    \n",
    "        evaluator: PipeTorch evaluator\n",
    "            An evaluator that was created by a different trainer or \n",
    "            DataFrame, to combine the results of different training\n",
    "            sessions.\n",
    "            \n",
    "        debug: bool (False)\n",
    "            stores X, y and y_pred in properties so that they can be inspected\n",
    "            when an error is thrown.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss, *data, metrics = [], \n",
    "                 optimizer=AdamW, optimizer_params=None, \n",
    "                 scheduler=None, scheduler_params=None,\n",
    "                 weight_decay=None, momentum=None, gpu=False,\n",
    "                 random_state=None, evaluator=None, \n",
    "                 debug=False, **kwargs):\n",
    "        \n",
    "        # the amount of epochs in a cycle, \n",
    "        # validation is only done at the end of each cycle\n",
    "        self.cycle = 1  \n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.gpu(gpu)\n",
    "        self.set_data(*data)\n",
    "        self._model = model\n",
    "        self._debug = debug\n",
    "        try:\n",
    "            self.post_forward = model.post_forward\n",
    "        except: pass\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_params = scheduler_params\n",
    "        if self.random_state is not None:\n",
    "            torch.backends.cudnn.deterministic=True\n",
    "            torch.manual_seed(self.random_state)\n",
    "        self._commit = {}\n",
    "        self.epochid = 0\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.lowest_score=None\n",
    "        self.highest_score=None\n",
    "        if evaluator is not None:\n",
    "            assert len(metrics) == 0, 'When you assign an evaluator, you cannot assign different metrics to a trainer'\n",
    "            self._evaluator = evaluator\n",
    "            self.metrics = evaluator.metrics\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "\n",
    "    def set_data(self, *data):\n",
    "        \"\"\"\n",
    "        Changes the dataset that is used by the trainer\n",
    "        \n",
    "        Arguments:\n",
    "            data: databunch or a list of iterables (DataLoaders)\n",
    "                a databunch is an object that has a train_dl, valid_dl,\n",
    "                and optionally test_dl property.\n",
    "                otherwise, a list of iterables can also be given. \n",
    "                Most often, these iterables are PyTorch DataLoaders that \n",
    "                are used to iterate over the respective datasets\n",
    "                for training and validation.\n",
    "        \"\"\"\n",
    "        assert len(data) > 0, 'You have to specify a data source. Either a databunch or a set of dataloaders'\n",
    "        if len(data) == 1:\n",
    "            self.databunch = data[0]\n",
    "        elif len(data) < 4:\n",
    "            try:\n",
    "                _ = iter(data[0])\n",
    "                self.train_dl = data[0]\n",
    "            except TypeError:\n",
    "                raise TypeError('The first data source must be iterable, preferably a DataLoader that provide an X and y')\n",
    "            try:\n",
    "                _ = iter(data[1])\n",
    "                self.valid_dl = data[1]\n",
    "            except TypeError:\n",
    "                raise TypeError('The second data source must be iterable, preferably a DataLoader that provide an X and y')\n",
    "            if len(data) > 2:\n",
    "                try:\n",
    "                    _ = iter(data[2])\n",
    "                    self.test_dl = data[2]\n",
    "                except TypeError:\n",
    "                    raise TypeError('The third data source must be iterable, preferably a DataLoader that provide an X and y')\n",
    "\n",
    "    def reset_evaluator(self):\n",
    "        try:\n",
    "            del self._evaluator\n",
    "        except: pass\n",
    "        self.epochid = 0\n",
    "                    \n",
    "    @property\n",
    "    def evaluator(self):\n",
    "        \"\"\"\n",
    "        The (PipeTorch) evaluator that is used to log training progress\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._evaluator\n",
    "        except:\n",
    "            self._evaluator = Evaluator(self, *self.metrics)\n",
    "            return self._evaluator\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return 'Trainer( ' + self.model + ')'\n",
    "\n",
    "    def to(self, device):\n",
    "        \"\"\"\n",
    "        Configures the device to train on\n",
    "        \n",
    "        Arguments:\n",
    "            device: bool, int or torch.device\n",
    "                The device to train on:\n",
    "                    False or -1: cpu\n",
    "                    True: cuda:0, this is probably what you want to train on gpu\n",
    "                    int: cuda:gpu\n",
    "                Setting the device will automatically move the model and data to\n",
    "                the given device. Note that the model is not automatically\n",
    "                transfered back to cpu afterwards.\n",
    "        \"\"\"\n",
    "        if device is True or (type(device) == int and device == 0):\n",
    "            device = torch.device('cuda:0')\n",
    "        elif device is False or (type(device) == int and device == -1):\n",
    "            device = torch.device('cpu')\n",
    "        elif type(device) == int:\n",
    "            assert device < torch.cuda.device_count(), 'Cannot use gpu {device}, note that if a gpu has already been selected it is always renumbered to 0'\n",
    "            device = torch.device(f'cuda:{device}')\n",
    "        try:\n",
    "            if device != self.device:\n",
    "                self.device = device\n",
    "                try:\n",
    "                    del self._optimizer\n",
    "                except: pass\n",
    "        except:\n",
    "            self.device = device\n",
    "        self._gpu = self.device == 'cuda'\n",
    "\n",
    "    def cpu(self):\n",
    "        \"\"\"\n",
    "        Configure the trainer to train on cpu\n",
    "        \"\"\"\n",
    "        self.to(False)\n",
    "\n",
    "    def gpu(self, gpu=True):\n",
    "        \"\"\"\n",
    "        Configure the trainer to train on gpu, see to(device)\n",
    "        \"\"\"\n",
    "        self.to(gpu)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Returns: list of metrics that is collected while training\n",
    "        \"\"\"\n",
    "        return self._metrics\n",
    "    \n",
    "    @metrics.setter\n",
    "    def metrics(self, value):\n",
    "        \"\"\"\n",
    "        Sets the metric(s) that are collected while training\n",
    "        \"\"\"\n",
    "        try:\n",
    "            iter(value)\n",
    "            self._metrics = value\n",
    "        except:\n",
    "            self._metrics = [] if value is None else [value] \n",
    "        \n",
    "    @property\n",
    "    def databunch(self):\n",
    "        \"\"\"\n",
    "        Returns: the databunch that is used\n",
    "        \n",
    "        thows an exception if no databunch has been configured\n",
    "        \"\"\"\n",
    "        return self._databunch\n",
    "\n",
    "    @databunch.setter\n",
    "    def databunch(self, db):\n",
    "        \"\"\"\n",
    "        Setter to use a databunch. The databunch object must have at least\n",
    "        a train_dl and a valid_dl property, and optional a test_dl. These\n",
    "        are often PyTorch DataLoaders, but can be any iterable over a\n",
    "        DataSet.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert hasattr(db, 'train_dl'), 'A single data source must be an object with a train_dl property (like a databunch)'\n",
    "        assert hasattr(db, 'valid_dl'), 'A single data source must be an object with a valid_dl property (like a databunch)'\n",
    "        self._databunch = db\n",
    "        self.train_dl = self.databunch.train_dl\n",
    "        self.valid_dl = self.databunch.valid_dl\n",
    "        try:\n",
    "            self.test_dl = self.databunch.test_dl\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        \"\"\"\n",
    "        return: the learning rate that was set, could be an interval\n",
    "        \"\"\"\n",
    "        return self._lr\n",
    "        \n",
    "    @lr.setter\n",
    "    def lr(self, lr):\n",
    "        \"\"\"\n",
    "        Sets the learning rate that is used for training. You can either use a single value\n",
    "        for a fixed lr, a tuple with an interval of two values for a linear decaying \n",
    "        scheduler, or a tuple with an interval of two values for a OneCyleLR scheduler.\n",
    "        The allocation of a scheduler can be overruled by setting a scheduler manually.\n",
    "        \n",
    "        If the lr did not change, nothing happens, otherwise a new optimizer is created\n",
    "        when needed.\n",
    "        \"\"\"\n",
    "        if type(lr) is tuple:\n",
    "            lr = tuple(sorted(lr))\n",
    "        elif type(lr) is list:\n",
    "            lr = sorted(lr)\n",
    "        try:\n",
    "            if self.lr == lr:\n",
    "                return\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "        self._lr = lr\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        \"\"\"\n",
    "        sets the learning rate without changing the learning rate settings\n",
    "        the scheduler or optimizer. is used by tuners like find_lr.\n",
    "        \"\"\"\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    @property\n",
    "    def min_lr(self):\n",
    "        \"\"\"\n",
    "        the learning rate or lowest of an interval of learning rates\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.lr[0]\n",
    "        except:\n",
    "            try:\n",
    "                return self.lr\n",
    "            except:\n",
    "                return 1e-2\n",
    "\n",
    "    @property\n",
    "    def max_lr(self):\n",
    "        \"\"\"\n",
    "        the learning rate or highest of an interval of learning rates\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.lr[1]\n",
    "        except: pass\n",
    "        try:\n",
    "            return self.lr[0]\n",
    "        except: pass\n",
    "        return self.lr\n",
    "\n",
    "    def set_optimizer_param(self, key, value):\n",
    "        \"\"\"\n",
    "        Set a parameter for the optimizer. A 'nonondict' is used, \n",
    "        meaning that setting a value to None will cause the default\n",
    "        to be used.\n",
    "        \n",
    "        Argument:\n",
    "            key: str\n",
    "                the key to use\n",
    "                \n",
    "            value: any\n",
    "                the value to use. When set to None, the key is removed.\n",
    "        \"\"\"\n",
    "        self.optimizer_params[key] = value\n",
    "        try:\n",
    "            del self._optimizer\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def weight_decay(self):\n",
    "        \"\"\"\n",
    "        Returns: the current value for the weight decay regularization\n",
    "        \n",
    "        only works when using an Adam(W) optimizer\n",
    "        \"\"\"\n",
    "        return self.optimizer.param_groups[0]['weight_decay']\n",
    "\n",
    "    @weight_decay.setter\n",
    "    def weight_decay(self, value):\n",
    "        \"\"\"\n",
    "        Sets the weight decay regularization on the Adam(W) optimizer\n",
    "        \"\"\"\n",
    "        self.set_optimizer_param('weight_decay', value)\n",
    "\n",
    "    @property\n",
    "    def momentum(self):\n",
    "        \"\"\"\n",
    "        Returns the momentum value on the Adam(W) optimizer\n",
    "        \"\"\"\n",
    "        return self.optimizer.param_groups[0]['betas']\n",
    "\n",
    "    @momentum.setter\n",
    "    def momentum(self, value):\n",
    "        \"\"\"\n",
    "        Sets the momentum value on the Adam(W) optimizer\n",
    "        \"\"\"\n",
    "        self.set_optimizer_param('betas', value)\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        \"\"\"\n",
    "        Returns: an optimizer for training the model, using the applied\n",
    "        configuration (e.g. weight_decay, momentum, learning_rate).\n",
    "        If no optimizer exists, a new one is created using the configured\n",
    "        optimizerclass (default: AdamW) and settings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._optimizer\n",
    "        except:\n",
    "            self.set_optimizer_param('lr', self.min_lr)\n",
    "            self._optimizer = self._optimizer_class(self.model.parameters(), **self.optimizer_params)\n",
    "            return self._optimizer\n",
    "\n",
    "    @optimizer.setter\n",
    "    def optimizer(self, value):\n",
    "        \"\"\"\n",
    "        Sets the optimizer class to use. \n",
    "        \"\"\"\n",
    "        self._optimizer_class = value\n",
    "        try:\n",
    "            del self._optimizer\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def optimizer_params(self):\n",
    "        try:\n",
    "            return self._optimizer_params\n",
    "        except:\n",
    "            self._optimizer_params = nonondict()\n",
    "            return self._optimizer_params\n",
    "    \n",
    "    @optimizer_params.setter\n",
    "    def optimizer_params(self, value):\n",
    "        \"\"\"\n",
    "        Setter for the optimizer parameters used, only applies them if\n",
    "        the value is set other than None. If you want to remove all\n",
    "        params, set them to an empty dict.\n",
    "        \n",
    "        Arguments:\n",
    "            value: dict\n",
    "                conform the optimizer class that is used\n",
    "        \"\"\"\n",
    "        if value is not None:\n",
    "            assert isinstance(value, dict), 'you have set optimizer_params to a dict'\n",
    "            self._optimizer_params = nonondict(value)\n",
    "        \n",
    "    @property\n",
    "    def scheduler_params(self):\n",
    "        try:\n",
    "            return self._scheduler_params\n",
    "        except:\n",
    "            self._scheduler_params = nonondict()\n",
    "            return self._scheduler_params\n",
    "    \n",
    "    @scheduler_params.setter\n",
    "    def scheduler_params(self, value):\n",
    "        \"\"\"\n",
    "        Setter for the scheduler parameters used, only applies them if\n",
    "        the value is set other than None. If you want to remove all\n",
    "        params, set them to an empty dict.\n",
    "        \n",
    "        Arguments:\n",
    "            value: dict\n",
    "                conform the scheduler class/initializer that is used\n",
    "        \"\"\"\n",
    "        if value is not None:\n",
    "            assert isinstance(value, dict), 'you have set scheduler_params to a dict'\n",
    "            self._scheduler_params = nonondict(value)\n",
    "        \n",
    "    def del_optimizer(self):\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "        self.del_scheduler()\n",
    "\n",
    "    def del_scheduler(self):\n",
    "        try:\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        \"\"\"\n",
    "        Returns: scheduler that is used to adapt the learning rate\n",
    "\n",
    "        When you have set a (partial) function to initialze a scheduler, it should accepts\n",
    "        (optimizer, lr, scheduler_params) as its parameters. Otherwise, one of three standard\n",
    "        schedulers is used based on the value of the learning rate. If the learning rate is \n",
    "        - float: no scheduler is used\n",
    "        - [max, min]: a linear decaying scheduler is used. \n",
    "        - (max, min): a OneCyleLR scheduler is used.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._scheduler\n",
    "        except:\n",
    "            #steps = int(round((len(self.train_dl) * self.cycle_epochs)))\n",
    "            if self._scheduler_class is None:\n",
    "                try:\n",
    "                    self.lr[1]\n",
    "                    if type(self.lr) == tuple:\n",
    "                        schedulerclass = OneCycleLR\n",
    "                    elif type(self.lr) == list:\n",
    "                        schedulerclass = ConstantLR\n",
    "                    else:\n",
    "                        raise NotImplementedError(f'Provide either an single value learning rate for a Uniform scheduler, list [low, high] for a Linear Decay, or tuple (low, high) for a OneCycleLR scheduler')\n",
    "                except:\n",
    "                    schedulerclass = UniformLR\n",
    "            else:\n",
    "                schedulerclass = self._scheduler_class\n",
    "            if schedulerclass == ConstantLR:\n",
    "                factor = (self.min_lr / self.max_lr) ** (1 / self._scheduler_epochs)\n",
    "                self._scheduler = ConstantLR(self.optimizer, factor,\n",
    "                                  self._scheduler_epochs, **self.scheduler_params)\n",
    "            elif schedulerclass == OneCycleLR:\n",
    "                scheduler_params = self.scheduler_params\n",
    "                scheduler_params['epochs'] = self._scheduler_epochs\n",
    "                scheduler_params['steps_per_epoch'] = len(self.train_dl)\n",
    "                self._scheduler = OneCycleLR(self.optimizer, \n",
    "                                  self.min_lr, **scheduler_params) \n",
    "            else:\n",
    "                try:\n",
    "                    self._scheduler = schedulerclass(self.optimizer, \n",
    "                                  self.lr, **self.scheduler_params)\n",
    "                except:\n",
    "                    raise NotImplementedError(f'The provided {schedulerclass} function does not work with ({self.optimizer}, {self.lr}, {self._scheduler_epochs}, {len(self.train_dl)}) to instantiate a scheduler')\n",
    "            return self._scheduler\n",
    "    \n",
    "    @scheduler.setter\n",
    "    def scheduler(self, value):\n",
    "        \"\"\"\n",
    "        Sets the schedulerclass (or function to initialize a scheduler) to use. At this moment,\n",
    "        there is no uniform way to initialize all PyTorch schedulers. \n",
    "        PipeTorch provides easy support for using a scheduler through the learning rate:\n",
    "        - float: no scheduler is used\n",
    "        - [max, min]: a linear annealing scheduler is used. \n",
    "        - (max, min): a OneCyleLR scheduler is used.\n",
    "        \n",
    "        To use another scheduler, set this to a function that accepts\n",
    "        the following parameters: (optimizer instance, learning rate, **scheduler_params)\n",
    "        \n",
    "        The scheduler_params can be supplied when calling train.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            del self._scheduler\n",
    "        except: pass\n",
    "        self._scheduler_class = value\n",
    "    \n",
    "    @property\n",
    "    def valid_ds(self):\n",
    "        return self.valid_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def train_ds(self):\n",
    "        return self.train_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def test_ds(self):\n",
    "        return self.test_dl.dataset\n",
    "\n",
    "    @property\n",
    "    def train_Xy(self):\n",
    "        for batch in self.train_dl:\n",
    "            yield [ t.to(self.model.device) for t in batch ]\n",
    "    \n",
    "    @property\n",
    "    def valid_Xy(self):\n",
    "        for batch in self.valid_dl:\n",
    "            yield [ t.to(self.model.device) for t in batch ]\n",
    "    \n",
    "    @property\n",
    "    def test_Xy(self):\n",
    "        for batch in self.test_dl:\n",
    "            yield [ t.to(self.model.device) for t in batch ]\n",
    "    \n",
    "    @property\n",
    "    def valid_tensors(self):\n",
    "        return self.valid_dl.dataset.tensors\n",
    "\n",
    "    @property\n",
    "    def train_tensors(self):\n",
    "        return self.train_dl.dataset.tensors\n",
    "\n",
    "    @property\n",
    "    def test_tensors(self):\n",
    "        return self.test_dl.dataset.tensors\n",
    "\n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.train_tensors[0]\n",
    "\n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train_tensors[-1]\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.valid_tensors[0]\n",
    "\n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid_tensors[-1]\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test_tensors[0]\n",
    "\n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test_tensors[-1]\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        When a device is configured to train the model on, the model\n",
    "        is automatically transferred to the device. A device property\n",
    "        is set on the model to transfer the data to the same device\n",
    "        as the model before using.\n",
    "        \n",
    "        Returns: the model \n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.device is not self._model.device:\n",
    "                self._model.device = self.device\n",
    "                self._model.to(self.device)\n",
    "                try:\n",
    "                    del self._optimizer\n",
    "                except: pass\n",
    "        except:\n",
    "            try:\n",
    "                self._model.device = self.device\n",
    "                self._model.to(self.device)\n",
    "                #print('change device')\n",
    "                try:\n",
    "                    del self._optimizer\n",
    "                except: pass\n",
    "            except: pass\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, value):\n",
    "        self._model = value\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._evaluator\n",
    "        except: pass\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Prints the (trainable) model parameters\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.data)\n",
    "\n",
    "    def reset_model(self):\n",
    "        \"\"\"\n",
    "        Resets all weights in the current model\n",
    "        \n",
    "        refs:\n",
    "            - https://discuss.pytorch.org/t/how-to-re-set-alll-parameters-in-a-network/20819/6\n",
    "            - https://stackoverflow.com/questions/63627997/reset-parameters-of-a-neural-network-in-pytorch\n",
    "            - https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "        \"\"\"\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def weight_reset(m: nn.Module):\n",
    "            # - check if the current module has reset_parameters & if it's callabed called it on m\n",
    "            reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "            if callable(reset_parameters):\n",
    "                m.reset_parameters()\n",
    "\n",
    "        # Applies fn recursively to every submodule see: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "        self.model.apply(fn=weight_reset)\n",
    "        try:\n",
    "            del self._optimizer\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._evaluator\n",
    "        except: pass        \n",
    "                \n",
    "    def forward(self, *X):\n",
    "        \"\"\"\n",
    "        Returns the results of the model's forward on the given input X.\n",
    "             \n",
    "        Arguments:\n",
    "            *X: tensor or collection of tensors\n",
    "                the tensor of collection of tensors that is passed to\n",
    "                the forward of the model. The inputs are automatically \n",
    "                transfered to the same device as the model is on.\n",
    "        \n",
    "        Returns: tensor\n",
    "            outputs that are returned by first the forward pass on\n",
    "            the model.\n",
    "        \"\"\"\n",
    "        X = [ x.to(self.model.device) for x in X ]\n",
    "        if self._debug:\n",
    "            self.lastx = X\n",
    "            self.lastyfw = self.model(*X)\n",
    "            return self.lastyfw\n",
    "        return self.model(*X)\n",
    "       \n",
    "    def predict(self, *X):\n",
    "        \"\"\"\n",
    "        Returns model predictions for the given input.\n",
    "        The difference with forward is that the outputs of the model\n",
    "        are optionally processed by a post_forward (for classification).\n",
    "        \n",
    "        Arguments:\n",
    "            *X: tensor or collection of tensors\n",
    "                the tensor of collection of tensors that is passed to\n",
    "                the forward of the model. The inputs are automatically \n",
    "                transfered to the same device as the model is on.\n",
    "        \n",
    "        Returns: tensor\n",
    "            Predictions that are returned by first the forward pass on\n",
    "            the model and optionally a post_forward for classification\n",
    "            tasks\n",
    "        \"\"\"\n",
    "        return self.post_forward(self.forward(*X))\n",
    "\n",
    "    def post_forward(self, y):\n",
    "        \"\"\"\n",
    "        For classification tasks, training may require a different \n",
    "        pred_y than the evaluation metrics do. Typically, the predictions\n",
    "        are logits or an estimated likelihood (e.g. 0.2), while the \n",
    "        evaluation function need a class label (e.g. 0 or 1). Using\n",
    "        PipeTorch, you need to add a post_forward(y) method to your model,\n",
    "        that will be called on the predictions before they are passed\n",
    "        to the evaluation functions. \n",
    "        \n",
    "        Returns: tensor\n",
    "            If the model has a post_forward to convert pred_y to predictions,\n",
    "            this returns the the results calling post_forward, otherise,\n",
    "            it will just return pred_y\n",
    "        \"\"\"\n",
    "        post_forward = getattr(self.model, \"post_forward\", None)\n",
    "        if callable(post_forward):\n",
    "            y = self.model.post_forward(y)\n",
    "            if self._debug:\n",
    "                self.lastypfw = y\n",
    "        return y\n",
    "\n",
    "    def list_commits(self):\n",
    "        \"\"\"\n",
    "        Returns: a list of the keys of committed (saved) models, during \n",
    "        or after training.\n",
    "        \"\"\"\n",
    "        return self._commit.keys()\n",
    "\n",
    "    def commit(self, label):\n",
    "        \"\"\"\n",
    "        Save the model and optimizer state, allowing to revert to a \n",
    "        previous state/version of the model.\n",
    "        \n",
    "        Arguments:\n",
    "            label: str\n",
    "                The key to save the model under\n",
    "        \"\"\"        \n",
    "        model_state = copy.deepcopy(self.model.state_dict())\n",
    "        optimizer_state = copy.deepcopy(self.optimizer.state_dict())\n",
    "        self._commit[label] = (model_state, optimizer_state, self.epochid, self.evaluator.results.clone())\n",
    "\n",
    "    def _model_filename(self, folder=None, filename=None, extension=None):\n",
    "        if folder is None:\n",
    "            folder = '.'\n",
    "        if filename is not None:\n",
    "            path = f'{folder}/{filename}'\n",
    "        else:\n",
    "            path = f'{folder}/{self.model.__class__.__name__}'\n",
    "        if '.pyt' not in path:\n",
    "            if extension is None:\n",
    "                return f'{path}.pyt{torch.__version__}'\n",
    "            else:\n",
    "                return f'{path}.{extension}'\n",
    "        return path\n",
    "        \n",
    "    def save(self, folder=None, filename=None, extension=None):\n",
    "        \"\"\"\n",
    "        Saves a (trained) model to file. This will only save the model parameters. To load the model, you will\n",
    "        first have to initialize a model with the same configuration, and then use `Trainer.load(path)` to load\n",
    "        the model from file.\n",
    "        \n",
    "        Aruments:\n",
    "            folder: str (None)\n",
    "                folder to save the model, default is the current folder\n",
    "            filename: str (None)\n",
    "                the basename of the saved file, default is the classname\n",
    "            extension: str (None)\n",
    "                the extension of the saved file, default is pyt with the pytorch version name\n",
    "        \"\"\"\n",
    "        path = self._model_filename(folder, filename, extension)\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f'Saved the model as {path}')\n",
    "        \n",
    "    def load(self, folder=None, filename=None, extension=None):\n",
    "        \"\"\"\n",
    "        Load a saved (trained) model from file. For this to work, the model for this trainer has to be configured\n",
    "        in the exact same way as the model that was saved. This will only load the model parameters.\n",
    "        \n",
    "        Aruments:\n",
    "            folder: str (None)\n",
    "                folder to save the model, default is the current folder\n",
    "            filename: str (None)\n",
    "                the basename of the saved file, default is the classname\n",
    "            extension: str (None)\n",
    "                the extension of the saved file, default is pyt with the pytorch version name\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(torch.load(self._model_filename(folder, filename, extension)))\n",
    "        \n",
    "    def to_trt(self):\n",
    "        \"\"\"\n",
    "        Converts the (trained) model into a TRT model that can be used on a Jetson\n",
    "        \n",
    "        Returns: TRTModule\n",
    "            The converted model\n",
    "        \"\"\"\n",
    "        from torch2trt import torch2trt\n",
    "        x = next(iter(self.train_Xy))[0]\n",
    "        print(x.shape)\n",
    "        return torch2trt(self.model, [x])\n",
    "        \n",
    "    def save_trt(self, folder=None, filename=None, extension='trt'):\n",
    "        \"\"\"\n",
    "        Converts the (trained) model to TRT and saves it.\n",
    "        \n",
    "        Aruments:\n",
    "            folder: str (None)\n",
    "                folder to save the model, default is the current folder\n",
    "            filename: str (None)\n",
    "                the basename of the saved file, default is the classname\n",
    "            extension: str ('trt')\n",
    "                the extension of the saved file\n",
    "        \"\"\"\n",
    "        path = self._model_filename(folder, filename, extension)\n",
    "        torch.save(self.to_trt().state_dict(), path)\n",
    "        print(f'Saved the TRT model as {path}')\n",
    "        \n",
    "    def save_onnx(self, folder=None, filename=None, extension='onnx'):\n",
    "        \"\"\"\n",
    "        Converts the (trained) model to ONNX and saves it.\n",
    "        \n",
    "        Aruments:\n",
    "            folder: str (None)\n",
    "                folder to save the model, default is the current folder\n",
    "            filename: str (None)\n",
    "                the basename of the saved file, default is the classname\n",
    "            extension: str ('onnx')\n",
    "                the extension of the saved file\n",
    "        \"\"\"\n",
    "        path = self._model_filename(folder, filename, extension)\n",
    "        x = next(iter(self.train_Xy))[0][:1]\n",
    "        torch.onnx.export(self.model, x, path, verbose=True)\n",
    "        print(f'Saved the ONNX model as {path}')\n",
    "        \n",
    "        \n",
    "    def revert(self, label):\n",
    "        \"\"\"\n",
    "        Revert the model and optimizer to a previously commited state, \n",
    "        and deletes the commit point to free memory. Prints a warning\n",
    "        when the label was not found.\n",
    "        \n",
    "        Arguments:\n",
    "            label: str\n",
    "                The key under which the model was commited\n",
    "        \"\"\"\n",
    "        if label in self._commit:\n",
    "            model_state, optimizer_state, self.epochid, self.evaluator.results = self._commit.pop(label)\n",
    "            self.model.load_state_dict(model_state)\n",
    "            self.del_optimizer()            \n",
    "            self.optimizer.load_state_dict(optimizer_state)\n",
    "        else:\n",
    "            print('commit point {label} not found')\n",
    "    \n",
    "    def checkout(self, label):\n",
    "        \"\"\"\n",
    "        Loads a previously commited state of the model and optimizer \n",
    "        but keeps the commit point. Prints a warning\n",
    "        when the label was not found.\n",
    "        \n",
    "        Arguments:\n",
    "            label: str\n",
    "                The key under which the model was commited\n",
    "        \"\"\"\n",
    "        if label in self._commit:\n",
    "            model_state, optimizer_state, self.epochid, self.evaluator.results = self._commit[label]\n",
    "            self.model.load_state_dict(model_state)\n",
    "            self.del_optimizer()            \n",
    "            self.optimizer.load_state_dict(optimizer_state)  \n",
    "        else:\n",
    "            print('commit point {label} not found')\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the cached results, for tuning purposes.\n",
    "        \"\"\"\n",
    "        self.reset_model()\n",
    "        self.reset_evaluator()\n",
    "            \n",
    "    def remove_checkpoint(self, label):\n",
    "        \"\"\"\n",
    "        Removes a previously committed state of the model.\n",
    "        \n",
    "        Arguments:\n",
    "            label: str\n",
    "                The key under which the model was commited\n",
    "        \"\"\"\n",
    "        self._commit.pop(label)\n",
    "\n",
    "    def purge(self, label):\n",
    "        \"\"\"\n",
    "        Switches the model and optimizer to a previously commited state, \n",
    "        and keeps only that commit point and removes all other versions.\n",
    "        \n",
    "        Arguments:\n",
    "            label: str\n",
    "                The key under which the model was commited\n",
    "        \"\"\"\n",
    "        if label in self._commit:\n",
    "            self.checkout(label)\n",
    "            self._commit = { l:s for l,s in self._commit.items() if l == label }\n",
    "        else:\n",
    "            print(f'commit point {label} not found')\n",
    "\n",
    "    def _loss_xy(self, *X, y=None, debug=False):\n",
    "        \"\"\"\n",
    "        Computes predictions for the given X.\n",
    "        \n",
    "        Arguments:\n",
    "            *X: tensor\n",
    "                inputs that are used by the forward of the model\n",
    "            y: tensor\n",
    "                ground truth labels, the predictions are compared against\n",
    "        \n",
    "        Returns: (float, tensor)\n",
    "            a tuple with the loss for the predictions on X,\n",
    "            and a tensor with the predicted values\n",
    "        \"\"\"\n",
    "        assert y is not None, 'Call _loss_xy with y=None'\n",
    "        if self._debug:\n",
    "            self.lasty = y\n",
    "        y_pred = self.forward(*X)\n",
    "        return self.loss(y_pred, y), self.post_forward(y_pred)\n",
    "    \n",
    "    def loss_dl(self, dl):\n",
    "        \"\"\"\n",
    "        Iterates over the given dataloader, the loss is computed in\n",
    "        evaluation mode and accumulated over the dataset.\n",
    "        \n",
    "        Arguments:\n",
    "            dl: DataLoader\n",
    "                the dataloader that is used to iterate over.\n",
    "        \n",
    "        Returns: float \n",
    "            weighted average loss over the given dataloader/set.\n",
    "        \"\"\"\n",
    "        if not dl:\n",
    "            dl = self.valid_Xy\n",
    "        losses = []\n",
    "        leny = 0\n",
    "        for *X, y in dl:\n",
    "            if self._debug:\n",
    "                self.lasty = y\n",
    "            y_pred = self.forward(*X)\n",
    "            l = self.loss(y_pred, y)\n",
    "            losses.append(l.item() * len(y))\n",
    "            leny += len(y)\n",
    "        return sum(losses) / leny\n",
    "\n",
    "    def validate_loss(self):\n",
    "        \"\"\"\n",
    "        Returns: weighted average loss over the validation set, or\n",
    "        the data that is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.loss_dl(self.valid_Xy)\n",
    "\n",
    "    @property\n",
    "    def eval_mode(self):\n",
    "        \"\"\"\n",
    "        A ContextManager to put the model in evaluation mode\n",
    "        \"\"\"\n",
    "        class CM(object):\n",
    "            def __init__(self, trainer):\n",
    "                self.trainer = trainer\n",
    "            def __enter__(self):\n",
    "                self.trainer.model.eval()\n",
    "                self.prev = torch.is_grad_enabled()\n",
    "                torch.set_grad_enabled(False)\n",
    "                return self.trainer.model\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                torch.set_grad_enabled(self.prev)\n",
    "                self.trainer.model.train()\n",
    "        return CM(self)\n",
    "\n",
    "    @property\n",
    "    def train_mode(self):\n",
    "        \"\"\"\n",
    "        A ContextManager to put the model in training mode\n",
    "        \"\"\"\n",
    "        class CM(object):\n",
    "            def __init__(self, trainer):\n",
    "                self.trainer = trainer\n",
    "            def __enter__(self):\n",
    "                self.trainer.model.train()\n",
    "                self.prev = torch.is_grad_enabled()\n",
    "                torch.set_grad_enabled(True)\n",
    "                return self.trainer.model\n",
    "            def __exit__(self, type, value, traceback):\n",
    "                torch.set_grad_enabled(self.prev)\n",
    "                self.trainer.model.eval()\n",
    "        return CM(self)\n",
    "\n",
    "    def validate(self, pbar=None, log={}):\n",
    "        \"\"\"\n",
    "        Run the validation set (in evaluation mode) and store the loss and metrics into the evaluator.\n",
    "        \n",
    "        Arguments:\n",
    "            pbar: tqdm progress bar (None)\n",
    "                if not None, progress is reported on the progress bar\n",
    "                \n",
    "            log: dict\n",
    "                additional labels to log when storing the results in the evaluator.\n",
    "                \n",
    "        Returns: float\n",
    "            weighted average loss over the validation set\n",
    "        \"\"\"\n",
    "        epochloss = 0\n",
    "        n = 0\n",
    "        epoch_y_pred = []\n",
    "        epoch_y = []\n",
    "\n",
    "        with self.eval_mode:\n",
    "            for *X, y in self.valid_Xy:\n",
    "                loss, y_pred = self._loss_xy(*X, y=y)\n",
    "                epochloss += loss.item() * len(y_pred)\n",
    "                n += len(y_pred)\n",
    "                epoch_y_pred.append(to_numpy(y_pred))\n",
    "                epoch_y.append(to_numpy(y))\n",
    "                if pbar is not None:\n",
    "                    pbar.update(self.valid_dl.batch_size)\n",
    "            epochloss /= n\n",
    "            epoch_y = np.concatenate(epoch_y, axis=0)\n",
    "            epoch_y_pred = np.concatenate(epoch_y_pred, axis=0)\n",
    "            metrics = self.evaluator._store_metrics(epoch_y, epoch_y_pred, phase='valid', epoch=self.epochid, **log)\n",
    "            self.evaluator._store_metric('loss', epochloss, phase='valid', epoch=self.epochid, **log)\n",
    "        return epochloss, metrics\n",
    "    \n",
    "    def _test(self, pbar=None, log={}):\n",
    "        \"\"\"\n",
    "        Run the test set (in evaluation mode) and store the loss and metrics into the evaluator.\n",
    "        \n",
    "        Arguments:\n",
    "            pbar: tqdm progress bar (None)\n",
    "                if not None, progress is reported on the progress bar\n",
    "                \n",
    "            log: dict\n",
    "                additional labels to log when storing the results in the evaluator.\n",
    "                \n",
    "        Returns: float\n",
    "            weighted average loss over the validation set\n",
    "        \"\"\"\n",
    "        epochloss = 0\n",
    "        n = 0\n",
    "        epoch_y_pred = []\n",
    "        epoch_y = []\n",
    "\n",
    "        with self.eval_mode:\n",
    "            for *X, y in self.test_Xy:\n",
    "                loss, y_pred = self._loss_xy(*X, y=y)\n",
    "                epochloss += loss.item() * len(y_pred)\n",
    "                n += len(y_pred)\n",
    "                epoch_y_pred.append(to_numpy(y_pred))\n",
    "                epoch_y.append(to_numpy(y))\n",
    "                if pbar is not None:\n",
    "                    pbar.update(self.test_dl.batch_size)\n",
    "            epochloss /= n\n",
    "            epoch_y = np.concatenate(epoch_y, axis=0)\n",
    "            epoch_y_pred = np.concatenate(epoch_y_pred, axis=0)\n",
    "            metrics = self.evaluator._store_metrics(epoch_y, epoch_y_pred, phase='test', epoch=self.epochid, **log)\n",
    "            self.evaluator._store_metric('loss', epochloss, phase='test', epoch=self.epochid, **log)\n",
    "        return epochloss, metrics\n",
    "            \n",
    "    def train_batch(self, *X, y=None):\n",
    "        \"\"\"\n",
    "        Train the model on a single batch X, y. The model should already\n",
    "        be in training mode.\n",
    "        \n",
    "        Arguments:\n",
    "            *X: tensor\n",
    "                inputs that are used by the forward of the model\n",
    "            y: tensor\n",
    "                ground truth labels, the predictions are compared against\n",
    "        \n",
    "        Returns: (float, tensor)\n",
    "            a tuple with the loss for the predictions on X,\n",
    "            and a tensor with the predicted values\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss, y_pred = self._loss_xy(*X, y=y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss, y_pred\n",
    "        \n",
    "    def _time(self):\n",
    "        try:\n",
    "            t = self._start_time\n",
    "        except:\n",
    "            t = timeit.default_timer()\n",
    "        self._start_time = timeit.default_timer()\n",
    "        return timeit.default_timer() - t\n",
    "    \n",
    "    def cross_validate(self, epochs, lr, cycle=None, data=None,\n",
    "              optimizer=None, optimizer_params=None, scheduler=False, \n",
    "              scheduler_params=None, weight_decay=None, momentum=None,\n",
    "              earlystop=False, silent=True, test=True, reset_evaluator=True):\n",
    "        \"\"\"\n",
    "        Only works with a Databunch from a DFrame that is configured for n-fold cross validation. \n",
    "        The model is trained n times (reinitializing every time), and the average metric is reported \n",
    "        over the trials.\n",
    "        \n",
    "        Arguments:\n",
    "            epochs: int\n",
    "                the maximum number of epochs to train. Training may be terminated early when\n",
    "                convergence requirements are met.\n",
    "            lr: float, (float, float) or [float, float]\n",
    "                the learning rate to use for the optimzer. See lr for train().\n",
    "            cycle: int\n",
    "                the number of epochs in a cycle. At the end of each cycle the validation is run.\n",
    "            data: generator (train_dl, valid_dl, test_dl) or a PipeTorch Databunch (None)\n",
    "                if None, the current databunch is used. On databunches iter_folds() is called to iterate\n",
    "                over the dataloaders that are constructed on a DFrame that was configured with folds().\n",
    "                Alternatively, you may pass an iterable that supplies (train_dl, valid_dl, test_dl)\n",
    "            for the other parameters, see train()\n",
    "        \"\"\"\n",
    "        from ..data import Databunch\n",
    "        \n",
    "        cycle = cycle or self.cycle\n",
    "        optimizer = optimizer or self._optimizer_class\n",
    "        scheduler = scheduler or self._scheduler_class\n",
    "        optimizer_params = optimizer_params or self.optimizer_params\n",
    "        scheduler_params = scheduler_params or self.scheduler_params\n",
    "        weight_decay = weight_decay or self.weight_decay\n",
    "        momentum = momentum or self.momentum\n",
    "        pbar = None\n",
    "        if reset_evaluator:\n",
    "            self.reset_evaluator()\n",
    "        \n",
    "        if data is None:\n",
    "            data = self.databunch\n",
    "        if type(data) == Databunch:\n",
    "            folds = data.folds or 1\n",
    "            data = data.iter_folds()\n",
    "        else:\n",
    "            folds = len(data)\n",
    "        data = iter(data)\n",
    "        \n",
    "        def run(trial):\n",
    "            nonlocal pbar\n",
    "            global t\n",
    "            self.reset_model()\n",
    "            train_dl, valid_dl, test_dl = next(data)\n",
    "            if pbar is None:\n",
    "                pbar = tqdm_trainer(epochs, cycle, train_dl, valid_dl, test_dl, folds=folds)\n",
    "            self.t = Trainer(self.model, self.loss, train_dl, valid_dl, test_dl, evaluator=self.evaluator,\n",
    "                        optimizer=optimizer, optimizer_params=optimizer_params,\n",
    "                        scheduler=scheduler, scheduler_params=scheduler_params,\n",
    "                        weight_decay=weight_decay, momentum=momentum, gpu=self._gpu,\n",
    "                        random_state=self.random_state, debug=self._debug, earlystop=earlystop)\n",
    "            self.t.train(epochs, lr, cycle=cycle, pbar=pbar, log={'fold':trial.number}, test=test, silent=silent)\n",
    "            return self.t.optimum(select={'fold':trial.number})\n",
    "        \n",
    "        return self.optimize(run, n_trials=folds)\n",
    "    \n",
    "    def optimize(self, func, n_trials=None, timeout=None, catch=(), callbacks=None, \n",
    "                 gc_after_trial=False, show_progress_bar=False):\n",
    "        \"\"\"\n",
    "        Run n_trials on the given func to optimize settings and hyperparameters. This uses an \n",
    "        extension to the Optuna library tio create a study. This extension allows to define your\n",
    "        trial func(trainer, trial) so that you can reuse the configured trainer. For the \n",
    "        other arguments, see Optuna.Study.optimize\n",
    "        \n",
    "        Returns: Study (extension to Optuna's Study)\n",
    "            That contains the collected metrics for the trials\n",
    "        \"\"\"\n",
    "        study = self.study()\n",
    "        study.optimize(func, n_trials=n_trials, timeout=timeout, catch=catch, callbacks=callbacks,\n",
    "                      gc_after_trial=gc_after_trial, show_progress_bar=show_progress_bar)\n",
    "        return study\n",
    "    \n",
    "    def train(self, epochs, lr=None, cycle=None, save=None, \n",
    "              optimizer=None, optimizer_params=None, scheduler=False, \n",
    "              scheduler_params=None, weight_decay=None, momentum=None, \n",
    "              save_lowest=False, save_highest=False, silent=False, pbar=None,\n",
    "              targetloss=None, earlystop=False, log={}, test=False):\n",
    "        \"\"\"\n",
    "        Train the model for the given number of epochs. Loss and metrics\n",
    "        are logged during training in an evaluator. If a model was already\n",
    "        (partially) trained, training will continue where it was left off.\n",
    "        \n",
    "        Arguments:\n",
    "            epochs: int\n",
    "                the number of epochs to train the model\n",
    "            \n",
    "            lr: float, tuple of floats, or list of floats\n",
    "                float: set the learning\n",
    "                (upper, lower): switch the scheduler to OneCycleLR and\n",
    "                    use a cyclic annealing learning rate\n",
    "                    between an upper and lower bound.\n",
    "                [upper, lower]: switch the scheduler to Linear Decay and\n",
    "                    use a linearly decaying learning rate\n",
    "                    between an upper and lower bound. \n",
    "            \n",
    "            cycle: int (None)\n",
    "                Configures after how many epochs there are in a cycle\n",
    "                the loss and metrics are logged and reported at the end of every cycle.\n",
    "                This is remembered for consecutive calls to train.\n",
    "            \n",
    "            silent: bool (False)\n",
    "                whether to report progress. Note that even when silent=True\n",
    "                the metrics are still logged at the end of every cycle.\n",
    "            \n",
    "            save: str (None)\n",
    "                If not None, saves (commits) the model at the end of each cycle\n",
    "                under the name 'save'-epochnr\n",
    "            \n",
    "            optimizer: PyTorch Optimizer (None)\n",
    "                If not None, changes the optimizer class to use.\n",
    "\n",
    "            optimizer_params: dict (None)\n",
    "                If not None, the parameters to configure the optimizer.\n",
    "\n",
    "            scheduler: None, custom scheduler class\n",
    "                used to adapt the learning rate. Set OneCycleLR or Linear Decay\n",
    "                through the learning rate. Otherwise, provide a custom\n",
    "                class/function to initialize a scheduler by accepting\n",
    "                (optimizer, learning_rate, scheduler_cycle)\n",
    "\n",
    "            scheduler_params: dict (None)\n",
    "                additional parameters that are passed when initializing the scheduler\n",
    "\n",
    "            weight_decay: float\n",
    "                Apply weight_decay regularization with the AdamW optimizer\n",
    "\n",
    "            momentum: float\n",
    "                Apply momentum with the AdamW optimizer\n",
    "\n",
    "            targetloss: float (None)\n",
    "                terminates training when the validation loss drops below the targetloss.\n",
    "                \n",
    "            earlystop: int (False)\n",
    "                terminates training when the validation loss has not improved for the last\n",
    "                earlystop cycles.\n",
    "                \n",
    "            save_lowest: bool (False)\n",
    "                when the validation loss is lower than seen before, the model is \n",
    "                saved/committed as 'lowest' and can be checked out by calling \n",
    "                lowest() on the trainer.\n",
    "                \n",
    "            test: bool (False)\n",
    "                run the test set every cycle (used for n-fold cross validation)\n",
    "        \"\"\"\n",
    "        \n",
    "        self._scheduler_start = self.epochid # used by OneCycleScheduler\n",
    "        self._scheduler_epochs = epochs\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.del_optimizer()\n",
    "        self.lr = lr or self.lr\n",
    "        if weight_decay is not None and self.weight_decay != weight_decay:\n",
    "            self.weight_decay = weight_decay\n",
    "        if momentum is not None and self.momentum != momentum:\n",
    "            self.momentum = momentum\n",
    "        if optimizer and self._optimizerclass != optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        if scheduler is not False:\n",
    "            self.scheduler = scheduler\n",
    "        self.cycle = cycle or self.cycle\n",
    "        cyclesnotimproved = 0\n",
    "        lastvalidation = None\n",
    "        model = self.model\n",
    "        torch.set_grad_enabled(False)\n",
    "        maxepoch = self.epochid + epochs\n",
    "        epochspaces = int(math.log(maxepoch)/math.log(10)) + 1\n",
    "        if pbar is None:\n",
    "            if test:\n",
    "                self.currentpbar = tqdm_trainer(epochs, self.cycle, self.train_dl, self.valid_dl, self.test_dl, silent=silent)\n",
    "            else:\n",
    "                self.currentpbar = tqdm_trainer(epochs, self.cycle, self.train_dl, self.valid_dl, silent=silent)\n",
    "        else:\n",
    "            self.currentpbar = pbar\n",
    "        self._time()\n",
    "        for i in range(epochs):\n",
    "            self.epochid += 1\n",
    "            epochloss = 0\n",
    "            n = 0\n",
    "            epoch_y_pred = []\n",
    "            epoch_y = []\n",
    "            self.scheduler\n",
    "            report = (((i + 1) % self.cycle) == 0 or i == epochs - 1)\n",
    "            with self.train_mode:\n",
    "                for *X, y in self.train_Xy:\n",
    "                    loss, y_pred = self.train_batch(*X, y=y)\n",
    "                    self.scheduler.step()\n",
    "                    try:\n",
    "                        # TODO naam aanpassen\n",
    "                        y_pred = model.post_forward(y_pred)\n",
    "                    except: pass\n",
    "                    if report:\n",
    "                        epochloss += loss.item() * len(y_pred)\n",
    "                        n += len(y_pred)\n",
    "                        epoch_y_pred.append(to_numpy(y_pred))\n",
    "                        epoch_y.append(to_numpy(y))\n",
    "                    self.currentpbar.update(self.train_dl.batch_size)\n",
    "            if report:\n",
    "                epochloss /= n\n",
    "                epoch_y = np.concatenate(epoch_y, axis=0)\n",
    "                epoch_y_pred = np.concatenate(epoch_y_pred, axis=0)\n",
    "                metrics = self.evaluator._store_metrics(epoch_y, epoch_y_pred, phase='train', epoch=self.epochid, **log)\n",
    "                self.evaluator._store_metric('loss', epochloss, phase='train', epoch=self.epochid, **log)\n",
    "                validloss, metrics = self.validate(pbar = self.currentpbar, log=log)\n",
    "                if test:\n",
    "                    self._test(pbar = self.currentpbar, log=log)\n",
    "                if not silent:\n",
    "                    reportmetric = ''\n",
    "                    for m in self.metrics:\n",
    "                        m = m.__name__\n",
    "                        value = metrics[m]\n",
    "                        try:\n",
    "                            reportmetric += f'{m}={value:.5f} '\n",
    "                        except: pass\n",
    "                    print(f'{self.epochid:>{epochspaces}} {self._time():.2f}s trainloss={epochloss:.5f} validloss={validloss:.5f} {reportmetric}')\n",
    "                if save is not None and save:\n",
    "                    self.commit(f'{save}-{self.epochid}')\n",
    "                if save_lowest is not None and save_lowest:\n",
    "                    if self.lowest_score is None or validloss < self.lowest_score:\n",
    "                        self.lowest_score = validloss\n",
    "                        self.commit('lowest')\n",
    "                        \n",
    "                # Checking early termination\n",
    "                if targetloss is not None and validloss <= targetloss:\n",
    "                    try:\n",
    "                        self.currentpbar.finish_fold()\n",
    "                    except:\n",
    "                        pass\n",
    "                    if not silent:\n",
    "                        print('Early terminating because the validation loss reached the target.')\n",
    "                    break\n",
    "                if earlystop:\n",
    "                    if lastvalidation is None:\n",
    "                        lastvalidation = validloss\n",
    "                    else:\n",
    "                        if validloss < lastvalidation:\n",
    "                            cyclesnotimproved = 0\n",
    "                        else:\n",
    "                            cyclesnotimproved += 1\n",
    "                            if cyclesnotimproved >= earlystop:                                \n",
    "                                try:\n",
    "                                    self.currentpbar.finish_fold()\n",
    "                                except:\n",
    "                                    pass\n",
    "                                if not silent:\n",
    "                                    print(f'Early terminating because the validation loss has not improved the last {earlystop} cycles.')\n",
    "                                break\n",
    "        if pbar is None:\n",
    "            try:\n",
    "                self.currentpbar.close()    \n",
    "            except: pass\n",
    "    \n",
    "    def lowest(self):\n",
    "        \"\"\"\n",
    "        Checkout the model with the lowest validation loss, that was committed when training with save_lowest=True\n",
    "        \"\"\"\n",
    "        self.checkout('lowest')\n",
    "\n",
    "    def debug(self):\n",
    "        if self._debug:\n",
    "            try:\n",
    "                print('last X', self.lastx)\n",
    "            except: pass\n",
    "            try:\n",
    "                print('last y', self.lasty)\n",
    "            except: pass\n",
    "            try:\n",
    "                print('last model(y)', self.lastyfw)\n",
    "            except: pass\n",
    "            try:\n",
    "                print('last post_forward(model(y))', self.lastypfw)\n",
    "            except: pass\n",
    "        \n",
    "    def learning_curve(self, y='loss', series='phase', select=None, xlabel = None, ylabel = None, title=None, label_prefix='', **kwargs):\n",
    "        \"\"\"\n",
    "        Plot a learning curve with the train and valid loss on the y-axis over the epoch on the x-axis. \n",
    "        The plot is generated by the evaluator that logged training progress. By default the evaluator logs:\n",
    "        - epoch: the epoch number\n",
    "        - phase: 'train' or 'valid'\n",
    "        - loss: the weighted average loss\n",
    "        under the name of each metric function, the resulting value when called with (y, y_pred)\n",
    "        and the additional values that are passed to train() through the log parameter. \n",
    "        \n",
    "        Arguments:\n",
    "            y: str or function\n",
    "                the metric that is used for the y-axis. It has to be a metric that was collected during training.\n",
    "                if a function is passed, the name of the function is used.\n",
    "            series: str ('phase')\n",
    "                the label to use as a series. By default, 'phase' is used to plot both the train and valid results.\n",
    "            select: see evaluator.select\n",
    "                using the values 'train' and 'valid' you can select to plot only the train or valid sets.\n",
    "            xlabel: str\n",
    "                the label used on the x-axis\n",
    "            ylabel: str\n",
    "                the label used on the y-axis\n",
    "            title: str\n",
    "                the title of the plot\n",
    "            label_prefix: str\n",
    "                prefixes the label, so that you can combine a plot with results from different metrics or models\n",
    "            **kwargs: dict\n",
    "                forwarded to matplotlib's plot or scatter function\n",
    "        \"\"\"\n",
    "        return self.evaluator.line_metric(x='epoch', series=series, select=select, y=y, xlabel = xlabel, ylabel = ylabel, title=title, label_prefix=label_prefix, **kwargs)\n",
    "        \n",
    "    def validation_curve(self, y=None, x='epoch', series='phase', select=None, xlabel = None, ylabel = None, title=None, label_prefix='', **kwargs):\n",
    "        \"\"\"\n",
    "        Plot a metric for the train and valid set, over epoch on the x-axis. The plot is generated by the evaluator\n",
    "        that logged training progress. By default the evaluator logs:\n",
    "        - epoch: the epoch number\n",
    "        - phase: 'train' or 'valid'\n",
    "        - loss: the weighted average loss\n",
    "        under the name of each metric function, the resulting value when called with (y, y_pred)\n",
    "        and the additional values that are passed to train() through the log parameter. \n",
    "        \n",
    "        Arguments:\n",
    "            y: str or function\n",
    "                the metric that is used for the y-axis. It has to be a metric that was collected during training.\n",
    "                if a function is passed, the name of the function is used.\n",
    "            x: str ('epoch')\n",
    "                the label used for the x-axis.\n",
    "            series: str ('phase')\n",
    "                the label to use as a series. By default, 'phase' is used to plot both the train and valid results.\n",
    "            select: see evaluator.select\n",
    "                using the values 'train' and 'valid' you can select to plot only the train or valid sets.\n",
    "            xlabel: str\n",
    "                the label used on the x-axis\n",
    "            ylabel: str\n",
    "                the label used on the y-axis\n",
    "            title: str\n",
    "                the title of the plot\n",
    "            label_prefix: str\n",
    "                prefixes the label, so that you can combine a plot with results from different metrics or models\n",
    "            **kwargs: dict\n",
    "                forwarded to matplotlib's plot or scatter function\n",
    "        \"\"\"\n",
    "        if y is not None and type(y) != str:\n",
    "            y = y.__name__\n",
    "        return self.evaluator.line_metric(x=x, series=series, select=select, y=y, xlabel = xlabel, ylabel = ylabel, title=title, label_prefix=label_prefix, **kwargs)\n",
    "       \n",
    "    def freeze(self, last=-1):\n",
    "        \"\"\"\n",
    "        Mostly used for transfer learning, to freeze all parameters of a model, until the given layer (exclusive).\n",
    "        \n",
    "        Arguments:\n",
    "            last: int (-1)\n",
    "                Freeze all layers up to this layer number. -1 is the last layer.\n",
    "        \"\"\"\n",
    "        for c in list(self.model.children())[:last]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad=False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        \"\"\"\n",
    "        Mostly used for transfer learning, to unfreeze all parameters of a model.\n",
    "        \"\"\"\n",
    "        for c in list(self.model.children()):\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad=True\n",
    "\n",
    "    def study(self, storage=None, sampler=None, pruner=None, study_name=None, direction=None, load_if_exists=False, directions=None):\n",
    "        \"\"\"\n",
    "        Creates an (extended) Optuna Study to study how hyperparameters affect the given target function \n",
    "        when training a model. This call will just instantiate and return the study object. Typical use is to\n",
    "        first define a `trial` function, that will sample values to use as hyperparameters, instantiate and train a model,\n",
    "        and return the optimal validation scores using `trainer.optimum`. Then call `study.optimize(trail, n_trials=)`\n",
    "        to run the trial n_trial times. You can use `tuner.plot_hyperparameters()` to visualize the results, or any\n",
    "        optuna method.\n",
    "        \n",
    "        If you want to create a study without optimizing for loss first, `Study.create_study` allows you to\n",
    "        set the targets and directions.\n",
    "        \n",
    "        Arguments:\n",
    "            for the arguments, see create_study in the Optuna library\n",
    "            \n",
    "        Returns:\n",
    "            Study (which is a subclass of Optuna.study.Study)\n",
    "        \"\"\"\n",
    "        from .study import Study\n",
    "        return Study.create_study(self, storage=storage, sampler=sampler, pruner=pruner, study_name=study_name, direction=direction, load_if_exists=load_if_exists, directions=directions)\n",
    "    \n",
    "    def optimum(self, *target, direction=None, directions=None, select=None):\n",
    "        \"\"\"\n",
    "        Finds the cycle at which optimal results where obtained over the validation set, on the given optimization\n",
    "        metric. \n",
    "        \n",
    "        Arguments:\n",
    "            *target: str or callable ('loss')\n",
    "                names or metric functions that are used to decide what training cycle the model was most optimal\n",
    "            direction: str or [ str ] (None)\n",
    "                for every target: 'minimize' or 'maximize' to find the highest or lowest value on the given target\n",
    "                If None, 'minimize' is used when optimize is 'loss', otherwise 'maximize' is used\n",
    "                \n",
    "        Returns:\n",
    "            [ target ]\n",
    "            A list of target values \n",
    "        \"\"\"\n",
    "        if len(target) == 0:\n",
    "            target = ['loss'] + [ m.__name__ for m in self.metrics ]\n",
    "        else:\n",
    "            target = [ t.__name__ if callable(t) else t for t in target ]\n",
    "            for t in target:\n",
    "                try:\n",
    "                    assert t == 'loss' or t in { m.__name__ for m in self.metrics }, \\\n",
    "                        f'Target {t} should be loss or a metric that is registered for the trainer'\n",
    "                except:\n",
    "                    assert False, f'Exception comparing target {t} to the registered metrics of the trainer'\n",
    "        if direction is None and directions is None:\n",
    "            if len(target) > 1:\n",
    "                directions = [ 'minimize' if t == 'loss' else 'maximize' for t in target ]\n",
    "            else:\n",
    "                direction = 'minimize' if target[0] == 'loss' else 'maximize'\n",
    "        r = self.evaluator.optimum(*target, direction=direction, directions=directions, select=select)\n",
    "        return [ r[t] for t in target ]\n",
    "        \n",
    "    def plot_hyperparameters(self, figsize=None):\n",
    "        self.tuner.plot_hyperparameters(figsize)\n",
    "        \n",
    "    def tune_old(self, params,setter, lr=[1e-6, 1e-2], steps=40, smooth=0.05, label=None, **kwargs):\n",
    "        lr_values = exprange(*lr, steps)\n",
    "        if label is None:\n",
    "            label = str(setter)\n",
    "        if len(params) == 2:\n",
    "            params = range3(*params)\n",
    "        with tuner(self, lr_values, self.set_lr, smooth=0.05, label=label) as t:\n",
    "            t.run_multi(params, setter)\n",
    "\n",
    "    def tune_weight_decay_old(self, lr=[1e-6,1e-4], params=[1e-6, 1], steps=40, smooth=0.05, yscale='log', **kwargs):\n",
    "        self.tune( params, partial(self.set_optimizer_param, 'weight_decay'), lr=lr, steps=steps, smooth=smooth, label='weight decay', yscale=yscale, **kwargs)\n",
    "\n",
    "    def lr_find(self, lr=[1e-6, 10], steps=40, smooth=0.05, cache_valid=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Run a learning rate finder on the dataset (as propesed by Leslie Smith and implemented in FastAI). \n",
    "        This saves the model, then starting with a very low learning rate\n",
    "        iteratively trains the model on a single mini-batch and logs the loss on the validation set. Gradually, the\n",
    "        learning rate is raised. The idea is that the graph contains information on a stable setting of the learning\n",
    "        rate. This does not always work, and often after some training, if learning is not stable, the learning rate\n",
    "        still needs to be adjusted. \n",
    "        \n",
    "        The result is a plot of the validation loss over the change in learning rate.\n",
    "        \n",
    "        Arguments:\n",
    "            lr: [small float, big float] ([1e-6, 10])\n",
    "                Interval of learning rates to inspect\n",
    "            steps: int (40)\n",
    "                number of (exponential) steps to divide the learning rate interval in\n",
    "            smooth: float (0.05)\n",
    "                smoothing parameter, to generate a more readable graph\n",
    "            cache_valid: bool (True)\n",
    "                whether to keep the validation set if possible in memory. Switch of if there is insufficient memory\n",
    "        \"\"\"\n",
    "        with tuner(self, exprange(lr[0], lr[1], steps), self.set_lr, label='lr', yscale='log', smooth=smooth, cache_valid=cache_valid, **kwargs) as t:\n",
    "            t.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
