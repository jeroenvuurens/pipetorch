{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeTorch demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipetorch as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either load a DataFrame in Pandas and wrap it as a PTDataFrame, or use the `PTDataFrame.read_csv()` which does exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pt.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pt.wines().to_ptarray().add_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that you can use all available operations on DataFrames to prepare and clean your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,[0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mixed'] = df.alcohol * df.pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='residual sugar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='citric acid', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all goes well, you should get a PipeTorch version of the DataFrame every time. There is no need to use inplace, the functions will also return a PipeTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data\n",
    "\n",
    "To prepare the data for machine learning, we usually split the data in a training and validation part. This allows for cross validation. Optionally we can also create a separate test part, which allows to evaluate over data that also was not used for model optimization (which is what you will probably use the validation part for). Splitting the dataset is mostly done before anything else, because other operations like scaling and balancing should be fit on the training set only.\n",
    "\n",
    "- `split(fractions, shuffle=False)`: You can split the data using the `split()` method. When you pass a single fraction, that will used for the validation part and the remainder will be in the training set. Alternatively, you can supply a tuple of two values. In that case, the first fraction represents the size of the validation set, the second fraction the size of the tes set and the remainder will we in the training set. `split()` is a non-destructive operation that returns a split version of a deep copy of the DataFrame and leaving the original DataFrame as is. By default, the dataset is split randomly. You can turn this off by passing `shuffle=False` to `split()`\n",
    "\n",
    "Important: initially it will appear as of nothing happened in your DataFrame. However, this is not the case. In the background, lists of row numbers are assigned to resp. train_indices, valid_indices and test_indices, and these indices will be used in future operations. There is no need to address the train_indices etc. directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.split(0.2).interpolate('alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Numpy Arrays\n",
    "\n",
    "Under the hood, converting the data to Numpy Arrays proceeds in two steps. First the PTDataFrame is converted to a PTArray, which is a subclass of Numpy's ndarray that preserves the preprocessing information (i.e. split) that we have just made on the PTDataFrame. and provides functions to turn the Numpy Array into either split Numpy arrays or \n",
    "\n",
    "- `to_arrays()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
