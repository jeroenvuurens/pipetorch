{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting perceptron.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile perceptron.py\n",
    "from torchvision.models import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "identity=lambda x:x\n",
    "\n",
    "class SingleLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input, output, last_activation=identity):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input, output)\n",
    "        self.a1 = last_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.a1(self.w1(x))\n",
    "        #return pred_y.view(-1)\n",
    "    \n",
    "class SingleLayerPerceptron_BinaryClass(SingleLayerPerceptron):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__(input, output, nn.Sigmoid())\n",
    "\n",
    "    def post_forward(self, y):\n",
    "        return (y > 0.5).float()\n",
    "\n",
    "class SingleLayerPerceptron_MultiClass(SingleLayerPerceptron):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__(input, output, nn.LogSoftmax(dim=1))\n",
    "\n",
    "def flatten_r_image(x):\n",
    "        return  x[:,0,:,:].view(x.shape[0], -1)\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that implements a generic MultiLayerPerceptron\n",
    "    \n",
    "    Args:\n",
    "        *with: int\n",
    "            Sequence of at least two ints, that provide the widths for all the layers in the network.\n",
    "            The first width should match the number of input features, the last width should match the\n",
    "            numbetr of target variables (usually 1).\n",
    "            \n",
    "        preprocess: func (identity)\n",
    "            function that will be used on the input first. The default means no preprocessing\n",
    "            \n",
    "        inner_activation: func (nn.ReLU())\n",
    "            activation function that is used on all layers except the output\n",
    "            \n",
    "        drop_prob: float (None)\n",
    "            if provided, Dropout layers are added in between all layers but the last, with a fixed\n",
    "            dropout probability.\n",
    "            \n",
    "        last_activation: func (None)\n",
    "            the activation function used on the last layer. The most common choices are None for regression\n",
    "            nn.Sigmoid() for binary classification and nn.Softmax() for multi-label classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *width, preprocess=identity, inner_activation=nn.ReLU(), drop_prob=None, last_activation=None):\n",
    "        super().__init__()\n",
    "        self.actions = [preprocess]\n",
    "        for n, (i, o) in enumerate(zip(width[:-1], width[1:])):\n",
    "            l = nn.Linear(i, o)\n",
    "            self.actions.append(l)\n",
    "            self.__setattr__(f'w{n+1}', l)\n",
    "            if n < len(width) - 2:\n",
    "                if drop_prob is not None:\n",
    "                    self.actions.append(nn.Dropout(p=drop_prob))\n",
    "                    self.__setattr__(f'drop{n+1}', self.actions[-1])\n",
    "                self.actions.append(inner_activation)\n",
    "                self.__setattr__(f'activation{n+1}', self.actions[-1])\n",
    "            elif last_activation is not None:\n",
    "                self.actions.append(last_activation)\n",
    "                self.__setattr__(f'activation{n+1}', self.actions[-1])\n",
    "        #if width[-1] == 1:\n",
    "        #    self.reshape = (-1)\n",
    "        #else:\n",
    "        #    self.reshape = (-1, width[-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for a in self.actions:\n",
    "            x = a(x)\n",
    "        return x #.view(self.reshape)\n",
    "\n",
    "class MultiLayerPerceptron_BinaryClass(MultiLayerPerceptron):\n",
    "    def __init__(self, *width, preprocess=identity, inner_activation=nn.ReLU(), drop_prob=None):\n",
    "        super().__init__(*width, preprocess=preprocess, inner_activation=inner_activation, drop_prob=drop_prob, last_activation=nn.Sigmoid())\n",
    "\n",
    "    def post_forward(self, y):\n",
    "        return (y > 0.5).float()\n",
    "\n",
    "class MultiLayerPerceptron_MultiClass(MultiLayerPerceptron):\n",
    "    def __init__(self, *width, preprocess=identity, inner_activation=nn.ReLU(), drop_prob=None):\n",
    "        super().__init__(*width, preprocess=preprocess, inner_activation=inner_activation, drop_prob=drop_prob)\n",
    "        \n",
    "    def post_forward(self, y):\n",
    "        return torch.argmax(y, axis=1)\n",
    "        \n",
    "class TwoLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input, hidden, output, last_activation=None):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input, hidden)\n",
    "        self.a1 = nn.ReLU()\n",
    "        self.w2 = nn.Linear(hidden, output)\n",
    "        if last_activation:\n",
    "            self.a2 = last_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.w1(x))\n",
    "        pred_y = self.a2(self.w2(x))\n",
    "        return pred_y #.view(-1)\n",
    "\n",
    "    def post_forward(self, y):\n",
    "        return y \n",
    "\n",
    "class TwoLayerPerceptron_BinaryClass(TwoLayerPerceptron):\n",
    "    def __init__(self, input, hidden, output):\n",
    "        super().__init__(input, hidden, output, last_activation=nn.Sigmoid())\n",
    "\n",
    "    def post_forward(self, y):\n",
    "        return (y > 0.5).float()\n",
    "\n",
    "class TwoLayerPerceptron_MultiClass(TwoLayerPerceptron):\n",
    "    def __init__(self, input, hidden, output):\n",
    "        super().__init__(input, hidden, output, last_activation=nn.LogSoftmax(dim=1))\n",
    "\n",
    "def zero_embedding(rows, columns):\n",
    "    e = nn.Embedding(rows, columns)\n",
    "    e.weight.data.zero_()\n",
    "    return e\n",
    "\n",
    "class factorization(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding( n_users,n_factors)\n",
    "        self.item_factors = nn.Embedding( n_items,n_factors)\n",
    "        self.user_bias = zero_embedding( n_users, 1)\n",
    "        self.item_bias = zero_embedding( n_items, 1)\n",
    "        self.fc = nn.Linear(n_factors, 4)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        user = X[:,0] - 1\n",
    "        item = X[:,1] - 1\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1) + self.user_bias(user).squeeze() + self.item_bias(item).squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
