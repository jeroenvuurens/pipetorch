{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "computational-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ptdataframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ptdataframe.py        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import path\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "from .evaluate import Evaluator\n",
    "from .databunch import Databunch\n",
    "from .ptdataset import PTDataSet\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "def get_filename(url):\n",
    "    fragment_removed = url.split(\"#\")[0]  # keep to left of first #\n",
    "    query_string_removed = fragment_removed.split(\"?\")[0]\n",
    "    scheme_removed = query_string_removed.split(\"://\")[-1].split(\":\")[-1]\n",
    "    if scheme_removed.find(\"/\") == -1:\n",
    "        filename = scheme_removed\n",
    "    else:\n",
    "        filename = os.path.basename(scheme_removed)\n",
    "    if '.' in filename:\n",
    "        filename = filename.rsplit( \".\", 1 )[ 0 ] + '.csv'\n",
    "    return filename\n",
    "\n",
    "def read_excel(path, alternativesource=None, sep=None, delimiter=None, **kwargs):\n",
    "    filename = get_filename(path)\n",
    "    if (Path.home() / '.pipetorchuser' / filename).is_file():\n",
    "        return PTDataFrame.read_csv(Path.home() / '.pipetorchuser' / filename, **kwargs)\n",
    "    if (Path.home() / '.pipetorch' / filename).is_file():\n",
    "        return PTDataFrame.read_csv(Path.home() / '.pipetorch' / filename, **kwargs)\n",
    "    if alternativesource:\n",
    "        df = pd.read_excel(alternativesource())\n",
    "    else:\n",
    "        print('Downloading new file ' + path)\n",
    "        df = pd.read_excel(path, **kwargs)\n",
    "        df.columns = df.columns.str.replace(' ', '') \n",
    "    (Path.home() / '.pipetorchuser').mkdir(exist_ok=True)\n",
    "    df.to_csv(Path.home() / '.pipetorchuser' / filename, index=False)\n",
    "    return PTDataFrame(df)\n",
    "\n",
    "def read_csv(path, alternativesource=None, sep=None, delimiter=None, **kwargs):\n",
    "    if sep:\n",
    "        kwargs['sep'] = sep\n",
    "    elif delimiter:\n",
    "        kwargs['delimiter'] = delimiter\n",
    "    filename = get_filename(path)\n",
    "    if (Path.home() / '.pipetorchuser' / filename).is_file():\n",
    "        #print(str(Path.home() / '.pipetorchuser' / filename))\n",
    "        return PTDataFrame.read_csv(Path.home() / '.pipetorchuser' / filename, **kwargs)\n",
    "    if (Path.home() / '.pipetorch' / filename).is_file():\n",
    "        #print(str(Path.home() / '.pipetorch' / filename))\n",
    "        return PTDataFrame.read_csv(Path.home() / '.pipetorch' / filename, **kwargs)\n",
    "    if alternativesource:\n",
    "        df = PTDataFrame(alternativesource())\n",
    "    else:\n",
    "        print('Downloading new file ' + path)\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "    (Path.home() / '.pipetorchuser').mkdir(exist_ok=True)\n",
    "    df.to_csv(Path.home() / '.pipetorchuser' / filename, index=False)\n",
    "    return df\n",
    "\n",
    "class PTDataFrame(pd.DataFrame):\n",
    "    _metadata = ['_pt_scale_columns', '_pt_scale_omit_interval', '_pt_scalertype', '_pt_train_indices', '_pt_train_indices_unbalanced', '_pt_valid_indices', '_pt_test_indices', '_pt_columny', '_pt_transposey', '_pt_bias', '_pt_polynomials']\n",
    "\n",
    "    _internal_names = pd.DataFrame._internal_names + ['_pt__scale_columns', '_pt__train', '_pt__valid', '_pt__test', '_pt__full', '_pt__train_unbalanced', '_pt__scalerx', '_pt__scalery', '_pt__train_x', '_pt__train_y', '_pt__valid_x', '_pt__valid_y']\n",
    "    _internal_names_set = set(_internal_names)\n",
    "\n",
    "    @classmethod\n",
    "    def read_csv(cls, path, **kwargs):\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        return cls(df)\n",
    "\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(data, **kwargs)\n",
    "        if type(data) != self._constructor:\n",
    "            for m in self._metadata:\n",
    "                self.__setattr__(m, None)\n",
    "    \n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTDataFrame\n",
    "\n",
    "    @property\n",
    "    def _columny(self):\n",
    "        try:\n",
    "            if len(self._pt_columny) > 0:\n",
    "                return self._pt_columny\n",
    "        except: pass\n",
    "        return [ self.columns[-1] ]\n",
    "        \n",
    "    @property\n",
    "    def _transposey(self):\n",
    "        return self._pt_transposey\n",
    "            \n",
    "    @property\n",
    "    def _columnx(self):\n",
    "        return [ c for c in self.columns if c not in self._columny ]\n",
    "   \n",
    "    def _scalerx(self):\n",
    "        try:\n",
    "            if self._pt__scalerx is not None:\n",
    "                return self._pt__scalerx\n",
    "        except: pass\n",
    "        X = self.train._x_polynomials\n",
    "        if self._pt_scale_columns == True or self._pt_scale_columns == 'x_only':         \n",
    "            self._pt__scalerx = tuple( self._create_scaler(self._pt_scalertype, X[:, i:i+1]) if (X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1]) else None for i in range(X.shape[1]) )    \n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            self._pt__scalerx = tuple( [ None ] * X.shape[1] )\n",
    "        else:\n",
    "            self._pt__scalerx = tuple( self._create_scaler(self._pt_scalertype, X[:, i:i+1]) if c in self._pt_scale_columns else None for i, c in enumerate(self._columnx) )\n",
    "        return self._pt__scalerx\n",
    "        \n",
    "    def _scalery(self):\n",
    "        try:\n",
    "            if self._pt__scalery is not None:\n",
    "                return self._pt__scalery\n",
    "        except: pass\n",
    "        y = self.train._y_numpy\n",
    "        if self._pt_scale_columns == True:         \n",
    "            self._pt__scalery = tuple( self._create_scaler(self._pt_scalertype, y[:, i:i+1]) if (y[:,i].min() < self._pt_scale_omit_interval[0] or y[:,i].max() > self._pt_scale_omit_interval[1]) else None for i in range(y.shape[1]) )    \n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0 or self._pt_scale_columns == 'x_only':\n",
    "            self._pt__scalery = tuple( [None] * y.shape[1] )\n",
    "        else:\n",
    "            self._pt__scalery = tuple( self._create_scaler(self._pt_scalertype, y[:, i:i+1]) if c in self._pt_scale_columns else None for i, c in enumerate(self._columny) )\n",
    "        return self._pt__scalery\n",
    "        \n",
    "    def columny(self, columns=None, transpose=True):\n",
    "        r = copy.deepcopy(self)\n",
    "        if columns is not None:\n",
    "            r._pt_columny = [columns] if type(columns) == str else columns\n",
    "        if transpose is not None:\n",
    "            r._pt_transposey = transpose\n",
    "        return r\n",
    "   \n",
    "    @property\n",
    "    def _indices(self):\n",
    "        return np.where(self.notnull().all(1))[0]\n",
    "\n",
    "    @property\n",
    "    def _train_indices(self):\n",
    "        if self._pt_train_indices is None:\n",
    "            return self._indices\n",
    "        return self._pt_train_indices\n",
    "\n",
    "    @property\n",
    "    def _valid_indices(self):\n",
    "        if self._pt_valid_indices is None:\n",
    "            return [ ]\n",
    "        return self._pt_valid_indices\n",
    "\n",
    "    @property\n",
    "    def _test_indices(self):\n",
    "        if self._pt_test_indices is None:\n",
    "            return [ ]\n",
    "        return self._pt_test_indices\n",
    "\n",
    "    @property\n",
    "    def _train_indices_unbalanced(self):\n",
    "        if self._pt_train_indices_unbalanced is None:\n",
    "            return self._train_indices\n",
    "        return self._pt_train_indices_unbalanced\n",
    " \n",
    "    def to_dataset(self):\n",
    "        \"\"\"\n",
    "        returns: a list with a train, valid and test DataSet. Every DataSet contains an X and y, where the \n",
    "        input data matrix X contains all columns but the last, and the target y contains the last column\n",
    "        columns: list of columns to convert, the last column is always the target. default=None means all columns.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        p = [ TensorDataset(torch.from_numpy(self.train.X), torch.from_numpy(self.train.y)) ]\n",
    "        p.append( TensorDataset(torch.from_numpy(self.valid.X), torch.from_numpy(self.valid.y)) )\n",
    "        p.append( TensorDataset(torch.from_numpy(self.test.X), torch.from_numpy(self.test.y)) )\n",
    "        return p\n",
    "    \n",
    "    def to_databunch(self, batch_size=32, num_workers=0, shuffle=True, pin_memory=False, balance=False):\n",
    "        \"\"\"\n",
    "        returns: a Databunch that contains dataloaders for the train, valid and test part.\n",
    "        batch_size, num_workers, shuffle, pin_memory: see Databunch/Dataloader constructor\n",
    "        \"\"\"\n",
    "        return Databunch(*self.to_dataset(), batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=pin_memory, scaler=self, balance=balance)    \n",
    "\n",
    "    def evaluate(self, *metrics):\n",
    "        #assert len(metrics) > 0, 'You need to provide at least one metric for the evaluation'\n",
    "        return Evaluator(self, metrics)\n",
    "    \n",
    "    def _ptdataset(self, data):\n",
    "        r = PTDataSet.from_ptdataframe(data, self)\n",
    "        r._pt_scalerx = self._scalerx\n",
    "        r._pt_scalery = self._scalery\n",
    "        r._pt_columny = self._columny\n",
    "        r._pt_transposey = self._transposey\n",
    "        r._pt_polynomials = self._pt_polynomials\n",
    "        r._pt_bias = self._pt_bias\n",
    "        return r\n",
    "\n",
    "    def _ptdataframe(self, data):\n",
    "        r = self._constructor(data)\n",
    "        r._pt__scalerx = self._scalerx()\n",
    "        r._pt__scalery = self._scalery()\n",
    "        r._pt_columny = self._columny\n",
    "        r._pt_transposey = self._transposey\n",
    "        r._pt_polynomials = self._pt_polynomials\n",
    "        r._pt_bias = self._pt_bias\n",
    "        return r  \n",
    "    \n",
    "    @classmethod\n",
    "    def _split_indices(self, indices, split):\n",
    "        length = len(indices)\n",
    "        try:\n",
    "            train_length = int((1 - sum(split))* length)\n",
    "            train_valid_length = int((1 - split[1])* length)\n",
    "            assert train_length >= 0, 'Non positive size of the training set, provide fractions for valid/test part, e.g. (0.2, 0.3)'\n",
    "            assert train_valid_length >= train_length, 'Non positive size of the validation set, provide fraction for valid part, bigger than 0, e.g. e.g. (0.2, 0.3)'\n",
    "            assert length >= train_valid_length, 'Negative fraction of the test set, provide fractions for valid/test part, e.g. (0.2, 0.3)'\n",
    "            train_indices = indices[:train_length]\n",
    "            valid_indices = indices[train_length:train_valid_length]\n",
    "            test_indices = indices[train_valid_length:]\n",
    "        except:\n",
    "            train_length = int((1 - split)* length)\n",
    "            assert train_length >= 0, 'Non positive size of the training set, provide fraction for valid part, smaller than 1, e.g. 0.2'\n",
    "            assert train_length <= length, 'Non positive size of the validation set, provide fraction for valid part, bigger than 0, e.g. 0.2'\n",
    "            train_indices = indices[:train_length]\n",
    "            valid_indices = indices[train_length:]\n",
    "            test_indices = []\n",
    "        return train_indices, valid_indices, test_indices\n",
    "\n",
    "    def split(self, split=0.2, shuffle=True, random_state=None):\n",
    "        assert len(self._train_indices_unbalanced) == len(self._train_indices), \"Split the DataFrame before balancing!\"\n",
    "        r = copy.deepcopy(self)\n",
    "        indices = r._indices\n",
    "        if shuffle:\n",
    "            if random_state is not None:\n",
    "                np.random.seed(random_state)\n",
    "            np.random.shuffle(indices)\n",
    "            if random_state is not None:\n",
    "                t = 1000 * time.time() # current time in milliseconds\n",
    "                np.random.seed(int(t) % 2**32)\n",
    "        r._pt_train_indices, r._pt_valid_indices, r._pt_test_indices = r._split_indices(indices, split)\n",
    "        r._pt_train_indices_unbalanced = r._pt_train_indices\n",
    "        return r\n",
    "    \n",
    "    def resample(self, n=True):\n",
    "        r = copy.copy(self)\n",
    "        if n == True:\n",
    "            n = len(r._pt_train_indices_unbalanced)\n",
    "        if n < 1:\n",
    "            n = n * len(r._pt_train_indices_unbalanced)\n",
    "        if n > 0:\n",
    "            r._pt_train_indices = resample(r._pt_train_indices_unbalanced, n_samples = int(n))\n",
    "        return r\n",
    "    \n",
    "    def polynomials(self, degree):\n",
    "        #assert not self._check_attr('_bias'), 'You should not add a bias before polynomials, rather use include_bias=True'\n",
    "        assert type(self._pt_scale_columns) != list or len(self._pt_scale_columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_polynomials = PolynomialFeatures(degree, include_bias=False)\n",
    "        try:\n",
    "            del r._pt_scalerx\n",
    "            del r._pt_scalery\n",
    "        except: pass\n",
    "        return r\n",
    "    \n",
    "    def from_numpy(self, x):\n",
    "        if x.shape[1] == len(self._columnx) + len(self._columny):\n",
    "            y = x[:,-len(self._columny):]\n",
    "            x = x[:,:-len(self._columny)]\n",
    "        elif x.shape[1] == len(self._columnx):\n",
    "            y = np.zeros((len(x), len(self._columny)))\n",
    "        else:\n",
    "            raise ValueError('x must either have as many columns in x or the entire df')\n",
    "        series = [ pd.Series(s.reshape(-1), name=c) for s, c in zip(x.T, self._columnx)]\n",
    "        series.extend([ pd.Series(s.reshape(-1), name=c) for s, c in zip(y.T, self._columny) ] )\n",
    "        df = pd.concat(series, axis=1)\n",
    "        return self._ptdataset(df)\n",
    "    \n",
    "    def from_list(self, x):\n",
    "        return self.from_numpy(np.array(x))\n",
    "    \n",
    "    def add_bias(self):\n",
    "        r = copy.copy(self)\n",
    "        r._pt_bias = True\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def train_unbalanced(self):\n",
    "        try:\n",
    "            return self._pt__train_unbalanced\n",
    "        except:\n",
    "            self._pt__train_unbalanced = self._ptdataset(self.iloc[self._train_indices_unbalanced])\n",
    "        return self._pt__train_unbalanced\n",
    "\n",
    "    @property\n",
    "    def full(self):\n",
    "        try:\n",
    "            return self._pt__full\n",
    "        except:\n",
    "            self._pt__full = self._ptdataset(self.iloc[np.concatenate([self._train_indices, self._valid_indices])])\n",
    "        return self._pt__full\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        try:\n",
    "            return self._pt__train\n",
    "        except:\n",
    "            self._pt__train = self._ptdataset(self.iloc[self._train_indices])\n",
    "        return self._pt__train\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        try:\n",
    "            return self._pt__valid\n",
    "        except:\n",
    "            self._pt__valid = self._ptdataset(self.iloc[self._valid_indices])\n",
    "        return self._pt__valid\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        try:\n",
    "            return self._pt__test\n",
    "        except:\n",
    "            self._pt__test = self._ptdataset(self.iloc[self._test_indices])\n",
    "        return self._pt__test\n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        try:\n",
    "            return self._pt__train_x\n",
    "        except:\n",
    "            self._pt__train_x = self.train.X\n",
    "        return self._pt__train_x\n",
    "            \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        try:\n",
    "            return self._pt__train_y\n",
    "        except:\n",
    "            self._pt__train_y = self.train.y\n",
    "        return self._pt__train_y\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        try:\n",
    "            return self._pt__valid_x\n",
    "        except:\n",
    "            self._pt__valid_x = self.valid.X\n",
    "        return self._pt__valid_x\n",
    "       \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        try:\n",
    "            return self._pt__valid_y\n",
    "        except:\n",
    "            self._pt__valid_y = self.valid.y\n",
    "        return self._pt__valid_y\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test.X\n",
    "            \n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test.y\n",
    "    \n",
    "    @property\n",
    "    def full_X(self):\n",
    "        return self.full.X\n",
    "            \n",
    "    @property\n",
    "    def full_y(self):\n",
    "        return self.full.y\n",
    "    \n",
    "    def scale(self, columns=True, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        if self._pt_polynomials and columns != 'x_only':\n",
    "            assert type(columns) != list or len(columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_scale_columns = columns\n",
    "        r._pt_scalertype = scalertype\n",
    "        r._pt_scale_omit_interval = omit_interval\n",
    "        try:\n",
    "            del r._pt_scalerx\n",
    "            del r._pt_scalery\n",
    "        except: pass\n",
    "        return r\n",
    "\n",
    "    def scalex(self, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        return self.scale(columns='x_only', scalertype=scalertype, omit_interval=omit_interval)\n",
    "    \n",
    "    def loss_surface(self, model, loss, **kwargs):\n",
    "        self.evaluate(loss).loss_surface(model, loss, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_scaler(scalertype, column):\n",
    "        scaler = scalertype()\n",
    "        scaler.fit(column)\n",
    "        return scaler\n",
    "\n",
    "    def inverse_transform_y(self, y):\n",
    "        y = to_numpy(y)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1,1)\n",
    "        return pd.DataFrame(self._inverse_transform(to_numpy(y), self._scalery(), self._columny))\n",
    "\n",
    "    def inverse_transform_X(self, X):\n",
    "        if self._pt_bias:\n",
    "            X = X[:, 1:]\n",
    "        if self._pt_polynomials is not None:\n",
    "            X = X[:, :len(self._columnx)]\n",
    "        return self._inverse_transform(to_numpy(X), self._scalerx()[:len(self._columnx)], self._columnx)\n",
    "\n",
    "    def _inverse_transform(self, data, scalerlist, columns):\n",
    "        data = to_numpy(data)\n",
    "        if scalerlist is not None:\n",
    "            data = [ data[:, i:i+1] if scaler is None else scaler.inverse_transform(data[:,i:i+1]) for i, scaler in enumerate(scalerlist) ]\n",
    "        series = [ pd.Series(x.reshape(-1), name=c) for x, c in zip(data, columns)]\n",
    "        return pd.concat(series, axis=1)\n",
    "\n",
    "    def inverse_transform(self, X, y):\n",
    "        y = self.inverse_transform_y(y)\n",
    "        X = self.inverse_transform_X(X)\n",
    "        return self._ptdataset(pd.concat([X, y], axis=1))\n",
    "    \n",
    "    def balance(self, weights=None):\n",
    "        if weights is None:\n",
    "            return self._balance_y_equal()\n",
    "        r = copy.deepcopy(self)\n",
    "        y = r.train[r._columny]\n",
    "        indices = {l:np.where(y==l)[0] for l in np.unique(y)}\n",
    "        classlengths = {l:len(i) for l,i in indices}\n",
    "        n = max([ int(math.ceil(classlength[c] / w)) for c, w in weights.items() ])\n",
    "        mask = np.hstack([np.random.choice(i, n*weights[l]-classlength[l], replace=True) for l, i in indices.items()])\n",
    "        indices = np.hstack([mask, range(len(y))])\n",
    "        r._pt_train_indices = r._train_indices_unbalanced[indices]\n",
    "        return r\n",
    "    \n",
    "    def _balance_y_equal(self):\n",
    "        r = copy.deepcopy(self)\n",
    "        y = r.train[r._columny]\n",
    "        indices = [np.where(y==l)[0] for l in np.unique(y)]\n",
    "        classlengths = [len(i) for i in indices]\n",
    "        n = max(classlengths)\n",
    "        mask = np.hstack([np.random.choice(i, n-l, replace=True) for l,i in zip(classlengths, indices)])\n",
    "        indices = np.hstack([mask, range(len(y))])\n",
    "        r._pt_train_indices = r._train_indices_unbalanced[indices]\n",
    "        return r\n",
    "    \n",
    "    def plot_boundary(self, predict):\n",
    "        self.evaluate().plot_boundary(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-graphics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-friday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
