{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipetorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipetorch.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pipetorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pipetorch.py\n",
    "\n",
    "def to_numpy(X):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pipetorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pipetorch.py\n",
    "\n",
    "class PTDataFrame(pd.DataFrame):\n",
    "    _metadata = ['_pt_scaler', '_pt_indices', '_train_indices', '_valid_indices', '_test_indices']\n",
    "\n",
    "    @classmethod\n",
    "    def read_csv(cls, *args, sep=',', **kwargs):\n",
    "        df = pd.read_csv(*args, sep=sep, **kwargs)\n",
    "        return cls(df)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTDataFrame\n",
    "\n",
    "    def _copy_meta(self, df):\n",
    "        for attr in self._constructor._metadata:\n",
    "            if hasattr(self, attr):\n",
    "                df.__dict__[attr] = getattr(self, attr)\n",
    "        return df\n",
    "\n",
    "    def _check_list_attr(self, attr):\n",
    "        try:\n",
    "            return hasattr(self, attr) and len(getattr(self, attr)) > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _pt_scaler_exists(self):\n",
    "        return self._check_list_attr('_pt_scaler')\n",
    "    \n",
    "    def _train_indices_exists(self):\n",
    "        return self._check_list_attr('_train_indices')\n",
    "    \n",
    "    def to_array(self, columns=None, ycolumns=1, dtype=np.float32):\n",
    "        df = self[columns] if columns else self\n",
    "        X = PTArray(df.to_numpy().astype(dtype))\n",
    "        X.__array_finalize__(self)\n",
    "        X.ycolumns = ycolumns\n",
    "        return X\n",
    "    \n",
    "    def to_dataset(self, columns=None, ycolumns=1, dtype=np.float32):\n",
    "        return self.to_array(columns, ycolumns, dtype).to_dataset()\n",
    "        \n",
    "    def to_arrays(self, columns=None, ycolumns=1, dtype=np.float32):\n",
    "        return self.to_array(columns, ycolumns, dtype).to_arrays()\n",
    "    \n",
    "    @property\n",
    "    def pt_indices(self):\n",
    "        return self._pt_indices\n",
    "    \n",
    "    @property\n",
    "    def train_indices(self):\n",
    "        return self._train_indices\n",
    "    \n",
    "    @property\n",
    "    def valid_indices(self):\n",
    "        return self._valid_indices\n",
    "    \n",
    "    @property\n",
    "    def test_indices(self):\n",
    "        return self._test_indices\n",
    "\n",
    "    @property\n",
    "    def pt_scaler(self):\n",
    "        return self._pt_scaler\n",
    "\n",
    "    @pt_indices.setter\n",
    "    def pt_indices(self, value):\n",
    "        self.__dict__['_pt_indices'] = value\n",
    "\n",
    "    @train_indices.setter\n",
    "    def train_indices(self, value):\n",
    "        self.__dict__['_train_indices'] = value\n",
    "\n",
    "    @valid_indices.setter\n",
    "    def valid_indices(self, value):\n",
    "        self.__dict__['_valid_indices'] = value\n",
    "\n",
    "    @test_indices.setter\n",
    "    def test_indices(self, value):\n",
    "        self.__dict__['_test_indices'] = value\n",
    "\n",
    "    @pt_scaler.setter\n",
    "    def pt_scaler(self, value):\n",
    "        self.__dict__['_pt_scaler'] = value\n",
    "\n",
    "    def split(self, split=0.2, shuffle=True):\n",
    "        r = copy.deepcopy(self)\n",
    "        r.pt_indices = np.where(r.iloc[:,-1].notnull())[0]\n",
    "        if shuffle:\n",
    "            np.random.shuffle(r.pt_indices)\n",
    "        r.train_indices, r.valid_indices, r.test_indices = r._split_indices(r.pt_indices, split)\n",
    "        return r\n",
    "    \n",
    "    def polynomials(self, degree):\n",
    "        return self.to_array().polynomials(degree)\n",
    "    \n",
    "    def scale(self, columns=True, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        assert self._train_indices_exists(), \"Split the DataFrame before scaling!\"\n",
    "        assert not self._pt_scaler_exists(), \"Trying to scale twice, which is a really bad idea!\"\n",
    "        if columns == True:\n",
    "            columns = self._guess_columns(omit_interval=omit_interval)\n",
    "        elif columns == False:\n",
    "            columns == []\n",
    "        r = copy.deepcopy(self)\n",
    "        r.pt_scaler = tuple( self._create_scaler(scalertype, s) if c in columns else None for c, s in self.items() )\n",
    "        return r.transform(self)\n",
    "    \n",
    "    def _guess_columns(df, omit_interval=(-2, 2)):\n",
    "        return [c for c, series in df.items() if series.dtype.kind == 'f' and (series.min() < omit_interval[0] or series.max() > omit_interval[1])]    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_scaler(scalertype, series):\n",
    "        scaler = scalertype()\n",
    "        if len(series.shape) == 1:\n",
    "            series = series.to_numpy().reshape(-1, 1)\n",
    "        scaler.fit(series)\n",
    "        return scaler\n",
    "\n",
    "    def transform(self, df):\n",
    "        out = []\n",
    "        for (c, series), scaler in zip(df.items(), self.pt_scaler):\n",
    "            if scaler is not None:\n",
    "                scaled = scaler.transform(series.to_numpy().reshape(-1,1))\n",
    "                out.append(pd.Series(scaled.reshape(-1), index=df.index, name=c))\n",
    "            else:\n",
    "                out.append(series)\n",
    "        return self._copy_meta(self._constructor(pd.concat(out, axis=1)))\n",
    "\n",
    "    def inverse_transform_y(self, y):\n",
    "        yscaler = self.pt_scaler[-1]\n",
    "        if yscaler is not None:\n",
    "            y = to_numpy(y)\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "                y = yscaler.inverse_transform(y)\n",
    "                y = y.reshape(-1)\n",
    "            else:\n",
    "                y = yscaler.inverse_transform(y)\n",
    "        return y\n",
    "\n",
    "    def inverse_transform_X(self, X):\n",
    "        X = to_numpy(X)\n",
    "        transform = [ X[i] if self.pt_scaler[i] is None else self.pt_scaler[i].inverse_transform(X[:,i]) for i in range(X.shape[1]) ]\n",
    "        transform = [ pd.Series(x, name=c) for x, c in zip(transform, self.columns[:-1])]\n",
    "        return pd.concat(transform, axis=1)\n",
    "\n",
    "    def inverse_transform(self, X, y):\n",
    "        y = self.inverse_transform_y(y)\n",
    "        X = self.inverse_transform_X(X)\n",
    "        return pd.concat([X, y], axis=1)\n",
    "    \n",
    "    def balance(self, weights=None):\n",
    "        assert self._train_indices_exists(), \"You have to split the DataFrame first\"\n",
    "        if weights is None:\n",
    "            return self.balance_y_equal()\n",
    "        y = self[[self.columns[-1]]]\n",
    "        y = y.iloc[self.train_indices]\n",
    "        indices = {l:np.where(y==l)[0] for l in np.unique(y)}\n",
    "        classlengths = {l:len(i) for l,i in indices}\n",
    "        n = max([ int(math.ceil(classlength[c] / w)) for c, w in weights.items() ])\n",
    "        mask = np.hstack([np.random.choice(i, n*weights[l]-classlength[l], replace=True) for l, i in indices.items()])\n",
    "        indices = np.hstack([mask, range(len(y))])\n",
    "        self.train_indices = self.train_indices[indices]\n",
    "        return self\n",
    "\n",
    "    def inverse_transform_y(self, y):\n",
    "        return self.pt_scaler.inverse_transform_y(to_numpy(y))\n",
    "\n",
    "    def inverse_transform_x(self, X):\n",
    "        return self.pt_scaler.inverse_transform_X(to_numpy(X))\n",
    "\n",
    "    def inverse_transform(self, X, y):\n",
    "        return self.pt_scaler.inverse_transform( to_numpy(X), to_numpy(y) )\n",
    "    \n",
    "    def balance_y_equal(self):\n",
    "        assert self._train_indices_exists(), \"You have to split the DataFrame first\"\n",
    "        y = self[[self.columns[-1]]]\n",
    "        y = y.iloc[self.train_indices]\n",
    "        indices = [np.where(y==l)[0] for l in np.unique(y)]\n",
    "        classlengths = [len(i) for i in indices]\n",
    "        n = max(classlengths)\n",
    "        mask = np.hstack([np.random.choice(i, n-l, replace=True) for l,i in zip(classlengths, indices)])\n",
    "        indices = np.hstack([mask, range(len(y))])\n",
    "        self.train_indices = self.train_indices[indices]\n",
    "        return self\n",
    "    \n",
    "    @classmethod\n",
    "    def _split_indices(self, indices, split):\n",
    "        length = len(indices)\n",
    "        try:\n",
    "            train_length = int((1 - sum(split))* length)\n",
    "            train_valid_length = int((1 - split[1])* length)\n",
    "            assert train_length > 0, 'Non positive size of the training set, provide fractions for valid/test part, e.g. (0.2, 0.3)'\n",
    "            assert train_valid_length > train_length, 'Non positive size of the validation set, provide fraction for valid part, bigger than 0, e.g. e.g. (0.2, 0.3)'\n",
    "            assert length >= train_valid_length, 'Negative fraction of the test set, provide fractions for valid/test part, e.g. (0.2, 0.3)'\n",
    "            train_indices = indices[:train_length]\n",
    "            valid_indices = indices[train_length:train_valid_length]\n",
    "            test_indices = indices[train_valid_length:]\n",
    "        except:\n",
    "            train_length = int((1 - split)* length)\n",
    "            assert train_length > 0, 'Non positive size of the training set, provide fraction for valid part, smaller than 1, e.g. 0.2'\n",
    "            assert train_length < length, 'Non positive size of the validation set, provide fraction for valid part, bigger than 0, e.g. 0.2'\n",
    "            train_indices = indices[:train_length]\n",
    "            valid_indices = indices[train_length:]\n",
    "            test_indices = []\n",
    "        return train_indices, valid_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pipetorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pipetorch.py\n",
    "\n",
    "class PTArray(np.ndarray):\n",
    "    _metadata = ['_pt_scaler', '_pt_indices', '_train_indices', '_valid_indices', '_test_indices', '_ycolumns']\n",
    "\n",
    "    def __new__(cls, input_array):\n",
    "        return np.asarray(input_array).view(cls)\n",
    "\n",
    "    def __array_finalize__(self, obj) -> None:\n",
    "        if obj is None: return\n",
    "        self._ycolumns = 1\n",
    "        d = { a:getattr(obj, a) for a in self._metadata if hasattr(obj, a) }\n",
    "        self.__dict__.update(d)\n",
    "\n",
    "    def __array_function__(self, func, types, *args, **kwargs):\n",
    "        return self._wrap(super().__array_function__(func, types, *args, **kwargs))\n",
    "        \n",
    "    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):\n",
    "        def cast(i):\n",
    "            if type(i) is PTArray:\n",
    "                return i.view(np.ndarray)\n",
    "            return i\n",
    "        \n",
    "        inputs = [ cast(i) for i in inputs ]\n",
    "        return self._wrap(super().__array_ufunc__(ufunc, method, *inputs, **kwargs))        \n",
    "    \n",
    "    def _check_list_attr(self, attr):\n",
    "        try:\n",
    "            return hasattr(self, attr) and len(getattr(self, attr)) > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _pt_scaler_exists(self):\n",
    "        return self._check_list_attr('_pt_scaler')\n",
    "\n",
    "    def _test_indices_exists(self):\n",
    "        return self._check_list_attr('_test_indices')\n",
    "\n",
    "    def _train_indices_exists(self):\n",
    "        return self._check_list_attr('_train_indices')\n",
    "    \n",
    "    def _wrap(self, a):\n",
    "        a = PTArray(a)\n",
    "        a.__dict__.update(self.__dict__)\n",
    "        return a\n",
    "\n",
    "    def polynomials(self, degree):\n",
    "        assert not self._pt_scaler_exists(), \"Run polynomials before scaling\"\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        p = poly.fit_transform(self[:,:-self.ycolumns])\n",
    "        return self._wrap(np.concatenate([p, self[:, -self.ycolumns:]], axis=1))\n",
    "    \n",
    "    def to_arrays(self, ycolumns=None, dtype=np.float32):\n",
    "        if ycolumns:\n",
    "            self._ycolumns = ycolumns\n",
    "        a = self._wrap(self.astype(dtype))\n",
    "        if self._test_indices_exists() and len(self._test_indices) > 0:\n",
    "            return a.train_X, a.valid_X, a.test_X, a.train_y, a.valid_y, a.test_y\n",
    "        return a.train_X, a.valid_X, a.train_y, a.valid_y\n",
    "\n",
    "    def scale(self, scalertype=StandardScaler):\n",
    "        assert self._train_indices_exists(), \"Split the DataFrame before scaling!\"\n",
    "        assert not self._pt_scaler_exists(), \"Trying to scale twice, which is a really bad idea!\"\n",
    "        r = self._wrap(copy.deepcopy(self))\n",
    "        r._pt_scaler = tuple(self._create_scaler(scalertype, column) for column in self[self._train_indices].T)\n",
    "        return r.transform(self)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_scaler(scalertype, column):\n",
    "        scaler = scalertype()\n",
    "        scaler.fit(column.reshape(-1,1))\n",
    "        return scaler\n",
    "\n",
    "    def transform(self, array):\n",
    "        out = []\n",
    "        for column, scaler in zip(array.T, self._pt_scaler):\n",
    "            if scaler is not None:\n",
    "                out.append(scaler.transform(column.reshape(-1,1)))\n",
    "            else:\n",
    "                out.append(column)\n",
    "        return self._wrap(np.concatenate(out, axis=1))\n",
    "\n",
    "    def inverse_transform_y(self, y):\n",
    "        y = to_numpy(y)\n",
    "        y = y.reshape(-1, self._ycolumns)\n",
    "        out = [ y[i] if self._pt_scaler[-self._ycolumns+i] is None else self._pt_scaler[-self._ycolumns+i].inverse_transform(y[:,i]) for i in range(y.shape[1]) ]\n",
    "        if len(out) == 1:\n",
    "            return self._wrap(out[0])\n",
    "        return self._wrap(np.concatenate(out, axis=1))\n",
    "    \n",
    "    def inverse_transform_X(self, X):\n",
    "        X = to_numpy(X)\n",
    "        transform = [ X[i] if self._pt_scaler[i] is None else self._pt_scaler[i].inverse_transform(X[:,i]) for i in range(X.shape[1]) ]\n",
    "        return np._wrap(np.concatenate(transform, axis=1))\n",
    "\n",
    "    def inverse_transform(self, X, y):\n",
    "        y = self.inverse_transform_y(y)\n",
    "        X = self.inverse_transform_X(X)\n",
    "        return pd.concat([X, y], axis=1)\n",
    "\n",
    "    def to_dataset(self):\n",
    "        \"\"\"\n",
    "        returns: a list with a train, valid and test DataSet. Every DataSet contains an X and y, where the \n",
    "        input data matrix X contains all columns but the last, and the target y contains the last column\n",
    "        columns: list of columns to convert, the last column is always the target. default=None means all columns.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        tensor_y = torch.from_numpy(self.y)\n",
    "        tensor_X = torch.from_numpy(self.X)\n",
    "\n",
    "        p = [ TensorDataset(tensor_X[self._train_indices], tensor_y[self._train_indices]) ]\n",
    "        p.append( TensorDataset(tensor_X[self._valid_indices], tensor_y[self._valid_indices]) )\n",
    "        if len(self._test_indices) > 0:\n",
    "            p.append( TensorDataset(tensor_X[self._test_indices], tensor_y[self._test_indices]) )\n",
    "        return p\n",
    "    \n",
    "    def to_databunch(self, batch_size=32, num_workers=0, shuffle=True, pin_memory=False, balance=False):\n",
    "        \"\"\"\n",
    "        returns: a Databunch that contains dataloaders for the train, valid and test part.\n",
    "        batch_size, num_workers, shuffle, pin_memory: see Databunch/Dataloader constructor\n",
    "        \"\"\"\n",
    "        ds = self.to_dataset()\n",
    "        return Databunch(*ds, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=pin_memory, scaler=self, balance=balance)    \n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self[self._pt_indices, :-self._ycolumns]    \n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self[self._pt_indices, -self._ycolumns:]    \n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self[self._train_indices, :-self._ycolumns]\n",
    "    \n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self[self._valid_indices, :-self._ycolumns]\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self[self._test_indices, :-self._ycolumns]\n",
    "    \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self[self._train_indices, -self._ycolumns:]\n",
    "    \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self[self._valid_indices, -self._ycolumns:]\n",
    "\n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self[self._test_indices, -self._ycolumns:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to pipetorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pipetorch.py\n",
    "\n",
    "class Databunch:\n",
    "    def __init__(self, train_ds, valid_ds, test_ds=None, batch_size=32, num_workers=0, shuffle=True, pin_memory=False, scaler=None, balance=False):\n",
    "        self.train_ds = train_ds\n",
    "        self.valid_ds = valid_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.pin_memory = pin_memory\n",
    "        self.scaler = scaler\n",
    "        self.balance = balance\n",
    "        \n",
    "    @property\n",
    "    def train_dl(self):\n",
    "        try:\n",
    "            return self._train_dl\n",
    "        except:\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            sampler = self._weighted_sampler(self.balance) if self.balance is not False else None\n",
    "            shuffle = False if self.balance is not False else self.shuffle\n",
    "            self._train_dl = DataLoader(self.train_ds, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=shuffle, pin_memory=self.pin_memory, sampler=sampler)\n",
    "            return self._train_dl\n",
    "\n",
    "    @property\n",
    "    def valid_dl(self):\n",
    "        try:\n",
    "            return self._valid_dl\n",
    "        except:\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            self._valid_dl = DataLoader(self.valid_ds, batch_size=len(self.valid_ds), num_workers=self.num_workers, shuffle=False, pin_memory=self.pin_memory)\n",
    "            return self._valid_dl\n",
    "\n",
    "    @property\n",
    "    def test_dl(self):\n",
    "        try:\n",
    "            return self._test_dl\n",
    "        except:\n",
    "            from torch.utils.data import DataLoader\n",
    "\n",
    "            self._test_dl = DataLoader(self.test_ds, batch_size=len(self.test_ds), num_workers=self.num_workers, shuffle=False, pin_memory=self.pin_memory)\n",
    "            return self._test_dl\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            del self._valid_dl\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._train_dl\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._test_dl\n",
    "        except: pass\n",
    "\n",
    "    def _weighted_sampler(self, weights):\n",
    "        import torch\n",
    "        from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "        target = self.train_ds.tensors[1].numpy().squeeze()\n",
    "        if weights == True:\n",
    "            weights = {t:(1. / c) for t, c in zip(*np.unique(target, return_counts=True))}\n",
    "        samples_weight = np.array([weights[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        return WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "      \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, value):\n",
    "        self._batch_size = min(value, len(self.train_ds))\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def num_workers(self):\n",
    "        return self._num_workers\n",
    "\n",
    "    @num_workers.setter\n",
    "    def num_workers(self, value):\n",
    "        self._num_workers = value\n",
    "        self.reset()\n",
    "    \n",
    "    def inverse_transform_y(self, y):\n",
    "        return self.scaler.inverse_transform_y(y)\n",
    "\n",
    "    def sample(self, device=None):\n",
    "        arrays = next(iter(self.train_dl))\n",
    "        if device is not None:\n",
    "            arrays = [ a.to(device) for a in arrays ]\n",
    "        return arrays\n",
    "\n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.train_ds.tensors[0]\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.valid_ds.tensors[0]\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test_ds.tensors[0]\n",
    "\n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train_ds.y.tensors[1]\n",
    "\n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid_ds.y.tensors[1]\n",
    "\n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test_ds.y.tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipetorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PTDataFrame.read_csv('/data/datasets/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = (df.quality >= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = df[['pH', 'alcohol', 'quality']].split(0.2).scale().to_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/anaconda/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.676829268292683"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(valid_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = df[['pH', 'alcohol', 'quality']].split(0.2).scale().to_array().to_databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu 4\n"
     ]
    }
   ],
   "source": [
    "from dl2.tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wine(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.sigmoid(self.w1(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = trainer(model, db, loss=nn.BCELoss(), metrics=[loss, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe89bed0dfd461e83feb1738d4341b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Total'), FloatProgress(value=0.0, max=320000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.11s train loss: 0.645604  f1: 0.524691 valid loss: 0.629726  f1: 0.597122\n",
      "2 0.04s train loss: 0.635023  f1: 0.570588 valid loss: 0.618584  f1: 0.627178\n",
      "3 0.04s train loss: 0.625190  f1: 0.607714 valid loss: 0.610255  f1: 0.660000\n",
      "4 0.04s train loss: 0.617749  f1: 0.629964 valid loss: 0.603269  f1: 0.683871\n",
      "5 0.04s train loss: 0.611546  f1: 0.637885 valid loss: 0.597957  f1: 0.700315\n",
      "6 0.03s train loss: 0.606604  f1: 0.651646 valid loss: 0.593139  f1: 0.712500\n",
      "7 0.03s train loss: 0.602177  f1: 0.658662 valid loss: 0.589824  f1: 0.727829\n",
      "8 0.04s train loss: 0.598938  f1: 0.670025 valid loss: 0.586538  f1: 0.728916\n",
      "9 0.03s train loss: 0.596044  f1: 0.676056 valid loss: 0.584011  f1: 0.734328\n",
      "10 0.03s train loss: 0.593873  f1: 0.683648 valid loss: 0.581829  f1: 0.735294\n",
      "11 0.04s train loss: 0.592093  f1: 0.679643 valid loss: 0.579499  f1: 0.735294\n",
      "12 0.03s train loss: 0.590293  f1: 0.684932 valid loss: 0.577873  f1: 0.742690\n",
      "13 0.04s train loss: 0.588942  f1: 0.684380 valid loss: 0.577038  f1: 0.736842\n",
      "14 0.03s train loss: 0.588005  f1: 0.683280 valid loss: 0.575413  f1: 0.747826\n",
      "15 0.03s train loss: 0.587018  f1: 0.684844 valid loss: 0.574131  f1: 0.751445\n",
      "16 0.04s train loss: 0.586082  f1: 0.686901 valid loss: 0.573919  f1: 0.749280\n",
      "17 0.04s train loss: 0.585404  f1: 0.685851 valid loss: 0.573528  f1: 0.749280\n",
      "18 0.04s train loss: 0.584787  f1: 0.691083 valid loss: 0.572785  f1: 0.752874\n",
      "19 0.03s train loss: 0.584338  f1: 0.690533 valid loss: 0.572034  f1: 0.752874\n",
      "20 0.03s train loss: 0.583756  f1: 0.691574 valid loss: 0.571381  f1: 0.752874\n",
      "21 0.04s train loss: 0.583432  f1: 0.691574 valid loss: 0.571359  f1: 0.752874\n",
      "22 0.03s train loss: 0.583194  f1: 0.692613 valid loss: 0.570978  f1: 0.750716\n",
      "23 0.03s train loss: 0.582802  f1: 0.692003 valid loss: 0.570846  f1: 0.755682\n",
      "24 0.04s train loss: 0.582647  f1: 0.693101 valid loss: 0.569829  f1: 0.754286\n",
      "25 0.03s train loss: 0.582273  f1: 0.694071 valid loss: 0.569617  f1: 0.759207\n",
      "26 0.04s train loss: 0.582158  f1: 0.694554 valid loss: 0.568769  f1: 0.762712\n",
      "27 0.03s train loss: 0.582059  f1: 0.695584 valid loss: 0.569043  f1: 0.764873\n",
      "28 0.03s train loss: 0.581842  f1: 0.697638 valid loss: 0.568306  f1: 0.766197\n",
      "29 0.04s train loss: 0.581811  f1: 0.699608 valid loss: 0.568657  f1: 0.764045\n",
      "30 0.04s train loss: 0.581716  f1: 0.703675 valid loss: 0.568467  f1: 0.767507\n",
      "31 0.04s train loss: 0.581685  f1: 0.702660 valid loss: 0.567941  f1: 0.767507\n",
      "32 0.04s train loss: 0.581486  f1: 0.702660 valid loss: 0.568263  f1: 0.767507\n",
      "33 0.03s train loss: 0.581492  f1: 0.700627 valid loss: 0.568157  f1: 0.764045\n",
      "34 0.04s train loss: 0.581455  f1: 0.707165 valid loss: 0.567440  f1: 0.765363\n",
      "35 0.03s train loss: 0.581333  f1: 0.708625 valid loss: 0.567278  f1: 0.765363\n",
      "36 0.04s train loss: 0.581309  f1: 0.708625 valid loss: 0.567594  f1: 0.765363\n",
      "37 0.03s train loss: 0.581269  f1: 0.708625 valid loss: 0.567089  f1: 0.765363\n",
      "38 0.03s train loss: 0.581186  f1: 0.709728 valid loss: 0.567425  f1: 0.767507\n",
      "39 0.04s train loss: 0.581168  f1: 0.709176 valid loss: 0.567168  f1: 0.767507\n",
      "40 0.03s train loss: 0.581223  f1: 0.708723 valid loss: 0.567530  f1: 0.767507\n",
      "41 0.03s train loss: 0.581261  f1: 0.708171 valid loss: 0.566709  f1: 0.765363\n",
      "42 0.04s train loss: 0.581157  f1: 0.708625 valid loss: 0.567192  f1: 0.765363\n",
      "43 0.04s train loss: 0.581138  f1: 0.707165 valid loss: 0.566531  f1: 0.765363\n",
      "44 0.04s train loss: 0.581122  f1: 0.707621 valid loss: 0.566982  f1: 0.765363\n",
      "45 0.03s train loss: 0.581120  f1: 0.707525 valid loss: 0.566127  f1: 0.765363\n",
      "46 0.03s train loss: 0.581094  f1: 0.707071 valid loss: 0.566710  f1: 0.767507\n",
      "47 0.04s train loss: 0.581150  f1: 0.707525 valid loss: 0.565905  f1: 0.767507\n",
      "48 0.03s train loss: 0.581082  f1: 0.708527 valid loss: 0.565582  f1: 0.767507\n",
      "49 0.03s train loss: 0.581126  f1: 0.707978 valid loss: 0.565740  f1: 0.767507\n",
      "50 0.03s train loss: 0.581103  f1: 0.709077 valid loss: 0.565878  f1: 0.769663\n",
      "51 0.03s train loss: 0.581163  f1: 0.705699 valid loss: 0.565561  f1: 0.766197\n",
      "52 0.04s train loss: 0.581230  f1: 0.709176 valid loss: 0.565424  f1: 0.769663\n",
      "53 0.04s train loss: 0.581080  f1: 0.706615 valid loss: 0.565245  f1: 0.766197\n",
      "54 0.04s train loss: 0.581091  f1: 0.706065 valid loss: 0.565263  f1: 0.769663\n",
      "55 0.04s train loss: 0.581103  f1: 0.708625 valid loss: 0.565360  f1: 0.769663\n",
      "56 0.03s train loss: 0.581261  f1: 0.708430 valid loss: 0.564551  f1: 0.767507\n",
      "57 0.04s train loss: 0.581090  f1: 0.707336 valid loss: 0.564724  f1: 0.767507\n",
      "58 0.04s train loss: 0.581100  f1: 0.707336 valid loss: 0.564864  f1: 0.767507\n",
      "59 0.04s train loss: 0.581133  f1: 0.709329 valid loss: 0.564667  f1: 0.767507\n",
      "60 0.04s train loss: 0.581365  f1: 0.711316 valid loss: 0.565437  f1: 0.765363\n",
      "61 0.03s train loss: 0.581053  f1: 0.709329 valid loss: 0.565050  f1: 0.767507\n",
      "62 0.04s train loss: 0.580988  f1: 0.709777 valid loss: 0.565547  f1: 0.765363\n",
      "63 0.03s train loss: 0.580987  f1: 0.708880 valid loss: 0.565804  f1: 0.765363\n",
      "64 0.03s train loss: 0.581110  f1: 0.707978 valid loss: 0.565675  f1: 0.767507\n",
      "65 0.04s train loss: 0.581084  f1: 0.707978 valid loss: 0.566277  f1: 0.765363\n",
      "66 0.04s train loss: 0.581067  f1: 0.710628 valid loss: 0.566695  f1: 0.765363\n",
      "67 0.04s train loss: 0.581108  f1: 0.709176 valid loss: 0.566846  f1: 0.767507\n",
      "68 0.04s train loss: 0.581033  f1: 0.707165 valid loss: 0.566447  f1: 0.765363\n",
      "69 0.03s train loss: 0.580998  f1: 0.707716 valid loss: 0.566392  f1: 0.765363\n",
      "70 0.04s train loss: 0.581081  f1: 0.708527 valid loss: 0.565671  f1: 0.765363\n",
      "71 0.04s train loss: 0.581046  f1: 0.710974 valid loss: 0.566106  f1: 0.765363\n",
      "72 0.04s train loss: 0.581172  f1: 0.709428 valid loss: 0.565307  f1: 0.765363\n",
      "73 0.04s train loss: 0.581035  f1: 0.707336 valid loss: 0.565073  f1: 0.767507\n",
      "74 0.04s train loss: 0.581019  f1: 0.708430 valid loss: 0.565503  f1: 0.767507\n",
      "75 0.04s train loss: 0.581064  f1: 0.708880 valid loss: 0.565134  f1: 0.767507\n",
      "76 0.03s train loss: 0.581181  f1: 0.709428 valid loss: 0.565528  f1: 0.767507\n",
      "77 0.04s train loss: 0.581101  f1: 0.708527 valid loss: 0.566098  f1: 0.765363\n",
      "78 0.04s train loss: 0.581059  f1: 0.708625 valid loss: 0.566001  f1: 0.769663\n",
      "79 0.04s train loss: 0.581155  f1: 0.711524 valid loss: 0.566090  f1: 0.765363\n",
      "80 0.04s train loss: 0.581046  f1: 0.711969 valid loss: 0.566184  f1: 0.765363\n",
      "81 0.04s train loss: 0.581063  f1: 0.712856 valid loss: 0.565626  f1: 0.765363\n",
      "82 0.04s train loss: 0.581043  f1: 0.709877 valid loss: 0.565528  f1: 0.765363\n",
      "83 0.04s train loss: 0.581112  f1: 0.711316 valid loss: 0.565633  f1: 0.765363\n",
      "84 0.04s train loss: 0.581100  f1: 0.712750 valid loss: 0.566147  f1: 0.765363\n",
      "85 0.04s train loss: 0.581048  f1: 0.712308 valid loss: 0.566282  f1: 0.765363\n",
      "86 0.04s train loss: 0.581055  f1: 0.711077 valid loss: 0.566329  f1: 0.765363\n",
      "87 0.04s train loss: 0.581032  f1: 0.711077 valid loss: 0.566175  f1: 0.765363\n",
      "88 0.04s train loss: 0.581082  f1: 0.710871 valid loss: 0.565614  f1: 0.765363\n",
      "89 0.04s train loss: 0.581061  f1: 0.710324 valid loss: 0.565424  f1: 0.765363\n",
      "90 0.04s train loss: 0.581100  f1: 0.710324 valid loss: 0.565861  f1: 0.765363\n",
      "91 0.03s train loss: 0.581022  f1: 0.711316 valid loss: 0.565948  f1: 0.765363\n",
      "92 0.03s train loss: 0.581058  f1: 0.711760 valid loss: 0.565683  f1: 0.765363\n",
      "93 0.04s train loss: 0.581026  f1: 0.710324 valid loss: 0.565508  f1: 0.765363\n",
      "94 0.03s train loss: 0.581082  f1: 0.711316 valid loss: 0.565813  f1: 0.765363\n",
      "95 0.04s train loss: 0.581036  f1: 0.711760 valid loss: 0.566141  f1: 0.765363\n",
      "96 0.04s train loss: 0.581096  f1: 0.711760 valid loss: 0.565682  f1: 0.765363\n",
      "97 0.03s train loss: 0.581114  f1: 0.712750 valid loss: 0.566379  f1: 0.765363\n",
      "98 0.04s train loss: 0.581066  f1: 0.710871 valid loss: 0.565886  f1: 0.765363\n",
      "99 0.03s train loss: 0.581051  f1: 0.710871 valid loss: 0.566054  f1: 0.765363\n",
      "100 0.04s train loss: 0.581088  f1: 0.711420 valid loss: 0.566806  f1: 0.765363\n",
      "101 0.03s train loss: 0.581139  f1: 0.712963 valid loss: 0.566099  f1: 0.765363\n",
      "102 0.03s train loss: 0.581040  f1: 0.711864 valid loss: 0.566399  f1: 0.765363\n",
      "103 0.04s train loss: 0.581178  f1: 0.711628 valid loss: 0.566653  f1: 0.765363\n",
      "104 0.03s train loss: 0.581138  f1: 0.711969 valid loss: 0.566378  f1: 0.765363\n",
      "105 0.03s train loss: 0.580995  f1: 0.712308 valid loss: 0.566143  f1: 0.765363\n",
      "106 0.03s train loss: 0.580996  f1: 0.708978 valid loss: 0.566343  f1: 0.765363\n",
      "107 0.03s train loss: 0.581127  f1: 0.711420 valid loss: 0.565777  f1: 0.765363\n",
      "108 0.04s train loss: 0.581068  f1: 0.710324 valid loss: 0.565521  f1: 0.765363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 0.04s train loss: 0.581042  f1: 0.710324 valid loss: 0.565453  f1: 0.765363\n",
      "110 0.04s train loss: 0.581027  f1: 0.710324 valid loss: 0.565640  f1: 0.765363\n",
      "111 0.04s train loss: 0.581057  f1: 0.710324 valid loss: 0.565414  f1: 0.765363\n",
      "112 0.03s train loss: 0.581049  f1: 0.710769 valid loss: 0.565758  f1: 0.765363\n",
      "113 0.04s train loss: 0.581029  f1: 0.711760 valid loss: 0.565855  f1: 0.765363\n",
      "114 0.03s train loss: 0.581026  f1: 0.710769 valid loss: 0.565289  f1: 0.765363\n",
      "115 0.03s train loss: 0.581035  f1: 0.709877 valid loss: 0.565261  f1: 0.767507\n",
      "116 0.04s train loss: 0.581081  f1: 0.709777 valid loss: 0.565663  f1: 0.765363\n",
      "117 0.03s train loss: 0.581112  f1: 0.709527 valid loss: 0.566191  f1: 0.765363\n",
      "118 0.04s train loss: 0.581077  f1: 0.709077 valid loss: 0.566557  f1: 0.765363\n",
      "119 0.03s train loss: 0.581146  f1: 0.710324 valid loss: 0.565812  f1: 0.765363\n",
      "120 0.04s train loss: 0.581032  f1: 0.709877 valid loss: 0.566123  f1: 0.765363\n",
      "121 0.04s train loss: 0.581060  f1: 0.710425 valid loss: 0.566230  f1: 0.765363\n",
      "122 0.04s train loss: 0.581099  f1: 0.711077 valid loss: 0.566762  f1: 0.765363\n",
      "123 0.04s train loss: 0.581020  f1: 0.707525 valid loss: 0.566328  f1: 0.765363\n",
      "124 0.04s train loss: 0.581148  f1: 0.711969 valid loss: 0.565726  f1: 0.765363\n",
      "125 0.04s train loss: 0.581124  f1: 0.707525 valid loss: 0.565829  f1: 0.767507\n",
      "126 0.04s train loss: 0.581027  f1: 0.708075 valid loss: 0.565809  f1: 0.767507\n",
      "127 0.04s train loss: 0.581027  f1: 0.709077 valid loss: 0.565922  f1: 0.765363\n",
      "128 0.04s train loss: 0.580992  f1: 0.709977 valid loss: 0.565903  f1: 0.765363\n",
      "129 0.04s train loss: 0.581093  f1: 0.708880 valid loss: 0.565470  f1: 0.765363\n",
      "130 0.04s train loss: 0.581229  f1: 0.707978 valid loss: 0.565166  f1: 0.767507\n",
      "131 0.04s train loss: 0.581067  f1: 0.707978 valid loss: 0.565000  f1: 0.767507\n",
      "132 0.04s train loss: 0.581138  f1: 0.707978 valid loss: 0.565731  f1: 0.767507\n",
      "133 0.04s train loss: 0.581091  f1: 0.708430 valid loss: 0.565028  f1: 0.767507\n",
      "134 0.03s train loss: 0.581104  f1: 0.707883 valid loss: 0.565306  f1: 0.767507\n",
      "135 0.04s train loss: 0.581074  f1: 0.708783 valid loss: 0.565066  f1: 0.765363\n",
      "136 0.04s train loss: 0.581152  f1: 0.707336 valid loss: 0.565608  f1: 0.767507\n",
      "137 0.04s train loss: 0.581007  f1: 0.707978 valid loss: 0.565556  f1: 0.767507\n",
      "138 0.04s train loss: 0.581093  f1: 0.708075 valid loss: 0.566176  f1: 0.765363\n",
      "139 0.04s train loss: 0.581082  f1: 0.708978 valid loss: 0.566002  f1: 0.765363\n",
      "140 0.03s train loss: 0.581257  f1: 0.713955 valid loss: 0.566627  f1: 0.765363\n",
      "141 0.04s train loss: 0.581139  f1: 0.709977 valid loss: 0.567138  f1: 0.765363\n",
      "142 0.04s train loss: 0.581206  f1: 0.709877 valid loss: 0.566222  f1: 0.765363\n",
      "143 0.04s train loss: 0.581069  f1: 0.711420 valid loss: 0.566530  f1: 0.765363\n",
      "144 0.04s train loss: 0.581076  f1: 0.709527 valid loss: 0.567087  f1: 0.765363\n",
      "145 0.04s train loss: 0.581166  f1: 0.711316 valid loss: 0.565878  f1: 0.765363\n",
      "146 0.04s train loss: 0.581140  f1: 0.710871 valid loss: 0.566610  f1: 0.765363\n",
      "147 0.04s train loss: 0.581114  f1: 0.710526 valid loss: 0.566383  f1: 0.765363\n",
      "148 0.04s train loss: 0.581183  f1: 0.710425 valid loss: 0.566004  f1: 0.765363\n",
      "149 0.04s train loss: 0.581061  f1: 0.709428 valid loss: 0.566328  f1: 0.765363\n",
      "150 0.03s train loss: 0.581238  f1: 0.709877 valid loss: 0.565895  f1: 0.765363\n",
      "151 0.04s train loss: 0.581069  f1: 0.709777 valid loss: 0.565855  f1: 0.765363\n",
      "152 0.04s train loss: 0.581042  f1: 0.710324 valid loss: 0.565802  f1: 0.765363\n",
      "153 0.04s train loss: 0.581053  f1: 0.710425 valid loss: 0.566344  f1: 0.765363\n",
      "154 0.04s train loss: 0.581083  f1: 0.712308 valid loss: 0.565929  f1: 0.765363\n",
      "155 0.04s train loss: 0.581237  f1: 0.710425 valid loss: 0.566417  f1: 0.765363\n",
      "156 0.04s train loss: 0.581032  f1: 0.711864 valid loss: 0.565982  f1: 0.765363\n",
      "157 0.04s train loss: 0.581100  f1: 0.711420 valid loss: 0.566314  f1: 0.765363\n",
      "158 0.04s train loss: 0.581165  f1: 0.712856 valid loss: 0.566078  f1: 0.765363\n",
      "159 0.04s train loss: 0.581071  f1: 0.711420 valid loss: 0.565890  f1: 0.765363\n",
      "160 0.03s train loss: 0.581178  f1: 0.711077 valid loss: 0.566259  f1: 0.765363\n",
      "161 0.04s train loss: 0.581006  f1: 0.711077 valid loss: 0.566253  f1: 0.765363\n",
      "162 0.03s train loss: 0.581021  f1: 0.712074 valid loss: 0.566128  f1: 0.765363\n",
      "163 0.03s train loss: 0.581031  f1: 0.709977 valid loss: 0.566038  f1: 0.765363\n",
      "164 0.04s train loss: 0.581107  f1: 0.708527 valid loss: 0.565271  f1: 0.767507\n",
      "165 0.03s train loss: 0.581051  f1: 0.709077 valid loss: 0.565714  f1: 0.769663\n",
      "166 0.04s train loss: 0.581041  f1: 0.709077 valid loss: 0.566278  f1: 0.769663\n",
      "167 0.03s train loss: 0.581019  f1: 0.709728 valid loss: 0.565976  f1: 0.767507\n",
      "168 0.03s train loss: 0.581031  f1: 0.707716 valid loss: 0.566143  f1: 0.769663\n",
      "169 0.04s train loss: 0.581050  f1: 0.706615 valid loss: 0.565713  f1: 0.769663\n",
      "170 0.03s train loss: 0.581145  f1: 0.709728 valid loss: 0.565731  f1: 0.769663\n",
      "171 0.04s train loss: 0.581066  f1: 0.707621 valid loss: 0.566010  f1: 0.767507\n",
      "172 0.04s train loss: 0.581048  f1: 0.707071 valid loss: 0.565727  f1: 0.769663\n",
      "173 0.03s train loss: 0.581031  f1: 0.709077 valid loss: 0.565537  f1: 0.767507\n",
      "174 0.04s train loss: 0.581055  f1: 0.707978 valid loss: 0.565321  f1: 0.767507\n",
      "175 0.04s train loss: 0.581028  f1: 0.707978 valid loss: 0.565379  f1: 0.767507\n",
      "176 0.04s train loss: 0.581075  f1: 0.709176 valid loss: 0.565889  f1: 0.767507\n",
      "177 0.03s train loss: 0.581048  f1: 0.708625 valid loss: 0.565693  f1: 0.769663\n",
      "178 0.03s train loss: 0.581030  f1: 0.708171 valid loss: 0.566068  f1: 0.769663\n",
      "179 0.04s train loss: 0.581047  f1: 0.707716 valid loss: 0.566044  f1: 0.769663\n",
      "180 0.03s train loss: 0.581048  f1: 0.707165 valid loss: 0.566524  f1: 0.765363\n",
      "181 0.03s train loss: 0.581095  f1: 0.711077 valid loss: 0.566118  f1: 0.765363\n",
      "182 0.03s train loss: 0.581067  f1: 0.708527 valid loss: 0.566196  f1: 0.765363\n",
      "183 0.03s train loss: 0.581029  f1: 0.708978 valid loss: 0.565808  f1: 0.765363\n",
      "184 0.03s train loss: 0.581162  f1: 0.706790 valid loss: 0.565146  f1: 0.767507\n",
      "185 0.03s train loss: 0.581094  f1: 0.707430 valid loss: 0.565746  f1: 0.767507\n",
      "186 0.03s train loss: 0.581130  f1: 0.710078 valid loss: 0.566287  f1: 0.765363\n",
      "187 0.04s train loss: 0.581060  f1: 0.708978 valid loss: 0.566236  f1: 0.765363\n",
      "188 0.03s train loss: 0.581108  f1: 0.710526 valid loss: 0.565824  f1: 0.765363\n",
      "189 0.03s train loss: 0.581072  f1: 0.710974 valid loss: 0.566023  f1: 0.765363\n",
      "190 0.03s train loss: 0.581018  f1: 0.706977 valid loss: 0.566279  f1: 0.765363\n",
      "191 0.03s train loss: 0.581067  f1: 0.711077 valid loss: 0.566341  f1: 0.765363\n",
      "192 0.03s train loss: 0.581079  f1: 0.709627 valid loss: 0.566767  f1: 0.765363\n",
      "193 0.03s train loss: 0.581055  f1: 0.709428 valid loss: 0.565714  f1: 0.767507\n",
      "194 0.03s train loss: 0.581137  f1: 0.710078 valid loss: 0.566343  f1: 0.765363\n",
      "195 0.03s train loss: 0.581094  f1: 0.709428 valid loss: 0.566124  f1: 0.765363\n",
      "196 0.03s train loss: 0.581041  f1: 0.710425 valid loss: 0.566087  f1: 0.765363\n",
      "197 0.03s train loss: 0.581213  f1: 0.710324 valid loss: 0.565425  f1: 0.765363\n",
      "198 0.03s train loss: 0.581082  f1: 0.712308 valid loss: 0.565740  f1: 0.765363\n",
      "199 0.03s train loss: 0.581127  f1: 0.710769 valid loss: 0.565225  f1: 0.765363\n",
      "200 0.03s train loss: 0.581217  f1: 0.712750 valid loss: 0.565723  f1: 0.765363\n"
     ]
    }
   ],
   "source": [
    "t.train(200, lr=[1e-3, 3e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
