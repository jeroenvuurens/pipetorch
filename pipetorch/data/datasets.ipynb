{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e4c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile datasets.py\n",
    "from .dframe import DFrame\n",
    "from .textcollection import TextCollection\n",
    "from pathlib import Path\n",
    "from getpass import getuser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "import os\n",
    "from io import StringIO\n",
    "import pkgutil\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.metrics import f1_score\n",
    "from io import BytesIO\n",
    "from functools import partial\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def path_user(dataset=None):\n",
    "    if dataset is not None:\n",
    "        return Path.home() / '.pipetorchuser' / dataset.split('/')[-1]\n",
    "    return Path.home() / '.pipetorchuser'\n",
    "\n",
    "def path_shared(dataset=None):\n",
    "    if dataset is not None:\n",
    "        return Path.home() / '.pipetorch' / dataset.split('/')[-1]    \n",
    "    return Path.home() / '.pipetorch'\n",
    "\n",
    "def dataset_path(dataset):\n",
    "    try:\n",
    "        p = path_shared(dataset)\n",
    "        if p.exists():\n",
    "            return p\n",
    "    except: pass\n",
    "    return path_user(dataset)\n",
    "\n",
    "def get_stored_path(filename, path=None):\n",
    "    if path is not None:\n",
    "        storedpath = path / filename\n",
    "    else:\n",
    "        storedpath = (path_user() / filename)\n",
    "        if not storedpath.exists():\n",
    "            storedpath = (path_shared() / filename)\n",
    "        if not storedpath.exists():\n",
    "            storedpath = (path_user() / filename)\n",
    "    return storedpath\n",
    "\n",
    "def get_filename(url):\n",
    "    fragment_removed = url.split(\"#\")[0]  # keep to left of first #\n",
    "    query_string_removed = fragment_removed.split(\"?\")[0]\n",
    "    scheme_removed = query_string_removed.split(\"://\")[-1].split(\":\")[-1]\n",
    "    if scheme_removed.find(\"/\") == -1:\n",
    "        filename = scheme_removed\n",
    "    else:\n",
    "        filename = os.path.basename(scheme_removed)\n",
    "    if '.' in filename:\n",
    "        filename = filename.rsplit( \".\", 1 )[ 0 ] + '.csv'\n",
    "    return filename\n",
    "\n",
    "def to_csv(df, filename, **kwargs):\n",
    "    kwargs = { key:value for key, value in kwargs.items() if key in {'sep', 'quoting', 'quotechar', 'lineterminator', 'decimal', 'line_terminator', 'doublequote', 'escapechar'}}\n",
    "    kwargs['index'] = False\n",
    "    if 'sep' in kwargs and len(kwargs['sep']) > 1:\n",
    "        sep = kwargs['sep']\n",
    "        kwargs['sep'] = '¤'\n",
    "        csv = df.to_csv(**kwargs).replace('¤', sep)\n",
    "        with open(filename, 'w') as fout:\n",
    "            fout.write(csv)\n",
    "    else:\n",
    "        df.to_csv(filename, **kwargs)   \n",
    "\n",
    "def read_pd_excel(path, filename=None, save=True, **kwargs):\n",
    "    if filename is None:\n",
    "        filename = get_filename(path)\n",
    "    if (path_user() / filename).is_file():\n",
    "        return pd.read_excel(path_user() / filename, **kwargs)\n",
    "    if (path_shared() / filename).is_file():\n",
    "        return pd.read_excel(path_shared() / filename, **kwargs)\n",
    "    #print('Downloading new file ' + path)\n",
    "    df = pd.read_excel(path, **kwargs)\n",
    "    df.columns = df.columns.str.replace(' ', '') \n",
    "    return df\n",
    "\n",
    "def read_excel(path, filename=None, **kwargs):\n",
    "    return DFrame(read_pd_excel(path, filename=filename, **kwargs))\n",
    "\n",
    "def create_kaggle_authentication(username, key):\n",
    "    \"\"\"\n",
    "    Writes the authentication by kaggle to a file, so that PipeTorch can use the kaggle api to \n",
    "    download datasets.\n",
    "    \n",
    "    Arguments:\n",
    "        username: str\n",
    "            your username at kaggle (you do need to register)\n",
    "        key: str\n",
    "            the token that you have generated on your kaggle account (can be read in the .json file)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(Path.home() / '.kaggle'):\n",
    "        os.path.mkdir(Path.home() / '.kaggle')\n",
    "    os.path.chmod(Path.home() / '.kaggle', 0o600)\n",
    "    kagglejson = Path.home() / '.kaggle' / '.kaggle.json'\n",
    "    if not os.path.exists(kagglejson):\n",
    "        with open (kagglejson, \"w\") as fout:\n",
    "            fout.write(f'{\"username\":\"{username}\", \"key\":\"{key}\"}')\n",
    "            print(f'kaggle authorization written to {kagglejson}')\n",
    "        os.path.chmod('~/.kaggle/kaggle.json', 0o600)\n",
    "    else:\n",
    "        print(f'kaggle authorization already exists, check {kagglejson}')\n",
    "\n",
    "def kaggle_download(dataset, shared=False):\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "    except:\n",
    "        print('''\n",
    "        Error authenticating kaggle: you need to (1) register at kaggle (2) generate a token/key and \n",
    "        (3) put your user credentials in ~/.kaggle/kaggle.json. You can perform (3) using \n",
    "        pipetorch.data.create_kaggle_authentication(username, key)\n",
    "        ''')\n",
    "        return\n",
    "    path = path_shared(dataset) if shared else dataset_path(dataset)\n",
    "    print(f'Downloading {dataset} from kaggle to {path}')\n",
    "    api.dataset_download_files(dataset, path=path, unzip=True)\n",
    "\n",
    "def kaggle_download_competition(dataset, shared=False):\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "    except:\n",
    "        print('''\n",
    "        Error authenticating kaggle: you need to (1) register at kaggle (2) generate a token/key and \n",
    "        (3) put your user credentials in ~/.kaggle/kaggle.json. You can perform (3) using \n",
    "        pipetorch.data.create_kaggle_authentication(username, key)\n",
    "        ''')\n",
    "        return\n",
    "    path = path_shared(dataset) if shared else dataset_path(dataset)\n",
    "    print(f'Downloading {dataset} from kaggle to {path}')\n",
    "    api.competition_download_files(dataset, path=path)\n",
    "    for zfile in list(path.glob('*.zip')):\n",
    "        zip = ZipFile(zfile)\n",
    "        zip.extractall(path=path)\n",
    "        zfile.unlink()\n",
    "\n",
    "def read_pd_from_kaggle(dataset, filename=None, shared=False, force=False, **kwargs):\n",
    "    if force:\n",
    "        try:\n",
    "            path = path_user(dataset)\n",
    "            shutil.rmtree(path)\n",
    "        except: pass\n",
    "    path = dataset_path(dataset)\n",
    "    if force or not path.exists():\n",
    "        kaggle_download(dataset, shared=shared)\n",
    "    path = dataset_path(dataset)\n",
    "    assert path.exists(), f'Problem downloading Kaggle dataset {dataset}'\n",
    "    if filename is None:\n",
    "        filename = '**/*'\n",
    "    files = list(path.glob(filename))\n",
    "    assert len(files) == 1, f'There are multiple files that match {files}, set filename to select a file'\n",
    "    return pd.read_csv(path / files[0], **kwargs)\n",
    "\n",
    "def read_pd_from_kaggle_competition(dataset, filename=None, shared=False, force=False, **kwargs):\n",
    "    if force:\n",
    "        try:\n",
    "            path = path_user(dataset)\n",
    "            shutil.rmtree(path)\n",
    "        except: pass\n",
    "    path = dataset_path(dataset)\n",
    "    if force or not path.exists():\n",
    "        kaggle_download_competition(dataset, shared=shared)\n",
    "    path = dataset_path(dataset)\n",
    "    assert path.exists(), f'Problem downloading Kaggle dataset {dataset}'\n",
    "    if filename is None:\n",
    "        filename = '**/*'\n",
    "    files = list(path.glob(filename))\n",
    "    assert len(files) == 1, f'There are multiple files that match {files}, set filename to select a file'\n",
    "    return pd.read_csv(path / files[0], **kwargs)\n",
    "\n",
    "def read_from_kaggle(dataset, train=None, test=None, shared=False, force=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a DFrame from a Kaggle dataset. The downloaded dataset is automatically stored so that the next time\n",
    "    it is read from file rather than downloaded. See `read_csv`. The dataset is stored by default in a folder\n",
    "    with the dataset name in `~/.pipetorchuser`.\n",
    "    \n",
    "    If the dataset is not cached, this functions requires a valid .kaggle/kaggle.json file, that you can \n",
    "    create manually or with the function `create_kaggle_authentication()`.\n",
    "\n",
    "    Note: there is a difference between a Kaggle dataset and a Kaggle competition. For the latter, \n",
    "    you have to use `read_from_kaggle_competition`.\n",
    "    \n",
    "    Example:\n",
    "        read_from_kaggle('uciml/autompg-dataset')\n",
    "            to read/download `https://www.kaggle.com/datasets/uciml/autompg-dataset`\n",
    "        read_from_kaggle('robmarkcole/occupancy-detection-data-set-uci', 'datatraining.txt', 'datatest.txt')\n",
    "            to combine a train and test set in a single DFrame\n",
    "    \n",
    "    Arguments:\n",
    "        dataset: str\n",
    "            the username/dataset part of the kaggle url, e.g. uciml/autompg-dataset for \n",
    "            \n",
    "        train: str (None)\n",
    "            the filename that is used as the train set, e.g. 'train.csv'\n",
    "        test: str (None)\n",
    "            the filename that is used as the test set, e.g. 'test.csv'\n",
    "        shared: bool (False)\n",
    "            save the dataset in ~/.pipetorch instead of ~/.pipetorchuser, allowing to share downloaded\n",
    "            files between users.\n",
    "        force: bool (False)\n",
    "            when True, the dataset is always downloaded\n",
    "        **kwargs:\n",
    "            additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "            you will have to set engine='python'.\n",
    "            \n",
    "    Returns: DFrame\n",
    "    \"\"\"\n",
    "    train = read_pd_from_kaggle(dataset, filename=train, shared=shared, force=force, **kwargs)\n",
    "    if test is not None:\n",
    "        test = read_pd_from_kaggle(dataset, filename=test, **kwargs)\n",
    "        return DFrame.from_train_test(train, test)\n",
    "    return DFrame(train)\n",
    "\n",
    "def read_from_kaggle_competition(dataset, train=None, test=None, shared=False, force=False, **kwargs):\n",
    "    train = read_pd_from_kaggle_competition(dataset, filename=train, shared=shared, force=force, **kwargs)\n",
    "    if test is not None:\n",
    "        test = read_pd_from_kaggle_competition(dataset, filename=test, **kwargs)\n",
    "        return DFrame.from_train_test(train, test)\n",
    "    return DFrame(train)\n",
    "\n",
    "def read_pd_csv(url, filename=None, path=None, save=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a .csv file from cache or url. The place to store the file is indicated by path / filename\n",
    "    and when a delimiter is used, this is also used to save the file so that the original delimiter is kept.\n",
    "    The file is only downloaded using the url if it does not exsists on the filing system. If the file is\n",
    "    downloaded and save=True, it is also stored for future use.\n",
    "    \n",
    "    Arguments:\n",
    "        url: str\n",
    "            the url to download or a full path pointing to a .csv file\n",
    "        filename: str (None)\n",
    "            the filename to store the downloaded file under. If None, the filename is extracted from the url.\n",
    "        path: str (None)\n",
    "            the path in which the file is stored. If None, it will first check the ~/.pipetorch (for sharing\n",
    "            dataset between users) and then ~/.pipetorchuser (for user specific caching of datasets).\n",
    "        save: bool (False)\n",
    "            whether to save a downloaded .csv\n",
    "        **kwargs:\n",
    "            additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "            you will have to set engine='python'.\n",
    "            \n",
    "    Returns: pd.DataFrame\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = get_filename(url)\n",
    "    storedpath = get_stored_path(filename, path)        \n",
    "    if not storedpath.exists():\n",
    "        storedpath = (path_user() / filename)\n",
    "        (path_user()).mkdir(exist_ok=True)\n",
    "        if '://' in url:\n",
    "            print(f'Downloading {url}')\n",
    "        df = pd.read_csv(url, **kwargs)\n",
    "        if save:\n",
    "            print(f'saving to {storedpath}')\n",
    "            to_csv(df, filename, **kwargs)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.read_csv(storedpath, **kwargs)\n",
    "\n",
    "def read_csv(url, filename=None, path=None, save=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Reads a .csv file from cache or url. The place to store the file is indicated by path / filename\n",
    "    and when a delimiter is used, this is also used to save the file so that the original delimiter is kept.\n",
    "    The file is only downloaded using the url if it does not exsists on the filing system. If the file is\n",
    "    downloaded and save=True, it is also stored for future use.\n",
    "    \n",
    "    Arguments:\n",
    "        url: str\n",
    "            the url to download or a full path pointing to a .csv file\n",
    "        filename: str (None)\n",
    "            the filename to store the downloaded file under. If None, the filename is extracted from the url.\n",
    "        path: str (None)\n",
    "            the path in which the file is stored. If None, it will first check the ~/.pipetorch (for sharing\n",
    "            dataset between users) and then ~/.pipetorchuser (for user specific caching of datasets).\n",
    "        save: bool (False)\n",
    "            whether to save a downloaded .csv\n",
    "        **kwargs:\n",
    "            additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "            you will have to set engine='python'.\n",
    "            \n",
    "    Returns: DFrame\n",
    "    \"\"\"\n",
    "    return DFrame(read_pd_csv(url, filename=filename, path=path, save=save, **kwargs))\n",
    "    \n",
    "def read_pd_from_package(package, filename, **kwargs):\n",
    "    csv = pkgutil.get_data(package, filename).decode()\n",
    "    return pd.read_csv(StringIO(csv), **kwargs)\n",
    "\n",
    "def read_from_package(package, filename, **kwargs):\n",
    "    return DFrame(read_pd_from_package(package, filename))\n",
    "\n",
    "def read_pd_from_function(filename, function, path=None, save=True, **kwargs):\n",
    "    \"\"\"\n",
    "    First checks if a .csv file is already stored, otherwise, calls the custom function to retrieve a \n",
    "    DataFrame. \n",
    "    \n",
    "    The place to store the file is indicated by path / filename.\n",
    "    The file is only retrieved from the function if it does not exsists on the filing system. \n",
    "    If the file is retrieved and save=True, it is also stored for future use.\n",
    "    \n",
    "    Arguments:\n",
    "        filename: str (None)\n",
    "            the filename to store the downloaded file under.\n",
    "        function: func\n",
    "            a function that is called to retrieve the DataFrame if the file does not exist.\n",
    "        path: str (None)\n",
    "            the path in which the file is stored. If None, it will first check the ~/.pipetorch (for sharing\n",
    "            dataset between users) and then ~/.pipetorchuser (for user specific caching of datasets).\n",
    "        save: bool (True)\n",
    "            whether to save a downloaded .csv\n",
    "        **kwargs:\n",
    "            additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "            you will have to set engine='python'.\n",
    "            \n",
    "    Returns: pd.DataFrame\n",
    "    \"\"\"\n",
    "    storedpath = get_stored_path(filename, path)\n",
    "    if storedpath.is_file():\n",
    "        return pd.read_csv(storedpath, **kwargs)\n",
    "    df = function()\n",
    "    to_csv(df, storedpath, **kwargs)\n",
    "    return df\n",
    "    \n",
    "def read_from_function(filename, function, path=None, save=True, **kwargs):\n",
    "    \"\"\"\n",
    "    First checks if a .csv file is already stored, otherwise, calls the custom function to retrieve a \n",
    "    DataFrame. \n",
    "    \n",
    "    The place to store the file is indicated by path / filename.\n",
    "    The file is only retrieved from the function if it does not exsists on the filing system. \n",
    "    If the file is retrieved and save=True, it is also stored for future use.\n",
    "    \n",
    "    Arguments:\n",
    "        filename: str (None)\n",
    "            the filename to store the downloaded file under.\n",
    "        function: func\n",
    "            a function that is called to retrieve the DataFrame if the file does not exist.\n",
    "        path: str (None)\n",
    "            the path in which the file is stored. If None, it will first check the ~/.pipetorch (for sharing\n",
    "            dataset between users) and then ~/.pipetorchuser (for user specific caching of datasets).\n",
    "        save: bool (True)\n",
    "            whether to save a downloaded .csv\n",
    "        **kwargs:\n",
    "            additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "            you will have to set engine='python'.\n",
    "            \n",
    "    Returns: DFrame\n",
    "    \"\"\"\n",
    "    return DFrame(read_pd_from_function(filename, function, path=path, save=save, **kwargs))\n",
    "    \n",
    "def read_torchtext(torchtext_function):\n",
    "    try:\n",
    "        return torchtext_function(root=path_shared() / torchtext_function.__name__)\n",
    "    except:\n",
    "        return torchtext_function(root=path_user() / torchtext_function.__name__)\n",
    "\n",
    "def wine_quality():\n",
    "    return read_from_kaggle('uciml/red-wine-quality-cortez-et-al-2009')\n",
    "\n",
    "def telco_churn():\n",
    "    return read_from_kaggle('blastchar/telco-customer-churn')\n",
    "\n",
    "def movielens_ratings():\n",
    "    COLS = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "    return read_from_kaggle('odedgolden/movielens-1m-dataset', 'ratings.dat', sep='::', engine='python', names=COLS)\n",
    "\n",
    "def movielens_movies():\n",
    "    COLS = ['movie_id', 'title', 'genre']\n",
    "    return read_from_kaggle('odedgolden/movielens-1m-dataset', 'movies.dat', sep='::', engine='python', names=COLS)\n",
    "    \n",
    "def movielens_users():\n",
    "    COLS = ['user_id', 'gender', 'age', 'occupation', 'zip_code']\n",
    "    return read_from_kaggle('odedgolden/movielens-1m-dataset', 'users.dat', sep='::', engine='python', names=COLS)\n",
    "    \n",
    "def dam_outflow():\n",
    "    return read_from_package('pipetorch', 'data/datasets/dam_outflow.csv')\n",
    "    \n",
    "def boston_housing_prices():\n",
    "    return read_from_kaggle('fedesoriano/the-boston-houseprice-data')\n",
    "\n",
    "def hotel_booking():\n",
    "    return read_from_kaggle('mojtaba142/hotel-booking')\n",
    "    df = df.sort_values(by=['arrival_date_year', 'arrival_date_week_number'])\n",
    "    df = df[[ c for c in df if c != 'is_canceled'] + ['is_canceled']]\n",
    "    return DFrame(df)\n",
    "\n",
    "def hotel():\n",
    "    df = hotel_booking()\n",
    "    train = df[(df.ArrivalDateYear < 2017) | (df.ArrivalDateWeekNumber < 14)]\n",
    "    return DFrame(train)\n",
    "\n",
    "def hotel_test_orig():\n",
    "    df = hotel_booking()\n",
    "    hotel = df[(df.ArrivalDateYear == 2017) & (df.ArrivalDateWeekNumber > 13)]\n",
    "    return DFrame(hotel)\n",
    "\n",
    "def hotel_test():\n",
    "    return hotel_test_orig().drop(columns='IsCanceled')\n",
    "\n",
    "def hotel_test_y():\n",
    "    return hotel_test_orig()[['IsCanceled']]\n",
    "\n",
    "def hotel_test_score(pred_y):\n",
    "    return f1_score(hotel_test_y(), pred_y)\n",
    "\n",
    "def iris():\n",
    "    return read_from_kaggle('uciml/iris')\n",
    "\n",
    "def bank_marketing():\n",
    "    return read_from_kaggle('janiobachmann/bank-marketing-dataset')\n",
    "\n",
    "def auto_mpg():\n",
    "    return read_from_kaggle('uciml/autompg-dataset')\n",
    "\n",
    "def big_mart_sales():\n",
    "    return read_from_kaggle('brijbhushannanda1979/bigmart-sales-data', train='Train.csv', test='Test.csv')\n",
    "\n",
    "def advertising_channels():\n",
    "    return read_from_kaggle('yasserh/advertising-sales-dataset')\n",
    "\n",
    "def titanic():\n",
    "    read_from_kaggle_competition('titanic', 'train.csv', 'test.csv')\n",
    "\n",
    "def diamonds():\n",
    "    return read_from_kaggle('shivam2503/diamonds')\n",
    "\n",
    "def indian_liver():\n",
    "    return read_from_kaggle('uciml/indian-liver-patient-records')\n",
    "\n",
    "def ames_housing():\n",
    "    return read_from_kaggle_competition('house-prices-advanced-regression-techniques', 'train.csv', 'test.csv')\n",
    "    \n",
    "def air_passengers():\n",
    "    return read_from_kaggle('rakannimer/air-passengers')\n",
    "\n",
    "def rossmann_store_sales():\n",
    "    return read_from_kaggle_competition('rossmann-store-sales', 'train.csv', 'test.csv')\n",
    "\n",
    "def rossmann_stores():\n",
    "    return read_from_kaggle_competition('rossmann-store-sales', 'store.csv')\n",
    "\n",
    "def california():\n",
    "    return read_from_kaggle('camnugent/california-housing-prices')\n",
    "    \n",
    "def heart_disease(**kwargs):\n",
    "    return read_from_kaggle('johnsmith88/heart-disease-dataset')\n",
    "    \n",
    "def speeddate(**kwargs):\n",
    "    return read_from_kaggle('annavictoria/speed-dating-experiment')\n",
    "    \n",
    "def occupancy():\n",
    "    return read_from_kaggle('robmarkcole/occupancy-detection-data-set-uci', 'datatraining.txt', 'datatest.txt')\n",
    "\n",
    "def ag_news(language='basic_english', min_freq=1, collate='pad'):\n",
    "    from torchtext.datasets import AG_NEWS\n",
    "    train_iter, test_iter = read_torchtext(AG_NEWS)\n",
    "    tc = TextCollection.from_iter(train_iter, None, test_iter, min_freq=min_freq).collate(collate)\n",
    "    return tc\n",
    "\n",
    "def bbc_news(language='basic_english', min_freq=1, collate='pad'):\n",
    "    return TextCollection.from_csv('/data/datasets/bbc-text.csv', language=language, min_freq=min_freq).collate(collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e4952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6fc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
