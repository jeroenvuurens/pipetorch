{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7edfcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting textcollection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile textcollection.py\n",
    "#from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "#from torchtext.vocab import Vocab, build_vocab_from_iterator, FastText, GloVe\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataset import random_split\n",
    "from .databunch import Databunch\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "class TextCollection:\n",
    "    \"\"\"\n",
    "    Setup an in memory text collection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train, valid=None, test=None, language='basic_english', min_freq=1, vocab=None, \n",
    "                 labels = None, specials=('<unk>', '<pad>'), collate=None):\n",
    "        self._vocab = vocab\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.test = test\n",
    "        self.language = language\n",
    "        self.min_freq = min_freq\n",
    "        self.labels = labels\n",
    "        self.specials = specials\n",
    "        self.__collate = collate\n",
    "    \n",
    "    @classmethod\n",
    "    def from_iter(cls, train_iter, valid_iter=None, test_iter=None, language='basic_english', min_freq=1, specials=('<unk>', '<pad>')):\n",
    "        train = TextDataSet.from_iter(train_iter)\n",
    "        valid = None if valid_iter is None else TextDataSet.from_iter(valid_iter)\n",
    "        test = None if test_iter is None else TextDataSet.from_iter(test_iter)\n",
    "        return cls(train, valid=valid, test=test, language=language, min_freq=min_freq, specials=specials)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, train_filename, valid_filename=None, test_filename=None, language='basic_english', min_freq=1, specials=('<unk>', '<pad>')):\n",
    "        train = TextDataSet.from_csv(train_filename)\n",
    "        valid = None if valid_filename is None else TextDataSet.from_csv(valid_filename)\n",
    "        test = None if test_filename is None else TextDataSet.from_csv(test_filename)\n",
    "        return cls(train, valid=valid, test=test, language=language, min_freq=min_freq, specials=specials)\n",
    "\n",
    "    @property\n",
    "    def _collate(self):\n",
    "        return self.__collate\n",
    "    \n",
    "    def collate(self, collate):\n",
    "        \"\"\"\n",
    "        Return a textcollection for which the dataset is prepared as:\n",
    "        'pad': the sentences ae padded with a '<pad>' token to have equal length\n",
    "        'offset': a batch is a list of tokenids and a seperate list of offsets indicate where a new sentence starts.\n",
    "        ...: a cusrom collation function\n",
    "        \n",
    "        There are several options in TorchText to train with either padded of offset datasets.\n",
    "        \"\"\"\n",
    "        if collate == 'offset':\n",
    "            return OffsetTextCollection(self.train, self.valid, test=self.test, \n",
    "                    language=self.language, min_freq=self.min_freq, vocab=self._vocab, labels = self.labels, \n",
    "                    specials=self.specials)\n",
    "        if collate == 'pad':\n",
    "            return PaddedTextCollection(self.train, self.valid, test=self.test,  \n",
    "                    language=self.language, min_freq=self.min_freq, vocab=self._vocab, labels = self.labels, \n",
    "                    specials=self.specials)\n",
    "        r = TextCollection(self.train, self.valid, test=self.test, \n",
    "                    language=self.language, min_freq=self.min_freq, vocab=self._vocab, labels = self.labels, \n",
    "                    specials=self.specials)\n",
    "        r.__collate = collate\n",
    "        return r   \n",
    "    \n",
    "    def split(self, valid_perc, test_perc=None):\n",
    "        \"\"\"\n",
    "        return: a splitted version the text collection.\n",
    "        valid_perc: the fraction of the training set to be used for validation\n",
    "        test_perc: the fraction of the training set to be used for testing\n",
    "        \n",
    "        Note: you cannot resplit a textcollection that was already split, or if it has a fixed validation or test set.\n",
    "        \"\"\"\n",
    "        assert test_perc is None or self.test is None, 'You cannot specify a test_perc if a fixed test set is given'\n",
    "        assert self.valid is None, 'You cannot resplit a text collection that was already split or that has a fixed validation set'\n",
    "        assert self.vocab_not_exists, 'You cannot split a text collection when a vocabulary is already built'\n",
    "        r = copy.copy(self)\n",
    "        test_count = 0 if test_perc is None else round(test_perc * len(r.train))\n",
    "        valid_count = round(valid_perc * len(r.train))\n",
    "        train_count = len(r.train) - test_count - valid_count\n",
    "        #print(len(r.train), train_count, valid_count, test_count)\n",
    "        #print(len(r.train), sum([train_count, valid_count, test_count]))\n",
    "        r.train, r.valid, r.test = random_split(r.train, [train_count, valid_count, test_count])\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def language(self):\n",
    "        return self._language\n",
    "        \n",
    "    @language.setter\n",
    "    def language(self, value):\n",
    "        assert self.vocab_not_exists, 'You cannot specify a language after the vocabulary was built'\n",
    "        if value is not None:\n",
    "            self._language = value\n",
    "            try:\n",
    "                del self._tokenizer\n",
    "                del self._vocab\n",
    "                del self._labels\n",
    "            except: pass\n",
    "        \n",
    "    @property\n",
    "    def min_freq(self):\n",
    "        return self._min_freq\n",
    "        \n",
    "    @min_freq.setter\n",
    "    def min_freq(self, value):\n",
    "        assert self.vocab_not_exists, 'You cannot specify a min_freq after the vocabulary was built'\n",
    "        if value is not None:\n",
    "            self._min_freq = value\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "        The tokenizer used to transform a text into a list of words/tokens.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._tokenizer\n",
    "        except:\n",
    "            from torchtext.data.utils import get_tokenizer\n",
    "            self._tokenizer = get_tokenizer(self.language)\n",
    "            return self._tokenizer\n",
    "\n",
    "    @tokenizer.setter\n",
    "    def tokenizer(self, value):\n",
    "        assert self.vocab_not_exists, 'You cannot change the tokenizer after the vocabulary was built'\n",
    "        self._tokenizer = value\n",
    "        \n",
    "    @property\n",
    "    def vocab_not_exists(self):\n",
    "        return self._vocab is None\n",
    "        \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        \"\"\"\n",
    "        A (generated) PyTorch Vocab that maps all tokens in the training set to numbers.\n",
    "        \"\"\"\n",
    "        if self._vocab is None:\n",
    "            self._build_vocab()\n",
    "        return self._vocab\n",
    "    \n",
    "    @vocab.setter\n",
    "    def vocab(self, value):\n",
    "        self._vocab = value\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"\n",
    "        Collection of groud truth labels for the TextCollection\n",
    "        \"\"\"\n",
    "        if self._labels is None:\n",
    "            self._build_vocab()\n",
    "        return self._labels\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self._labels = value\n",
    "            \n",
    "    def decode_sentence_index(self, words, index):\n",
    "        \"\"\"\n",
    "        returns a list of tokens for the text at index in words.\n",
    "        \"\"\"\n",
    "        return self.decode_sentence(words[index])\n",
    "    \n",
    "    def decode_sentence(self, tokenids):\n",
    "        # lookup_tokens on vocab transforms List(int) -> List(str)\n",
    "        return self.vocab.lookup_tokens(list(tokenids))\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        # lookup_indices on vocab transforms List(str) -> List(int)\n",
    "        return self.vocab.lookup_indices(self.tokenizer(sentence))\n",
    "\n",
    "    def token_iterator(self, dataset):\n",
    "        \"\"\"\n",
    "        returns: iterator over lists of tokenids that represent the original sentences\n",
    "        may be overriden by subclassing TextCollection\n",
    "        \"\"\"\n",
    "        def yield_tokens(dataset):\n",
    "            for _, text in dataset:\n",
    "                yield self.tokenizer(text)\n",
    "        return yield_tokens(dataset)\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "        labels = Counter([ l for l, _ in self.train ])\n",
    "        self._vocab = build_vocab_from_iterator(self.token_iterator(self.train), specials=self.specials, special_first=True, min_freq=self.min_freq)\n",
    "        self._vocab.set_default_index(self._vocab['<unk>'])\n",
    "        self._labels = LabelSet(labels)\n",
    "        \n",
    "    def to_databunch(self, batch_size=32, shuffle=True, balance=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Return: a databunch (object with a dataloader for the train, valid, (test) set)\n",
    "        batch_size (None): the batch_size used by the DataLoader\n",
    "        shuffle (True): whether the samples are shuffled between epochs\n",
    "        balance (False): if the training set is to be balanced (only works for binary classification)\n",
    "        **kwargs: any other named argument that Databunch accepts\n",
    "        \n",
    "        Note: creating a databunch does not modify the original TextCollection, instead a shallow copy is made\n",
    "        and creating the databunch usually triggers split, balance and creation of the vocabulary to be triggered.\n",
    "        You can access the shallow copy of the textcollection through db.textcollection and the generated \n",
    "        vocabulary through db.vocab.\n",
    "        \"\"\"\n",
    "        r = copy.copy(self)\n",
    "        db = Databunch(None, r.train, r.valid, r.test, batch_size=batch_size, valid_batch_size=batch_size, shuffle=shuffle, balance=balance, collate=r._collate, **kwargs)\n",
    "        db.textcollection = r\n",
    "        db.vocab = r.vocab\n",
    "        if r._using_pretrained():\n",
    "            db.pretrained_embeddings = r.pretrained_embeddings\n",
    "            db.load_pretrained_embeddings_in_layer = r.load_pretrained_embeddings_in_layer\n",
    "        return db\n",
    "    \n",
    "    def GloVe(self, name='6B', cache='/data/datasets', max_vectors=None):\n",
    "        \"\"\"\n",
    "        Setup the use of pretrained GloVe embeddings. There are 4 sets available for download:\n",
    "        name (6B): 6B, 42B, 840B or twitter.27B\n",
    "        cache: shared folder to store downloaded embeddings. Do not change unless you know what you are doing\n",
    "        since embeddings take up a lot of storage.\n",
    "        max_vectors: use only the indicated number of vectors to save RAM. Since the tokens are sorted on frequency,\n",
    "        using only the n most frequently appearing tokens works well in most cases.\n",
    "        \"\"\"\n",
    "        from torchtext.vocab import GloVe\n",
    "        r = copy.copy(self)\n",
    "        r._pretrained = GloVe\n",
    "        r._pretrained_params = {'name':name, 'cache':cache, 'max_vectors':max_vectors}\n",
    "        return r\n",
    "    \n",
    "    def _using_pretrained(self):\n",
    "        try:\n",
    "            self._pretrained\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def pretrained_embeddings(self, dim):\n",
    "        assert self._using_pretrained(), 'Can only prepare a table of pretrained embeddings if you configure using GloVe'\n",
    "        self._pretrained_params['dim'] = dim\n",
    "        self._embeddings = self._pretrained(**self._pretrained_params).get_vecs_by_tokens(self.vocab.get_itos())\n",
    "        return self._embeddings\n",
    "    \n",
    "    def load_pretrained_embeddings_in_layer(self, layer):\n",
    "        dim = embedding_layer.embedding_dim\n",
    "        embedding_layer.weight.data = self.pretrained_embeddings(dim)              \n",
    "        \n",
    "class PaddedTextCollection(TextCollection):\n",
    "    def _collate(self, batch):\n",
    "        label_list, text_list = [], []\n",
    "        for (_label, _text) in batch:\n",
    "            label_list.append(self.labels[_label])\n",
    "            processed_text = torch.tensor(self.encode_sentence(_text), dtype=torch.int64)\n",
    "            text_list.append(processed_text)\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        text_list = pad_sequence(text_list, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "        return text_list, label_list\n",
    "\n",
    "    def decode_sentence_index(self, words, index):\n",
    "        return super().decode_sentence(words[index].squeeze())\n",
    "        \n",
    "    def decode_sentences(self, words):\n",
    "        return [ self.decode_sentence(words, i) for i in range(len(words))]\n",
    "    \n",
    "class OffsetTextCollection(TextCollection):\n",
    "    def _collate(self, batch):\n",
    "        label_list, text_list, offsets = [], [], [0]\n",
    "        for (_label, _text) in batch:\n",
    "            label_list.append(self.labels[_label])\n",
    "            processed_text = torch.tensor(self.encode_sentence(_text), dtype=torch.int64)\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return text_list, offsets, label_list\n",
    "\n",
    "    def decode_sentence_index(self, words, offsets, index):\n",
    "        start = offsets[index]\n",
    "        end = offsets[index+1] if index < len(offsets)-1 else len(words)\n",
    "        return super().decode_sentence(words[start:end])\n",
    "        \n",
    "    def decode_sentences(self, words, offsets):\n",
    "        return [ self.decode_sentence(words, offsets, i) for i in range(len(offsets))]\n",
    "    \n",
    "class LabelSet:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self._itol = list(labels.keys())\n",
    "        self._ltoi = {l:i for i, l in enumerate(self._itol)}\n",
    "        \n",
    "    def __getitem__(self, label):\n",
    "        return self._ltoi[label]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._itol)\n",
    "    \n",
    "    def lookup_labels(self, labels):\n",
    "        return [self._ltoi[l] for l in labels]\n",
    "    \n",
    "    def lookup_ints(self, ints):\n",
    "        return [self._itol[i] for i in ints]\n",
    "\n",
    "class TextDataSet:    \n",
    "    @classmethod\n",
    "    def from_iter(cls, it):\n",
    "        r = cls()\n",
    "        r.data = list(it)\n",
    "        return r\n",
    "        \n",
    "    @classmethod\n",
    "    def from_csv(cls, filename, header=True, delimiter=','):\n",
    "        r = cls()\n",
    "        with open(filename) as fin:\n",
    "            r.data = [ l.split(delimiter, 1) for l in fin ]\n",
    "        if header:\n",
    "            r.data = r.data[1:]\n",
    "        return r\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data.__getitem__(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514db33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92ee92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
