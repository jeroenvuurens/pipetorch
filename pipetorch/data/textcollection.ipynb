{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7edfcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting textcollection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile textcollection.py\n",
    "#from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "#from torchtext.vocab import Vocab, build_vocab_from_iterator, FastText, GloVe\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataset import random_split\n",
    "from .databunch import Databunch\n",
    "from .kagglereader import Kaggle\n",
    "from .helper import path_shared, path_user, read_torchtext\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "class TextCollection:\n",
    "    \"\"\"\n",
    "    Setup an in memory text collection\n",
    "    \n",
    "    Args:\n",
    "        train: TextDataSet\n",
    "        valid: TextDataSet (None)\n",
    "        test: TextDataSet (None)\n",
    "        language: str ('basic_english')\n",
    "            the langage used by the tokenizer\n",
    "        min_freq: int (1)\n",
    "            the minimum document frequency for words to be added to the vocabulary\n",
    "        vocab: (None)\n",
    "            a vocabulary to use. If None, a new vocabulary will be made from the trainset\n",
    "        labels: LabelSet (None)\n",
    "            a set of labes to use. If None, a LabelSet is constructed from the train set\n",
    "        specials: tuple of str ('<unk>', 'pad')\n",
    "            special tokens that are added to the vocabulary. Most commonly, <unk> is used\n",
    "            for words that are not in the vocabulary and <pad> for making data points\n",
    "            the same size.\n",
    "        collate: callable (None)\n",
    "            a function to prepare a batch, for example by padding all sentences to equal\n",
    "            length.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train, test=None, valid=None, language='basic_english', min_freq=1, vocab=None, \n",
    "                 labels = None, specials=('<unk>', '<pad>'), collate=None):\n",
    "        self._vocab = vocab\n",
    "        self.test = test\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.language = language\n",
    "        self.min_freq = min_freq\n",
    "        self.labels = labels\n",
    "        self.specials = specials\n",
    "        self.__collate = collate\n",
    "    \n",
    "    @classmethod\n",
    "    def from_iter(cls, train_iter, test_iter=None, valid_iter=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Reads a TextCollection from iterators. The most common use is from TorchText\n",
    "        DataSets.\n",
    "        \n",
    "        Arguments:\n",
    "            train_iter: str\n",
    "                iterator that is used as the train set, e.g. 'train.csv'\n",
    "            test_iter: str (None)\n",
    "                iterator that is used as the test set, e.g. 'test.csv'\n",
    "            valid_iter: str (None)\n",
    "                iterator that is used as the valid set, e.g. 'valid.csv'\n",
    "            **kwargs: see the TextCollection constructor for additional arguments\n",
    "                      such as language, min_freq, vocab, labels, special, collate \n",
    "        Returns: TextCollection\n",
    "        \"\"\"\n",
    "        train = TextDataSet.from_iter(train_iter)\n",
    "        test = None if test_iter is None else TextDataSet.from_iter(test_iter)\n",
    "        valid = None if valid_iter is None else TextDataSet.from_iter(valid_iter)\n",
    "        return cls(train, test=test, valid=valid, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, train_filename, test_filename=None, valid_filename=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Reads a TextCollection from csv files. \n",
    "        \n",
    "        Typically, the files are assumed to be organized in lines with the format: label, text\n",
    "        Although often these files have a .csv extension, the files are not true csv files.\n",
    "        \n",
    "        Example:\n",
    "            read_from_csv('train.csv', test_filename='test.csv', min_freq=1)\n",
    "            to combine a train and test set in a single TextCollection\n",
    "\n",
    "        Arguments:\n",
    "            train_filename: str\n",
    "                the filename that is used as the train set, e.g. 'train.csv'\n",
    "            test_filename: str (None)\n",
    "                the filename that is used as the test set, e.g. 'test.csv'\n",
    "            valid_filename: str (None)\n",
    "                the filename that is used as the valid set, e.g. 'valid.csv'\n",
    "            **kwargs: see the TextCollection constructor for additional arguments\n",
    "                      such as language, min_freq, vocab, labels, special, collate \n",
    "        Returns: TextCollection\n",
    "        \"\"\"\n",
    "        train = TextDataSet.from_csv(train_filename)\n",
    "        test = None if test_filename is None else TextDataSet.from_csv(test_filename)\n",
    "        valid = None if valid_filename is None else TextDataSet.from_csv(valid_filename)\n",
    "        return cls(train, test=test, valid=valid, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_kaggle(cls, dataset, train=None, test=None, valid=None, shared=True, force=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Reads a TextCollection from a Kaggle dataset. The files are forwarded to from_csv().\n",
    "        \n",
    "        The downloaded dataset is automatically stored so that the next time\n",
    "        it is read from file rather than downloaded. \n",
    "        The dataset is stored by default in a folder\n",
    "        with the dataset name in `~/.pipetorchuser`. \n",
    "\n",
    "        If the dataset is not cached, this functions requires a valid .kaggle/kaggle.json file, that you can \n",
    "        create manually or with the function `create_kaggle_authentication()`.\n",
    "\n",
    "        Note: there is a difference between a Kaggle dataset and a Kaggle competition. For the latter, \n",
    "        you have to use `read_from_kaggle_competition`.\n",
    "\n",
    "        Example:\n",
    "            read_from_kaggle('yufengdev/bbc-text-categorization')\n",
    "                to read/download `https://www.kaggle.com/datasets/yufengdev/bbc-text-categorization`\n",
    "\n",
    "        Arguments:\n",
    "            dataset: str\n",
    "                the username/dataset part of the kaggle url, e.g. uciml/autompg-dataset for \n",
    "            train: str (None)\n",
    "                the filename that is used as the train set, e.g. 'train.csv'\n",
    "            test: str (None)\n",
    "                the filename that is used as the test set, e.g. 'test.csv'\n",
    "            valid: str (None)\n",
    "                the filename that is used as the valid set, e.g. 'valid.csv'\n",
    "            shared: bool (False)\n",
    "                save the dataset in ~/.pipetorch instead of ~/.pipetorchuser, allowing to share downloaded\n",
    "                files between users.\n",
    "            force: bool (False)\n",
    "                when True, the dataset is always downloaded\n",
    "            **kwargs: see the TextCollection constructor for additional arguments\n",
    "                      such as language, min_freq, vocab, labels, special, collate \n",
    "        Returns: TextCollection\n",
    "        \"\"\"\n",
    "        k = Kaggle(dataset, shared=shared)\n",
    "        if force:\n",
    "            k.remove_user()\n",
    "        train = k.file(train)\n",
    "        if test is not None:\n",
    "            test = k.file(test)\n",
    "        if valid is not None:\n",
    "            valid = k.file(valid)\n",
    "        return cls.from_csv(train, test_filename=test, valid_filename=valid, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_torchtext(cls, func, min_freq=1, collate='pad', **kwargs): \n",
    "        \"\"\"\n",
    "        Reads a TextCollection from a torchtext data function.\n",
    "        \n",
    "        Arguments:\n",
    "            func: callable\n",
    "                from torchtext.datasets, a a function that behaves similarly and returns\n",
    "                a train and test iterator for the dataset. By default the datasets are\n",
    "                cached in the .pipetorchuser folder or used from the .pipetorch folder\n",
    "                if present.\n",
    "            **kwargs: see the TextCollection constructor for additional arguments\n",
    "                      such as language, min_freq, vocab, labels, special, collate \n",
    "        Returns: TextCollection\n",
    "        \"\"\"\n",
    "        try:\n",
    "            train_iter, test_iter = read_torchtext( func )\n",
    "        except:\n",
    "            raise ValueError(f'cannot convert given function {func} to an iterator. Is it a torchtext data function?')\n",
    "        return cls.from_iter(train_iter=train_iter, test_iter=test_iter, min_freq=min_freq, collate=collate, **kwargs)\n",
    "            \n",
    "    @property\n",
    "    def _collate(self):\n",
    "        return self.__collate\n",
    "    \n",
    "    def collate(self, collate):\n",
    "        \"\"\"\n",
    "        Return a textcollection for which the dataset is prepared as:\n",
    "        'pad': the sentences ae padded with a '<pad>' token to have equal length\n",
    "        'offset': a batch is a list of tokenids and a seperate list of offsets indicate where a new sentence starts.\n",
    "        ...: a cusrom collation function\n",
    "        \n",
    "        There are several options in TorchText to train with either padded of offset datasets.\n",
    "        \"\"\"\n",
    "        if collate == 'offset':\n",
    "            return OffsetTextCollection(self.train, test=self.test, valid=self.valid, \n",
    "                    language=self.language, min_freq=self.min_freq, vocab=self._vocab, labels = self.labels, \n",
    "                    specials=self.specials)\n",
    "        if collate == 'pad':\n",
    "            return PaddedTextCollection(self.train, test=self.test, valid=self.valid,  \n",
    "                    language=self.language, min_freq=self.min_freq, vocab=self._vocab, labels = self.labels, \n",
    "                    specials=self.specials)\n",
    "        r = TextCollection(self.train, test=self.test, valid=self.valid, \n",
    "                    language=self.language, min_freq=self.min_freq, vocab=self._vocab, labels = self.labels, \n",
    "                    specials=self.specials)\n",
    "        r.__collate = collate\n",
    "        return r   \n",
    "    \n",
    "    def split(self, *perc, test_perc=None, valid_perc=None):\n",
    "        \"\"\"\n",
    "        Creates out-of-sample test and valid sets.\n",
    "        \n",
    "        Args:\n",
    "            perc, test_perc, valid_perc: float[0-1]\n",
    "                fraction of the data that is used resp. for the test and valid set.\n",
    "                When a fixed test set is used, the first positional argument is used as\n",
    "                valid_perc, otherwise as test_perc.\n",
    "\n",
    "        Returns: TextCollection\n",
    "            a shallow copy of this TextCollection that is configured to \n",
    "            split the data in a train, test and/or valid set\n",
    "\n",
    "        Note: you cannot resplit a textcollection that was already split, or if it has a fixed validation or test set.\n",
    "        \"\"\"\n",
    "        assert test_perc is None or self.test is None, 'You cannot specify a test_perc if a fixed test set is given'\n",
    "        assert self.valid is None, 'You cannot resplit a text collection that was already split or that has a fixed validation set'\n",
    "        assert self.vocab_not_exists, 'You cannot split a text collection when a vocabulary is already built'\n",
    "        assert len(perc) < 2 or test_perc is None or valid_perc is None, 'Too many percentages'\n",
    "        assert len(perc) < 3, 'Too many percentages'\n",
    "        if len(perc) > 0:\n",
    "            if test_perc is None:\n",
    "                if self.test:\n",
    "                    valid_perc = perc[0]\n",
    "                else:\n",
    "                    test_perc = perc[0]\n",
    "            else:\n",
    "                valid_perc = perc[0]\n",
    "            if len(perc) == 2:\n",
    "                test_perc, valid_perc = 1\n",
    "                \n",
    "        r = copy.copy(self)\n",
    "        test_count = 0 if test_perc is None else round(test_perc * len(r.train))\n",
    "        valid_count = 0 if valid_perc is None else round(valid_perc * len(r.train))\n",
    "        train_count = len(r.train) - test_count - valid_count\n",
    "        #print(len(r.train), train_count, valid_count, test_count)\n",
    "        #print(len(r.train), sum([train_count, valid_count, test_count]))\n",
    "        r.train, r.test, r.valid = random_split(r.train, [train_count, test_count, valid_count])\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def language(self):\n",
    "        return self._language\n",
    "        \n",
    "    @language.setter\n",
    "    def language(self, value):\n",
    "        assert self.vocab_not_exists, 'You cannot specify a language after the vocabulary was built'\n",
    "        if value is not None:\n",
    "            self._language = value\n",
    "            try:\n",
    "                del self._tokenizer\n",
    "                del self._vocab\n",
    "                del self._labels\n",
    "            except: pass\n",
    "        \n",
    "    @property\n",
    "    def min_freq(self):\n",
    "        return self._min_freq\n",
    "        \n",
    "    @min_freq.setter\n",
    "    def min_freq(self, value):\n",
    "        assert self.vocab_not_exists, 'You cannot specify a min_freq after the vocabulary was built'\n",
    "        if value is not None:\n",
    "            self._min_freq = value\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        \"\"\"\n",
    "        The tokenizer used to transform a text into a list of words/tokens.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._tokenizer\n",
    "        except:\n",
    "            from torchtext.data.utils import get_tokenizer\n",
    "            self._tokenizer = get_tokenizer(self.language)\n",
    "            return self._tokenizer\n",
    "\n",
    "    @tokenizer.setter\n",
    "    def tokenizer(self, value):\n",
    "        assert self.vocab_not_exists, 'You cannot change the tokenizer after the vocabulary was built'\n",
    "        self._tokenizer = value\n",
    "        \n",
    "    @property\n",
    "    def vocab_not_exists(self):\n",
    "        return self._vocab is None\n",
    "        \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        \"\"\"\n",
    "        A (generated) PyTorch Vocab that maps all tokens in the training set to numbers.\n",
    "        \"\"\"\n",
    "        if self._vocab is None:\n",
    "            self._build_vocab()\n",
    "        return self._vocab\n",
    "    \n",
    "    @vocab.setter\n",
    "    def vocab(self, value):\n",
    "        self._vocab = value\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"\n",
    "        Collection of groud truth labels for the TextCollection\n",
    "        \"\"\"\n",
    "        if self._labels is None:\n",
    "            self._build_vocab()\n",
    "        return self._labels\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self._labels = value\n",
    "            \n",
    "    def decode_sentence_index(self, words, index):\n",
    "        \"\"\"\n",
    "        returns a list of tokens for the text at index in words.\n",
    "        \"\"\"\n",
    "        return self.decode_sentence(words[index])\n",
    "    \n",
    "    def decode_sentence(self, tokenids):\n",
    "        # lookup_tokens on vocab transforms List(int) -> List(str)\n",
    "        return self.vocab.lookup_tokens(list(tokenids))\n",
    "\n",
    "    def encode_sentence(self, sentence):\n",
    "        # lookup_indices on vocab transforms List(str) -> List(int)\n",
    "        return self.vocab.lookup_indices(self.tokenizer(sentence))\n",
    "\n",
    "    def token_iterator(self, dataset):\n",
    "        \"\"\"\n",
    "        returns: iterator over lists of tokenids that represent the original sentences\n",
    "        may be overriden by subclassing TextCollection\n",
    "        \"\"\"\n",
    "        def yield_tokens(dataset):\n",
    "            for _, text in dataset:\n",
    "                yield self.tokenizer(text)\n",
    "        return yield_tokens(dataset)\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "        labels = Counter([ l for l, _ in self.train ])\n",
    "        self._vocab = build_vocab_from_iterator(self.token_iterator(self.train), specials=self.specials, special_first=True, min_freq=self.min_freq)\n",
    "        self._vocab.set_default_index(self._vocab['<unk>'])\n",
    "        self._labels = LabelSet(labels)\n",
    "        \n",
    "    def to_databunch(self, batch_size=32, shuffle=True, balance=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Return: a databunch (object with a dataloader for the train, valid, (test) set)\n",
    "        batch_size (None): the batch_size used by the DataLoader\n",
    "        shuffle (True): whether the samples are shuffled between epochs\n",
    "        balance (False): if the training set is to be balanced (only works for binary classification)\n",
    "        **kwargs: any other named argument that Databunch accepts\n",
    "        \n",
    "        Note: creating a databunch does not modify the original TextCollection, instead a shallow copy is made\n",
    "        and creating the databunch usually triggers split, balance and creation of the vocabulary to be triggered.\n",
    "        You can access the shallow copy of the textcollection through db.textcollection and the generated \n",
    "        vocabulary through db.vocab.\n",
    "        \"\"\"\n",
    "        r = copy.copy(self)\n",
    "        db = Databunch(None, r.train, r.test, r.valid, batch_size=batch_size, test_batch_size=batch_size, shuffle=shuffle, balance=balance, collate=r._collate, **kwargs)\n",
    "        db.textcollection = r\n",
    "        db.vocab = r.vocab\n",
    "        if r._using_pretrained():\n",
    "            db.pretrained_embeddings = r.pretrained_embeddings\n",
    "            db.load_pretrained_embeddings_in_layer = r.load_pretrained_embeddings_in_layer\n",
    "        return db\n",
    "    \n",
    "    def GloVe(self, name='6B', cache='/data/datasets', max_vectors=None):\n",
    "        \"\"\"\n",
    "        Setup the use of pretrained GloVe embeddings. There are 4 sets available for download:\n",
    "        name (6B): 6B, 42B, 840B or twitter.27B\n",
    "        cache: shared folder to store downloaded embeddings. Do not change unless you know what you are doing\n",
    "        since embeddings take up a lot of storage.\n",
    "        max_vectors: use only the indicated number of vectors to save RAM. Since the tokens are sorted on frequency,\n",
    "        using only the n most frequently appearing tokens works well in most cases.\n",
    "        \"\"\"\n",
    "        from torchtext.vocab import GloVe\n",
    "        r = copy.copy(self)\n",
    "        r._pretrained = GloVe\n",
    "        r._pretrained_params = {'name':name, 'cache':cache, 'max_vectors':max_vectors}\n",
    "        return r\n",
    "    \n",
    "    def _using_pretrained(self):\n",
    "        try:\n",
    "            self._pretrained\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def pretrained_embeddings(self, dim):\n",
    "        assert self._using_pretrained(), 'Can only prepare a table of pretrained embeddings if you configure using GloVe'\n",
    "        self._pretrained_params['dim'] = dim\n",
    "        self._embeddings = self._pretrained(**self._pretrained_params).get_vecs_by_tokens(self.vocab.get_itos())\n",
    "        return self._embeddings\n",
    "    \n",
    "    def load_pretrained_embeddings_in_layer(self, layer):\n",
    "        dim = embedding_layer.embedding_dim\n",
    "        embedding_layer.weight.data = self.pretrained_embeddings(dim)              \n",
    "        \n",
    "class PaddedTextCollection(TextCollection):\n",
    "    def _collate(self, batch):\n",
    "        label_list, text_list = [], []\n",
    "        for (_label, _text) in batch:\n",
    "            label_list.append(self.labels[_label])\n",
    "            processed_text = torch.tensor(self.encode_sentence(_text), dtype=torch.int64)\n",
    "            text_list.append(processed_text)\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        text_list = pad_sequence(text_list, batch_first=True, padding_value=self.vocab['<pad>'])\n",
    "        return text_list, label_list\n",
    "\n",
    "    def decode_sentence_index(self, words, index):\n",
    "        return super().decode_sentence(words[index].squeeze())\n",
    "        \n",
    "    def decode_sentences(self, words):\n",
    "        return [ self.decode_sentence(words, i) for i in range(len(words))]\n",
    "    \n",
    "class OffsetTextCollection(TextCollection):\n",
    "    def _collate(self, batch):\n",
    "        label_list, text_list, offsets = [], [], [0]\n",
    "        for (_label, _text) in batch:\n",
    "            label_list.append(self.labels[_label])\n",
    "            processed_text = torch.tensor(self.encode_sentence(_text), dtype=torch.int64)\n",
    "            text_list.append(processed_text)\n",
    "            offsets.append(processed_text.size(0))\n",
    "        label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "        offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "        text_list = torch.cat(text_list)\n",
    "        return text_list, offsets, label_list\n",
    "\n",
    "    def decode_sentence_index(self, words, offsets, index):\n",
    "        start = offsets[index]\n",
    "        end = offsets[index+1] if index < len(offsets)-1 else len(words)\n",
    "        return super().decode_sentence(words[start:end])\n",
    "        \n",
    "    def decode_sentences(self, words, offsets):\n",
    "        return [ self.decode_sentence(words, offsets, i) for i in range(len(offsets))]\n",
    "    \n",
    "class LabelSet:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self._itol = list(labels.keys())\n",
    "        self._ltoi = {l:i for i, l in enumerate(self._itol)}\n",
    "        \n",
    "    def __getitem__(self, label):\n",
    "        return self._ltoi[label]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._itol)\n",
    "    \n",
    "    def lookup_labels(self, labels):\n",
    "        return [self._ltoi[l] for l in labels]\n",
    "    \n",
    "    def lookup_ints(self, ints):\n",
    "        return [self._itol[i] for i in ints]\n",
    "\n",
    "class TextDataSet:    \n",
    "    @classmethod\n",
    "    def from_iter(cls, it):\n",
    "        r = cls()\n",
    "        r.data = list(it)\n",
    "        return r\n",
    "        \n",
    "    @classmethod\n",
    "    def from_csv(cls, filename, header=True, delimiter=','):\n",
    "        r = cls()\n",
    "        with open(filename) as fin:\n",
    "            r.data = [ l.split(delimiter, 1) for l in fin ]\n",
    "        if header:\n",
    "            r.data = r.data[1:]\n",
    "        return r\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data.__getitem__(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514db33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92ee92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
