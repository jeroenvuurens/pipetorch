{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5dfc0a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dframe.py        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import linecache\n",
    "from ..evaluate.evaluate import Evaluator\n",
    "from .helper import read_from_package, read_from_function\n",
    "from .kagglereader import Kaggle\n",
    "from .databunch import Databunch\n",
    "from .dset import DSet\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "import functools\n",
    "from pandas.util import hash_pandas_object\n",
    "import hashlib\n",
    "from pandas.util._exceptions import find_stack_level\n",
    "import warnings\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "class show_warning:\n",
    "    def __enter__(self):\n",
    "        self.warning = warnings.catch_warnings(record=True)\n",
    "        self.w = self.warning.__enter__()\n",
    "        warnings.filterwarnings('error')\n",
    "        warnings.simplefilter('default')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        for wi in self.w:\n",
    "            if wi.line is None:\n",
    "                print(wi.filename)\n",
    "                wi.line = linecache.getline(wi.filename, wi.lineno)\n",
    "        print(f'line number {wi.lineno}  line {wi.line}') \n",
    "        self.warning.__exit__(exc_type, exc_value, exc_traceback)\n",
    "        \n",
    "class _DFrame:\n",
    "    _config   = ['_pt_scale_columns', '_pt_scale_omit_interval', '_pt_scalertype', '_pt_columny', '_pt_columnx', \n",
    "                 '_pt_vectory', '_pt_bias', '_pt_polynomials', '_pt_dtype', '_pt_category', '_pt_category_sort', \n",
    "                 '_pt_dummies', '_pt_evaluator', '_pt_transform',\n",
    "                 '_pt_dataset_transforms', '_pt_dataset_train_transforms' ]\n",
    "    \n",
    "    _config_indices = [\n",
    "                 '_pt_sequence_window', '_pt_sequence_shift_y', \n",
    "                 '_pt_split_shuffle', '_pt_split_stratify', '_pt_split_stratify_test', \n",
    "                 '_pt_folds_stratify', '_pt_folds_random_state', \n",
    "                 '_pt_split_random_state', '_pt_folds_shuffle', \n",
    "                 '_pt_valid_size', '_pt_test_size', '_pt_balance', '_pt_filterna', \n",
    "                 '_pt_train_valid_indices', '_pt_test_indices', '_pt_folds', '_pt_fold',\n",
    "                 '_cached_index' ]\n",
    "    \n",
    "    _config_fold = [ '_pt_fold' ]\n",
    "    \n",
    "    _cached =  [ #'_cached_changed', #'_cached_changed_fold', \n",
    "                 '_cached_train', '_cached_valid', '_cached_test', '_cached_raw_train',\n",
    "                 '_cached_scalerx', '_cached_scalery',\n",
    "                 '_cached_categoryx', '_cached_categoryy',\n",
    "                 '_cached_columntransformerx', '_cached_columntransformery',\n",
    "                 '_cached_dummiesx', '_cached_dummiesy']\n",
    "                \n",
    "    _cached_indices = [ '_cached_train_indices', '_cached_valid_indices', '_cached_test_indices',\n",
    "                        '_cached_indices_before_testsplit', '_cached_indices_after_testsplit' ]\n",
    "    \n",
    "    _cached_fold = [ '_cached_folds' ]\n",
    "\n",
    "    _metadata = _config + _config_indices + _config_fold + _cached + _cached_indices + _cached_fold\n",
    "    _config_set = set( _config )\n",
    "    _config_indices_set = set( _config_indices )\n",
    "    _config_fold_set = set( _config_fold )\n",
    "    \n",
    "    _internal_names = pd.DataFrame._internal_names + _metadata\n",
    "    \n",
    "    _internal_names_set = set( _internal_names )\n",
    "    \n",
    "    def __init__(self, data, **kwargs):\n",
    "        for m in self._config:\n",
    "            setattr(self, m, None)\n",
    "        for m in self._config_indices:\n",
    "            setattr(self, m, None)\n",
    "        self._index_changed()\n",
    "        for m in self._metadata:\n",
    "            try:\n",
    "                setattr(self, m, getattr(data, m))\n",
    "            except: pass\n",
    "    \n",
    "    @classmethod\n",
    "    def stop_warnings(cls):\n",
    "        pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    def _copy_meta(self, r):\n",
    "        for c in self._metadata:\n",
    "            try:\n",
    "                setattr(r, c, getattr(self, c))\n",
    "            except: pass\n",
    "        return r\n",
    "\n",
    "    def _dframe(self, data):\n",
    "        return self._copy_meta( DFrame(data) )\n",
    "\n",
    "    def _dset(self, data, transforms=None):\n",
    "        return DSet.from_dframe(data, self, transforms)\n",
    "        \n",
    "    @property\n",
    "    def _columny(self):\n",
    "        try:\n",
    "            if len(self._pt_columny) > 0:\n",
    "                return self._pt_columny\n",
    "        except: pass\n",
    "        return [ self.columns[-1] ]\n",
    "        \n",
    "    def dtype(self, dtype):\n",
    "        \"\"\"\n",
    "        Lazily executed change of dtype to the data.\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_dtype', dtype)\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def _columnx(self):\n",
    "        if self._pt_columnx is None:\n",
    "            return [ c for c in self.columns if c not in self._columny ]\n",
    "        return [ c for c in self._pt_columnx if c not in self._columny ]\n",
    "   \n",
    "    @property\n",
    "    def _all_columns(self):\n",
    "        return list(set(self._columnx).union(set(self._columny)))\n",
    "\n",
    "    @property\n",
    "    def _columnsx_scale_indices(self):\n",
    "        if self._pt_polynomials is not None and self._pt_scale_columns is not None:\n",
    "            X = self.raw_train._x_polynomials\n",
    "            return [ i for i in range(X.shape[1]) if (X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1]) ]\n",
    "        columnx = self._columnx\n",
    "        cat = set(self._pt_category) if type(self._pt_category) == tuple else []\n",
    "        if self._pt_scale_columns == True or self._pt_scale_columns == 'x_only':\n",
    "            r = [ c for c in columnx if c not in cat]\n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            r = []\n",
    "        else:\n",
    "            r = [ c for c in columnx if c in self._pt_scale_columns and c not in cat ]\n",
    "        if len(r) > 0:\n",
    "            X = self.raw_train._x_polynomials\n",
    "            r = [ columnx.index(c) for i, c in enumerate(columnx) if c in r and ((X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1])) ]\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def _columnsy_scale_indices(self):\n",
    "        columny = self._columny\n",
    "        cat = set(self._pt_category) if type(self._pt_category) == tuple else []\n",
    "        if self._pt_scale_columns == True:\n",
    "            y = self.raw_train._y_numpy\n",
    "            r = [ c for i, c in enumerate(columny) if c not in cat and (y[:,i].min() < self._pt_scale_omit_interval[0] or y[:,i].max() > self._pt_scale_omit_interval[1]) ]\n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            r = []\n",
    "        else:\n",
    "            r = [ c for c in columny if c in self._pt_scale_columns and c not in cat ]\n",
    "        return [ columny.index(c) for c in r ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_scaler(scalertype, column):\n",
    "        scaler = scalertype()\n",
    "        scaler.fit(column)\n",
    "        return scaler\n",
    "    \n",
    "    @property\n",
    "    def _scalerx(self):\n",
    "        if self._unchanged() and self._cached_scalerx is not None:\n",
    "            return self._cached_scalerx\n",
    "        \n",
    "        X = self.raw_train._x_polynomials\n",
    "        r = [ None ] * X.shape[1]\n",
    "        for i in self._columnsx_scale_indices:\n",
    "            r[i] = self._create_scaler(self._pt_scalertype, X[:, i:i+1])\n",
    "        self._change('_cached_scalerx', r)\n",
    "        return r\n",
    "     \n",
    "    @_scalerx.setter\n",
    "    def _scalerx(self, value):\n",
    "        self._change('_cached_scalerx', value)\n",
    "        \n",
    "    @property\n",
    "    def _scalery(self):\n",
    "        if self._unchanged() and self._cached_scalery is not None:\n",
    "            return self._cached_scalery\n",
    "        \n",
    "        y = self.raw_train._y_numpy\n",
    "        r = [ None ] * y.shape[1]\n",
    "        for i in self._columnsy_scale_indices:\n",
    "            r[i] = self._create_scaler(self._pt_scalertype, y[:, i:i+1])\n",
    "        self._change('_cached_scalery', r)\n",
    "        return r\n",
    "    \n",
    "    @_scalery.setter\n",
    "    def _scalery(self, value):\n",
    "        self._change('_cached_scalery', value)\n",
    "        \n",
    "    def _create_category(self, column):\n",
    "        sort = self._pt_category_sort\n",
    "        class Category:\n",
    "            def fit(self, X):\n",
    "                s = X.unique()\n",
    "                if sort:\n",
    "                    s = sorted(s)\n",
    "                self.dict = defaultdict(lambda:0, { v:(i+1) for i, v in enumerate(s) })\n",
    "                self.inverse_dict = { (i+1):v for i, v in enumerate(s) }\n",
    "                self.inverse_dict[0] = np.NaN\n",
    "            \n",
    "            def transform(self, X):\n",
    "                return X.map(self.dict)\n",
    "            \n",
    "            def inverse_transform(self, X):\n",
    "                return X.map(self.inverse_dict)\n",
    "            \n",
    "        if column not in self._pt_category:\n",
    "            return None\n",
    "        \n",
    "        c = Category()\n",
    "        c.fit(self.raw_train[column])\n",
    "        return c\n",
    "    \n",
    "    def _categoryx(self):\n",
    "        if self._unchanged() and self._cached_categoryx is not None:\n",
    "            return self._cached_categoryx\n",
    "        \n",
    "        if self._pt_category is None or len(self._pt_category) == 0:\n",
    "            return None\n",
    "        r = [ self._create_category(c) for c in self._columnx ]\n",
    "        self._change('_cached_categoryx', r)\n",
    "        return self._cached_categoryx\n",
    "    \n",
    "    def _categoryy(self):\n",
    "        if self._unchanged() and self._cached_categoryy is not None:\n",
    "            return self._cached_categoryy\n",
    "        \n",
    "        if self._pt_category is None or len(self._pt_category) == 0:\n",
    "            return None\n",
    "        r = [ self._create_category(c) for c in self._columny ] \n",
    "        self._change('_cached_categoryy', r)\n",
    "        return r\n",
    "\n",
    "    def _columntransformerx(self):\n",
    "        if self._unchanged() and self._cached_columntransformerx is not None:\n",
    "            return self._cached_columntransformerx\n",
    "        \n",
    "        if self._pt_transform is None or len(self._pt_transform) == 0:\n",
    "            return None\n",
    "        r = []\n",
    "        for c in self._columnx:\n",
    "            try:\n",
    "                t = self._pt_transform[c]\n",
    "                t.fit(self.raw_train[c])\n",
    "                r.append(t)\n",
    "            except:\n",
    "                r.append(None)\n",
    "                \n",
    "        self._change('_cached_columntransformerx', r)\n",
    "        return self._cached_columntransformerx\n",
    "        \n",
    "    def _columntransformery(self):\n",
    "        if self._unchanged() and self._cached_columntransformery is not None:\n",
    "            return self._cached_columntransformery\n",
    "        \n",
    "        if self._pt_transform is None or len(self._pt_transform) == 0:\n",
    "            return None\n",
    "        r = []\n",
    "        for c in self._columny:\n",
    "            try:\n",
    "                t = self._pt_transform[c]\n",
    "                t.fit(self.raw_train[c])\n",
    "                r.append(t)\n",
    "            except:\n",
    "                r.append(None)\n",
    "                \n",
    "        self._change('_cached_columntransformery', r)\n",
    "        return self._cached_columntransformery\n",
    "    \n",
    "    def _create_dummies(self, column):    \n",
    "        if column not in self._pt_dummies:\n",
    "            return None\n",
    "        \n",
    "        c = OneHotEncoder(handle_unknown='ignore')\n",
    "        c.fit(self.raw_train[[column]])\n",
    "        return c\n",
    "    \n",
    "    def _dummiesx(self):\n",
    "        if self._unchanged() and self._cached_dummiesx is not None:\n",
    "            return self._cached_dummiesx\n",
    "        \n",
    "        if self._pt_dummies is None or len(self._pt_dummies) == 0:\n",
    "            r = [ None ] * len(self._columnx)\n",
    "        else:\n",
    "            r = [ self._create_dummies(c) for c in self._columnx ]\n",
    "        self._change('_cached_dummiesx', r)\n",
    "        return r\n",
    "    \n",
    "    def _dummiesy(self):\n",
    "        if self._unchanged() and self._cached_dummiesy is not None:\n",
    "            return self._cached_dummiesy\n",
    "        \n",
    "        if self._pt_dummies is None or len(self._pt_dummies) == 0:\n",
    "            r = [ None ] * len(self._columny)\n",
    "        else:\n",
    "            r = [ self._create_dummies(c) for c in self._columny ]\n",
    "        self._change('_cached_dummiesy', r)\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def _shift_y(self):\n",
    "        if self._pt_sequence_shift_y is not None:\n",
    "            return self._pt_sequence_shift_y\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @property\n",
    "    def _sequence_window(self):\n",
    "        try:\n",
    "            if self._pt_sequence_window is not None:\n",
    "                return self._pt_sequence_window\n",
    "        except:pass\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def _sequence_index_y(self):\n",
    "        return self._sequence_window+self._shift_y-1\n",
    "        \n",
    "    @property\n",
    "    def _indices_unshuffled(self):\n",
    "        if self._pt_train_valid_indices is not None:\n",
    "            return np.intersect1d(self.index.values, self._pt_train_valid_indices)\n",
    "        return self.index.values\n",
    "\n",
    "    @property\n",
    "    def _indices_before_testsplit(self):\n",
    "        if self._unchanged() and self._cached_indices_before_testsplit is not None:\n",
    "            return self._cached_indices_before_testsplit\n",
    "        \n",
    "        test_indices = set(self.fixed_test_indices)\n",
    "        if len(test_indices) > 0:\n",
    "            r = np.array([ i for i in self._indices_unshuffled if i not in test_indices ])\n",
    "        else:\n",
    "            r = self._indices_unshuffled\n",
    "        self._change('_cached_indices_before_testsplit', r)\n",
    "        return r\n",
    "           \n",
    "    @property\n",
    "    def _indices_after_testsplit(self):\n",
    "        if self._unchanged() and self._cached_indices_after_testsplit is not None:\n",
    "            return self._cached_indices_after_testsplit\n",
    "        \n",
    "        test_indices = set(self._test_indices)\n",
    "        r = np.array([ i for i in self._indices_before_testsplit if i not in test_indices ])\n",
    "        self._change('_cached_indices_after_testsplit', r)\n",
    "        return r\n",
    "\n",
    "    @property\n",
    "    def _test_size(self):\n",
    "        return self._pt_test_size or 0\n",
    "    \n",
    "    @property\n",
    "    def _valid_size(self):\n",
    "        return self._pt_valid_size or 0\n",
    "    \n",
    "    @property\n",
    "    def _shuffle(self):\n",
    "        return ((self._pt_split_shuffle is None and \\\n",
    "                (self._test_size > 0 or self._valid_size > 0)) \\\n",
    "                or self._pt_split_shuffle) and self._pt_sequence_window is None\n",
    "    \n",
    "    def _stratifyable_columns(self, indices, columns, bins):\n",
    "        if columns == True:\n",
    "            columns = self._all_columns\n",
    "        df = self.loc[indices, columns]\n",
    "        if bins > 1:\n",
    "            bins = len(df) // bins\n",
    "        else:\n",
    "            if bins >= 0.5:\n",
    "                bins = 1 - bins\n",
    "            bins = math.ceil(1 / bins)\n",
    "            bins = len(df) // bins\n",
    "        for c in df.columns:\n",
    "            if pd.api.types.is_float_dtype(df[c]):\n",
    "                df[c] = pd.qcut(df[c] + np.random.random(len(df))/1000, bins, labels=False)\n",
    "        return df\n",
    "    \n",
    "    @property\n",
    "    def fixed_test_indices(self):\n",
    "        if self._pt_test_indices is not None:\n",
    "            r = np.intersect1d(self.index.values, self._pt_test_indices)\n",
    "            if len(r) < len(self._pt_test_indices):\n",
    "                warnings.warn(\"Test rows were lost because of a previous operation.\",\n",
    "                    RuntimeWarning,\n",
    "                )\n",
    "                self._pt_test_indices = r\n",
    "            return self._pt_test_indices\n",
    "        return []\n",
    "        \n",
    "    @property\n",
    "    def _test_indices(self):\n",
    "        if self._pt_test_indices is not None:\n",
    "            return self.fixed_test_indices\n",
    "        if self._unchanged() and self._cached_test_indices is not None:\n",
    "            return self._cached_test_indices\n",
    "        if self._pt_folds is not None and self._test_size == 1:\n",
    "            r = self._test_fold\n",
    "        elif self._test_size > 0:\n",
    "            if self._pt_split_stratify_test is None:\n",
    "                if self._shuffle:\n",
    "                    _, r = train_test_split(self._indices_before_testsplit, test_size=self._test_size, random_state=self._pt_split_random_state)\n",
    "                    r = sorted(r)\n",
    "                else:\n",
    "                    test_size = int(self._test_size * len(self._indices_before_testsplit))\n",
    "                    r = self._indices_before_testsplit[-test_size:]\n",
    "            else:\n",
    "                if self._pt_split_stratify_test == True or len(self._pt_split_stratify_test) > 1:\n",
    "                    splitter = MultilabelStratifiedShuffleSplit(n_splits=1, \n",
    "                                                       random_state=self._pt_split_random_state, \n",
    "                                                       test_size=self._test_size)\n",
    "                else:\n",
    "                    splitter = StratifiedShuffleSplit(n_splits=1, \n",
    "                                                       random_state=self._pt_split_random_state, \n",
    "                                                       test_size=self._test_size) \n",
    "                target = self._stratifyable_columns(self._indices_before_testsplit, \n",
    "                                                    self._pt_split_stratify_test, self._test_size / len(self))\n",
    "                _, r = next(splitter.split(target, target))\n",
    "                r = sorted(r)\n",
    "        else:\n",
    "            r = []  \n",
    "        self._change('_cached_test_indices', r)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _train_indices_unbalanced(self):\n",
    "        return sorted(set(self._indices_after_testsplit) - set(self._valid_indices))\n",
    "\n",
    "    def _pseudo_choose(self, indices, items):\n",
    "        if self._pt_split_random_state is not None:\n",
    "            random.seed(self._pt_split_random_state)\n",
    "        r = np.array(random.sample(sorted(indices), items % len(indices)))\n",
    "        r = np.hstack([indices for i in range(items // len(indices))] + [r])\n",
    "        return r\n",
    "\n",
    "    @property\n",
    "    def _train_indices(self):\n",
    "        if self._unchanged() and self._cached_train_indices is not None:\n",
    "            return self._cached_train_indices\n",
    "        r = self._train_indices_unbalanced\n",
    "        if self._pt_balance is not None:\n",
    "            y = self.loc[r, self._columny]\n",
    "            classes = np.unique(y)\n",
    "            classindices = {c:np.where(y==c)[0] for c in classes}\n",
    "            classlengths = {c:len(indices) for c, indices in classindices.items()}\n",
    "            if self._pt_balance == True: # equal classes\n",
    "                n = max(classlengths.values())\n",
    "                mask = np.hstack([self._pseudo_choose(classindices[c], n) for c in classes])\n",
    "            else:                        # use given weights\n",
    "                weights = self._pt_balance\n",
    "                n = max([ int(math.ceil(classlengths[c] / w)) for c, w in weights.items() ])\n",
    "                mask = np.hstack([self._pseudo_choose(classindices[c], round(n*weights[c])) for c in classes])\n",
    "            r = np.array(r)[ mask.astype(int) ]\n",
    "        self._change('_cached_train_indices', r)\n",
    "        return r\n",
    "\n",
    "    @property\n",
    "    def _valid_indices(self):\n",
    "        if self._unchanged() and self._cached_valid_indices is not None:\n",
    "            return self._cached_valid_indices\n",
    "        if self._pt_folds is not None:\n",
    "            r = self._valid_fold\n",
    "        elif self._valid_size > 0:\n",
    "            if self._test_size < 1:\n",
    "                valid_size = self._valid_size / (1 - self._test_size)\n",
    "            else:\n",
    "                valid_size = self._valid_size\n",
    "            if valid_size > 0:\n",
    "                if self._pt_split_stratify is None:\n",
    "                    if self._shuffle:\n",
    "                        _, r = train_test_split(self._indices_after_testsplit, test_size=valid_size, random_state=self._pt_split_random_state)\n",
    "                        r = sorted(r)\n",
    "                    else:\n",
    "                        valid_size = int(valid_size * len(self._indices_before_testsplit))\n",
    "                        r = self._indices_after_testsplit[-valid_size:]                        \n",
    "                else:\n",
    "                    if self._pt_split_stratify == True or len(self._pt_split_stratify) > 1:\n",
    "                        splitter = MultilabelStratifiedShuffleSplit(n_splits=1, \n",
    "                                                           random_state=self._pt_split_random_state, \n",
    "                                                           test_size=valid_size)\n",
    "                    else:\n",
    "                        splitter = StratifiedShuffleSplit(n_splits=1, \n",
    "                                                           random_state=self._pt_split_random_state, \n",
    "                                                           test_size=valid_size) \n",
    "                    target = self._stratifyable_columns(self._indices_after_testsplit, self._pt_split_stratify, self._valid_size / (len(self)-self._test_size))\n",
    "                    _, r = next(splitter.split(target, target))\n",
    "                    r = sorted(r)\n",
    "        else:\n",
    "            r = []\n",
    "        self._change('_cached_valid_indices', r)\n",
    "        return r\n",
    "\n",
    "    @property\n",
    "    def _folds(self):\n",
    "        if self._unchanged() and self._cached_folds is not None:\n",
    "            return self._cached_folds\n",
    "        r = []\n",
    "        if 0 < self._test_size < 1:\n",
    "            indices = self._indices_after_testsplit\n",
    "        else:\n",
    "            indices = self._indices_before_testsplit\n",
    "        if self._pt_folds_stratify is None:\n",
    "            target = self.loc[indices]\n",
    "            splitter = KFold(n_splits = self._pt_folds, shuffle=self._pt_folds_shuffle, random_state=self._pt_folds_random_state)\n",
    "            for train_indices, valid_indices in splitter.split(target, target):\n",
    "                r.append(sorted(indices[valid_indices]))\n",
    "        else:\n",
    "            if self._pt_folds_stratify != True and len(self._pt_folds_stratify) > 1:\n",
    "                splitter = MultilabelStratifiedKFold(n_splits = self._pt_folds,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=self._pt_folds_random_state)\n",
    "            else:\n",
    "                splitter = StratifiedKFold(n_splits = self._pt_folds,\n",
    "                                    shuffle=True,\n",
    "                                    random_state=self._pt_folds_random_state)\n",
    "            target = self._stratifyable_columns(indices, self._pt_folds_stratify, self._pt_folds)\n",
    "            for train_indices, valid_indices in splitter.split(target, target):\n",
    "                r.append(sorted(indices[valid_indices]))\n",
    "        self._change('_cached_folds', r)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _fold(self):\n",
    "        \"\"\"\n",
    "        The current valid fold number, set by df.fold(i)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self._pt_fold + 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    @property\n",
    "    def _test_fold(self):\n",
    "        \"\"\"\n",
    "        the current test fold, determined by df.fold(i) + 1\n",
    "        \"\"\"\n",
    "        test_fold = (self._fold + 1 + (self._fold // self._pt_folds % (self._pt_folds - 1))) % self._pt_folds\n",
    "        return self._folds[test_fold]\n",
    "    \n",
    "    @property\n",
    "    def _valid_fold(self):\n",
    "        \"\"\"\n",
    "        The current valid fold, determined by df.fold(i)\n",
    "        \"\"\"\n",
    "        return self._folds[self._fold]\n",
    "    \n",
    "    def fold(self, i):\n",
    "        \"\"\"\n",
    "        Utilize n-fold cross validation by first calling `folds(n)` and then calling `fold(i)` to obtain \n",
    "        a DFrame in which cross validation is set up using the fold (i) as the validation set, \n",
    "        another fold as the test set and the remainder as the training set.\n",
    "        \n",
    "        A first call will trigger rows to be assigned to the train, valid and test part, which are stored\n",
    "        in place to reproduce the exact same split for consecutive calls. The call will however return\n",
    "        a copy of the DataFrame in which the requested fold is selected.\n",
    "\n",
    "        Arguments:\n",
    "            i: int\n",
    "                the fold to use as the validation set\n",
    "        \n",
    "        Returns: copy of the PipeTorch DataFrame\n",
    "            In this copy, the train, valid and test sets are shifted to fold n\n",
    "        \"\"\"\n",
    "        self._folds\n",
    "        r = self.copy(deep=False)\n",
    "        \n",
    "        i = i % r._pt_folds\n",
    "        r._change('_pt_fold', i)\n",
    "        return r\n",
    "    \n",
    "    def iterfolds(self):\n",
    "        \"\"\"\n",
    "        Iterate over the folds for n-fold cross validation. \n",
    "        \n",
    "        A first call will trigger rows to be assigned to the train, valid and test part, which are stored\n",
    "        in place to reproduce the exact same split for consecutive calls.\n",
    "        \n",
    "        Yields:\n",
    "            train, valid (DSet)\n",
    "        \"\"\"\n",
    "        for i in range(self._pt_folds):\n",
    "            yield self.fold(i)\n",
    "    \n",
    "    def df_to_dataset(self, df, datasetclass=None):\n",
    "        \"\"\"\n",
    "        Converts the given df to a DataSet using the pipeline of this DFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame or DFrame\n",
    "                to convert into a DataSet\n",
    "        \n",
    "        returns: Converts the given df to a DataSet.\n",
    "        \"\"\"\n",
    "        return self.df_to_dset(df).to_dataset(datasetclass)\n",
    "        \n",
    "    def to_datasets(self, datasetclass=None):\n",
    "        \"\"\"\n",
    "        Prepares the train, valid and (optionally) test subsets as a DSet, which can be used to complete \n",
    "        the data preparation.\n",
    "        \n",
    "        A first call will trigger rows to be assigned to the train, valid and test part, which are stored\n",
    "        in place to reproduce the exact same split for consecutive calls.\n",
    "\n",
    "        Arguments:\n",
    "            dataset: class (None)\n",
    "                The DataSet class to use\n",
    "        \n",
    "        Returns: list(DataSet)\n",
    "        \"\"\"\n",
    "        res = [ self.train.to_dataset(datasetclass) ]\n",
    "        if len(self._valid_indices) > 0:\n",
    "            res.append(self.valid.to_dataset(datasetclass))\n",
    "        if len(self._test_indices) > 0:\n",
    "            res.append(self.test.to_dataset(datasetclass))\n",
    "        return res\n",
    "        \n",
    "    def df_to_dset(self, df):\n",
    "        \"\"\"\n",
    "        Converts a DataFrame to a DSet that has the same pipeline as this DFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame\n",
    "        \n",
    "        Returns: DSet\n",
    "        \"\"\"\n",
    "        return self._dset(df)\n",
    "        \n",
    "    def to_databunch(self, datasetclass=None, batch_size=32, valid_batch_size=None, \n",
    "                     num_workers=0, shuffle=True, pin_memory=False, balance=False, \n",
    "                     collate=None):\n",
    "        \"\"\"\n",
    "        Prepare the data as a Databunch that contains dataloaders for the train, valid and test part.\n",
    "        \n",
    "        batch_size, num_workers, shuffle, pin_memory: see Databunch/Dataloader constructor.\n",
    "        \n",
    "        A first call will trigger rows to be assigned to the train, valid and test part, which are stored\n",
    "        in place to reproduce the exact same split for consecutive calls.\n",
    "\n",
    "        Returns: Databunch\n",
    "        \"\"\"\n",
    "        return Databunch(self, *self.to_datasets(datasetclass=datasetclass), \n",
    "                         batch_size=batch_size, valid_batch_size=valid_batch_size, \n",
    "                         num_workers=num_workers, shuffle=shuffle, \n",
    "                         pin_memory=pin_memory, balance=balance, collate=collate)    \n",
    "\n",
    "    def _evaluator(self, *metrics):\n",
    "        \"\"\"\n",
    "        Creates a PipeTorch Evaluator, that can be used to visualize the data, the results\n",
    "        of a model or cache learning/validation diagnostics. Since datasets are often\n",
    "        reused in repeated experiments, every call will create a new Evaluator to prevent\n",
    "        mixing results from different experiments. If you do want to compare results from\n",
    "        multiple experiments, create a single evaluator and reuse that for the experiments\n",
    "        that you wish to compare.\n",
    "        \n",
    "        Arguments:\n",
    "            *metrics: callable\n",
    "                One or more functions, that will take y_true, y_pred as parameter to\n",
    "                compute an evaluation metric. Typically, functions from SKLearn.metrics \n",
    "                can be used.\n",
    "        \n",
    "        returns: Evaluator\n",
    "        \"\"\"\n",
    "        return Evaluator(self, *metrics)\n",
    "   \n",
    "    def evaluator(self, *metrics):\n",
    "        try:\n",
    "            if self._pt_evaluator.metrics == metrics:\n",
    "                return self._pt_evaluator\n",
    "        except: pass\n",
    "        self._pt_evaluator = self._evaluator(*metrics)\n",
    "        return self._pt_evaluator\n",
    "\n",
    "    def from_numpy(self, x):\n",
    "        if x.shape[1] == len(self._columnx) + len(self._columny):\n",
    "            y = x[:,-len(self._columny):]\n",
    "            x = x[:,:-len(self._columny)]\n",
    "        elif x.shape[1] == len(self._columnx):\n",
    "            y = np.zeros((len(x), len(self._columny)))\n",
    "        else:\n",
    "            raise ValueError('x must either have as many columns in x or the entire df')\n",
    "        series = [ pd.Series(s.reshape(-1), name=c) for s, c in zip(x.T, self._columnx)]\n",
    "        series.extend([ pd.Series(s.reshape(-1), name=c) for s, c in zip(y.T, self._columny) ] )\n",
    "        df = pd.concat(series, axis=1)\n",
    "        return self._dset(df)\n",
    "    \n",
    "    def from_list(self, x):\n",
    "        return self.from_numpy(np.array(x))\n",
    "    \n",
    "    def _dset_indices(self, indices, transforms):\n",
    "        if self._pt_sequence_window is None:\n",
    "            return self._dset(self.loc[indices], transforms=transforms)\n",
    "        else:\n",
    "            try:\n",
    "                low, high = min(indices), max(indices) + self._sequence_window + self._shift_y - 1\n",
    "                return self._dset(self.loc[low:high], transforms=transforms)\n",
    "            except:\n",
    "                return self._dset(self.loc[:0], transforms=transforms)\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Prepares the train subset as a DSet and optionally trains transformation parameters, e.g. for image\n",
    "        normalization or text tokenization. Transformations are only supported for PyTorch DataSets.\n",
    "        \n",
    "        A first call will trigger rows to be assigned to the train, valid and test part, which are stored\n",
    "        in place to reproduce the exact same split for consecutive calls.\n",
    "        \n",
    "        Returns: DSet\n",
    "        \"\"\"\n",
    "        if self._unchanged() and self._cached_train is not None:\n",
    "            return self._cached_train\n",
    "        \n",
    "        r = self._dset_indices(self._train_indices, self._dataset_transforms())\n",
    "        if self._dataset_train_transformation_parameters(r):\n",
    "            r = self._dset_indices(self._train_indices, self._dataset_transforms())\n",
    "        self._change('_cached_train', r)\n",
    "        self._cached_raw_train = None\n",
    "        return r\n",
    "                    \n",
    "    @property\n",
    "    def raw_train(self):\n",
    "        if self._unchanged() and self._cached_raw_train is not None:\n",
    "            return self._cached_raw_train\n",
    "        self._cached_raw_train = self._dset_indices(self._train_indices, self._dataset_transforms())\n",
    "        return self._cached_raw_train\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        if self._unchanged() and self._cached_valid is not None:\n",
    "            return self._cached_valid\n",
    "        \n",
    "        r = self._dset_indices(self._valid_indices, self._dataset_transforms(train=False))\n",
    "        self._change('_cached_valid', r)\n",
    "        return r\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        if self._unchanged() and self._cached_test is not None:\n",
    "            return self._cached_test\n",
    "        \n",
    "        if self._pt_sequence_window is None:\n",
    "            r = DSet.df_to_testset(self.loc[self._test_indices], self, self._dataset_transforms(train=False))\n",
    "        else:\n",
    "            low, high = min(self._test_indices), max(self._test_indices) + self._sequence_window + self._shift_y - 1\n",
    "            r = DSet.df_to_testset(self.loc[low:high], self, self._dataset_transforms(train=False))\n",
    "        self._change('_cached_test', r)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.train.X\n",
    "            \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train.y\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.valid.X\n",
    "       \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid.y\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test.X\n",
    "    \n",
    "    def numpy(self):\n",
    "        \"\"\"\n",
    "        Returns: train_X, train_y, valid_X, valid_y\n",
    "        \"\"\"\n",
    "        return self.train_X, self.train_y, self.valid_X, self.valid_y\n",
    "        \n",
    "    def tensor(self):\n",
    "        return self.train.X_tensor, self.train.y_tensor, self.valid.X_tensor, self.valid.y_tensor\n",
    "    \n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test.y\n",
    "    \n",
    "    def loss_surface(self, model, loss, **kwargs):\n",
    "        self._evaluator(loss).loss_surface(model, loss, **kwargs)\n",
    "    \n",
    "    def inverse_scale_y(self, y):\n",
    "        \"\"\"\n",
    "        Inversely scale an output y vector.\n",
    "        \n",
    "        Arguments:\n",
    "        y: Numpy array or PyTorch tensor\n",
    "            with the output that where preprocessed by this DFrame or predicted by the model\n",
    "        \n",
    "        Return: Pandas DataFrame\n",
    "            That is reconstructed from Numpy arrays/Pytorch tensors\n",
    "            that are transformed back to the original scale. \n",
    "        \"\"\"\n",
    "        y = to_numpy(y)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1,1)\n",
    "        df = pd.DataFrame(self._inverse_scale(to_numpy(y), self._scalery, self._columny))\n",
    "        if self._categoryy() is not None:\n",
    "            for c, cat in zip(self._columny, self._categoryy()):\n",
    "                if cat is not None:\n",
    "                    df[c] = cat.inverse_transform(df[c])\n",
    "        return df\n",
    "\n",
    "    def add_column(self, y, indices, erase_y=True, columns=None):\n",
    "        \"\"\"\n",
    "        Intended for internal use. Adds a column with values for the target variable to the DataFrame. \n",
    "        When applicable, the transformation for the target variable is automatically inverted. \n",
    "        This is useful to evaluate or visualize results.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "        \n",
    "        Arguments:\n",
    "            y: Numpy/PyTorch array\n",
    "                values in the same for as those generated for the target variable \n",
    "                (possibly predictions) that will be added to the DataFrame\n",
    "            erase_y: bool (True)\n",
    "                whether the original target variable is removed \n",
    "        \n",
    "        Returns: DFrame \n",
    "        \"\"\"\n",
    "        df_y = self.inverse_scale_y(y)\n",
    "        r = self.copy()\n",
    "        if columns is None:\n",
    "            columns = [ c + '_pred' for c in self._columny ]\n",
    "        r[columns] = np.NaN\n",
    "        r.loc[r.index[indices], columns] = df_y.values\n",
    "        r = self._dframe(r)\n",
    "        r._cached_changed = True\n",
    "        return r\n",
    "    \n",
    "    def inverse_scale_X(self, X):\n",
    "        \"\"\"\n",
    "        Inversely scale an input X matrix.\n",
    "        \n",
    "        Arguments:\n",
    "        X: Numpy array or PyTorch tensor\n",
    "            with the input features that where preprocessed by this DataFrame\n",
    "        \n",
    "        Return: Pandas DataFrame\n",
    "            That is reconstructed from Numpy arrays/Pytorch tensors\n",
    "            that are transformed back to the orignal scale. \n",
    "        \"\"\"\n",
    "        if self._pt_bias:\n",
    "            X = X[:, 1:]\n",
    "        if self._pt_polynomials is not None:\n",
    "            X = X[:, :len(self._columnx)]\n",
    "        df = self._inverse_scale(to_numpy(X), self._scalerx[:len(self._columnx)], self._columnx)\n",
    "        if self._categoryx() is not None:\n",
    "            for c, cat in zip(self._columnx, self._categoryx()):\n",
    "                if cat is not None:\n",
    "                    df[c] = cat.inverse_transform(df[c])\n",
    "        return df\n",
    "\n",
    "    def _inverse_scale(self, data, scalerlist, columns):\n",
    "        data = to_numpy(data)\n",
    "        if scalerlist is not None:\n",
    "            data = [ data[:, i:i+1] if scaler is None else scaler.inverse_transform(data[:,i:i+1]) for i, scaler in enumerate(scalerlist) ]\n",
    "        series = [ pd.Series(x.reshape(-1), name=c) for x, c in zip(data, columns)]\n",
    "        return pd.concat(series, axis=1)\n",
    "\n",
    "    def inverse_scale(self, X, y, y_pred = None, cum=None):\n",
    "        \"\"\"\n",
    "        Reconstructs a DSet from Numpy arrays/Pytorch tensors\n",
    "        that are scaled back to the original scale. \n",
    "        This is useful for evaluation or to visualize results.\n",
    "        \n",
    "        Arguments:\n",
    "            X: Numpy array or PyTorch tensor \n",
    "                with the same format as input features that were generated by the DataFrame\n",
    "                \n",
    "            y: Numpy array or PyTorch tensor\n",
    "                with the same format as the target variable that was generated by the\n",
    "                DataFrame\n",
    "                \n",
    "            pred_y (optional): Numpy array or PyTorch tensor\n",
    "                with the same format as the target variable that was generated by the\n",
    "                DataFrame\n",
    "                \n",
    "            cum (optional): DSet\n",
    "                an dataset to add the results to, \n",
    "                to accumulate predictions over several mini-batches.\n",
    "        \n",
    "        Returns: DSet \n",
    "        \"\"\"\n",
    "        y = self.inverse_scale_y(y)\n",
    "        X = self.inverse_scale_X(X)\n",
    "        if y_pred is not None:\n",
    "            y_pred = self.inverse_scale_y(y_pred).add_suffix('_pred')\n",
    "            df = pd.concat([X, y, y_pred], axis=1)\n",
    "        else:\n",
    "            df = pd.concat([X, y], axis=1)\n",
    "        if cum is not None:\n",
    "            df = pd.concat([cum, df])\n",
    "        df = self._dset(df)\n",
    "        return df\n",
    "        \n",
    "    def plot_boundary(self, predict):\n",
    "        self._evaluator().plot_boundary(predict)\n",
    "\n",
    "    def balance(self, weights=True):\n",
    "        \"\"\"\n",
    "        Oversamples rows in the training set, so that the values of the target variable \n",
    "        are better balanced. Does not affect the valid/test set.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            weights: True or dict\n",
    "                when set to True, the target values of the training set are \n",
    "                uniformely distributed,\n",
    "                otherwise a dictionary can be passed that map target values to the \n",
    "                desired fraction of the training set (e.g. {0:0.4, 1:0.6}).\n",
    "        \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_balance', weights)\n",
    "        return r    \n",
    "  \n",
    "    def scale(self, columns=True, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        \"\"\"\n",
    "        Scales the features and target variable in the DataFrame. A scaler is fitted on the\n",
    "        train data and applied to train, valid and test.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            columns: True, str or list of str (True)\n",
    "                the columns to scale (True for all)\n",
    "                \n",
    "            scalertype: a SKLearn type scaler (StandardScaler)\n",
    "                the scaler class that is used\n",
    "            \n",
    "            omit_interval: (-2,2) when colums is set to True\n",
    "                features whose values lie within this interval are not scaled, the default value\n",
    "                ensures that binary values will not be scaled.\n",
    "                \n",
    "        Return: DFrame\n",
    "        \"\"\"\n",
    "        if self._pt_polynomials and columns != 'x_only':\n",
    "            assert type(columns) != list or len(columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_scale_columns', columns)\n",
    "        r._change('_pt_scalertype', scalertype)\n",
    "        r._change('_pt_scale_omit_interval', omit_interval)\n",
    "        return r\n",
    "    \n",
    "    def scalex(self, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        \"\"\"\n",
    "        Scale all input features. A scaler is fitted on the\n",
    "        train data and applied to train, valid and test.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            scalertype: a SKLearn scaler class (StandardScaler)\n",
    "                the scaler class that is used\n",
    "                \n",
    "            omit_interval: tupple (-2,2) \n",
    "                features whose values lie within this interval are not scaled, the default value\n",
    "                ensures that binary values will not be scaled.\n",
    "        \n",
    "        Returns: DFrame \n",
    "        \"\"\"\n",
    "        return self.scale(columns='x_only', scalertype=scalertype, omit_interval=omit_interval)\n",
    "    \n",
    "    def add_bias(self):\n",
    "        \"\"\"\n",
    "        Adds a bias column of value 1 to generated inputs.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_bias', True)\n",
    "        return r\n",
    "    \n",
    "    def reshuffle(self, random_state=None):\n",
    "        \"\"\"\n",
    "        Resamples the train, valid and test set with the existing settings.\n",
    "        \n",
    "        This effect is not inplace, but applied to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            random_state: int (None)\n",
    "                set a random_state for reproducible results   \n",
    "                \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_split_random_state', random_state)\n",
    "        return r\n",
    "    \n",
    "    def _index_changed(self):\n",
    "        for p in self._cached_indices:\n",
    "            setattr(self, p, None)\n",
    "        for p in self._cached_fold:\n",
    "            setattr(self, p, None)\n",
    "        for p in self._cached:\n",
    "            setattr(self, p, None)\n",
    "        self._cached_index = self.index.copy()\n",
    "\n",
    "    def _columns_changed(self):\n",
    "        for p in self._cached:\n",
    "            setattr(self, p, None)\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        \"\"\"\n",
    "        Clears the currently sampled split() and folds(), which is stored whenever data preparation\n",
    "        is called. Therefore, resampling the split or fold on the next call to data preparation.\n",
    "        However, this call will not reset a random_state, use reshuffle() for that.\n",
    "        \n",
    "        This effect is not inplace, but applied to a copy that is returned. \n",
    "        \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._index_changed()\n",
    "        return r\n",
    "        \n",
    "    def split(self, \n",
    "              valid_size=None, \n",
    "              test_size=None, \n",
    "              shuffle=None, \n",
    "              random_state=None, \n",
    "              stratify=None, \n",
    "              stratify_test=None):\n",
    "        \"\"\"\n",
    "        Split the data in a train/valid/(test) set. \n",
    "        \n",
    "        This effect is not inplace, but applied to a copy that is returned. \n",
    "        \n",
    "        Arguments:\n",
    "            valid_size: float (None)\n",
    "                the fraction of the dataset that is used for the validation set.\n",
    "                \n",
    "            test_size: float (None)\n",
    "                the fraction of the dataset that is used for the test set. When combined with folds\n",
    "                if 1 > test_size > 0, the test set is split before the remainder is divided in folds \n",
    "                to apply n-fold cross validation.\n",
    "                \n",
    "            shuffle: bool (None)\n",
    "                shuffle the rows before splitting. None means True unless sequence() is called to process\n",
    "                the data as a (time) series.\n",
    "                \n",
    "            random_state: int (None)\n",
    "                set a random_state for reproducible results\n",
    "                \n",
    "            stratify: str, [ str ], True or None (None)\n",
    "                Apply stratified sampling over these columns (True = all columns)\n",
    "                For multiple columns, multi-label stratification is applied with support over\n",
    "                continuous variables.\n",
    "                \n",
    "            stratify_test: str, [ str ], True or (None)\n",
    "                Apply stratified sampling over these columns (True = all columns)\n",
    "                For multiple columns, multi-label stratification is applied with support over\n",
    "                continuous variables.\n",
    "                If None, the value for stratify is used. To supress stratification, pass [].\n",
    "            \n",
    "        Returns: DFrame \n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_valid_size', valid_size)\n",
    "        r._change('_pt_test_size', test_size)\n",
    "        r._change('_pt_split_shuffle', shuffle)\n",
    "        r._change('_pt_split_random_state', random_state)\n",
    "        r._change('_pt_split_stratify', [stratify] if type(stratify) == str else stratify)\n",
    "        r._change('_pt_split_stratify_test', [stratify_test] if type(stratify_test) == str else \\\n",
    "                                    (r._pt_split_stratify if stratify_test is None else stratify_test))\n",
    "        return r\n",
    "    \n",
    "    def folds(self, folds=5, shuffle=True, random_state=None, stratify=None, test=None):\n",
    "        \"\"\"\n",
    "        Divide the data in folds to setup n-Fold Cross Validation in a reproducible manner. \n",
    "        \n",
    "        By combining folds() with split(0 < test_size < 1) , a single testset is split before \n",
    "        dividing the remainder in folds that are used for training and validation. \n",
    "        When used without split, by default a single fold is used for testing.\n",
    "        \n",
    "        The folds assigned to the validation and test-set rotate differently, \n",
    "        giving 5x4 combinations for 5-fold cross validation. You can access all 20 combinations\n",
    "        by calling fold(0) through fold(19).\n",
    "    \n",
    "        This effect is not inplace, but applied to a copy that is returned. \n",
    "    \n",
    "        Arguments:\n",
    "            folds: int (None)\n",
    "                The number of times the data will be split in preparation for n-fold cross validation. The\n",
    "                different splits can be used through the fold(n) method.\n",
    "                SKLearn's SplitShuffle is used, therefore no guarantee is given that the splits are\n",
    "                different nor that the validation splits are disjoint. For large datasets, that should not\n",
    "                be a problem.\n",
    "                \n",
    "            shuffle: bool (None)\n",
    "                shuffle the rows before splitting. None means True unless sequence() is called to process\n",
    "                the data as a (time) series.\n",
    "                \n",
    "            random_state: int (None)\n",
    "                set a random_state for reproducible results.\n",
    "                \n",
    "            stratify: str, [ str ], True or None (None)\n",
    "                Apply stratified sampling over the given columns. True means all columns. If column \n",
    "                Per value for the given column, the rows are sampled. When a list\n",
    "                of columns is given, multi-label stratification is applied.\n",
    "                \n",
    "            test: bool (None)\n",
    "                whether to use one fold as a test set. The default None is interpreted as True when\n",
    "                split is not used. Often for automated n-fold cross validation studies, the validation set\n",
    "                is used for early termination, and therefore you should use an out-of-sample\n",
    "                test set that was not used for optimizing.\n",
    "            \n",
    "        Returns: copy of DFrame \n",
    "            schedules the data to be split in folds.\n",
    "        \"\"\"\n",
    "        assert type(folds) == int and folds > 1, 'You have to set split(folds) to an integer > 1'\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_folds', folds)\n",
    "        r._change('_pt_folds_shuffle', shuffle)\n",
    "        r._change('_pt_folds_random_state', random_state)\n",
    "        r._change('_pt_folds_stratify', [stratify] if type(stratify) == str else stratify)\n",
    "        if test or (r._pt_test_size is None and test is None):\n",
    "            r._change('_pt_test_size', 1)\n",
    "        return r\n",
    "    \n",
    "    def leave_one_out(self):\n",
    "        \"\"\"\n",
    "        Configures folds() to perform a leave-one-out cross validation.\n",
    "        \"\"\"\n",
    "        return self.folds(len(self._indices_after_testsplit), shuffle=False)\n",
    "        \n",
    "    def polynomials(self, degree, include_bias=False):\n",
    "        \"\"\"\n",
    "        Adds (higher-order) polynomials to the data pipeline.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "        \n",
    "        Arguments:\n",
    "            degree: int - degree of the higher order polynomials (e.g. 2 for squared)\n",
    "            include_bias: bool (False) - whether to generate a bias column\n",
    "        \n",
    "        Returns: copy of DFrame \n",
    "        \"\"\"\n",
    "        assert type(self._pt_scale_columns) != list or len(self._pt_scale_columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_polynomials', PolynomialFeatures(degree, include_bias=include_bias))\n",
    "        return r\n",
    "    \n",
    "    def no_columny(self):\n",
    "        \"\"\"\n",
    "        PipeTorch cannot currently handle a dataset without a target variable, \n",
    "        however, it can work by simply assigning one of the used input features\n",
    "        as a target variable.\n",
    "\n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Returns: copy of DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_columny', [self._pt_columnx[0]])\n",
    "        return r\n",
    "    \n",
    "    def columny(self, columns=None, vector=None):\n",
    "        \"\"\"\n",
    "        Configures the generated target variable and shape.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            columns: str or list of str (None)\n",
    "                single column name or list of columns that is to be used as target column. \n",
    "                None: use the last column\n",
    "            vector: bool (None)\n",
    "                By default, y will be prepared as a 1d vector and y_tensor as an (n, 1) matrix\n",
    "                because most SKLearn algorithmms prefer vector and for PyTorch (n, 1) matrices \n",
    "                are used more often. Setting vector=False causes both y and y_tensor to return 1d shapes\n",
    "                and vector=True causes both y and y_tensor to return (n, 1) shapes.\n",
    "        \n",
    "        Returns: DFrame \n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.copy(deep=False)\n",
    "        if columns is not None:\n",
    "            r._change('_pt_columny', [columns] if type(columns) == str else columns)\n",
    "        assert r._pt_columny is None or len(r._pt_columny) == 1 or not vector, 'You cannot create target vector with multiple columns'\n",
    "        r._change('_pt_vectory', vector)\n",
    "        return r\n",
    "\n",
    "    def columnx(self, *columns, omit=False):\n",
    "        \"\"\"\n",
    "        Specify which columns to use as input. The target variable is always excluded\n",
    "        so if you want to add that (for sequence learning), you should copy the\n",
    "        target column. \n",
    "        Unlocks the DFrame, train and valid sets are therefore resampled.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            columns: str or list of str\n",
    "                columns to be used as input features\n",
    "            omit: bool (False)\n",
    "                when True, all columns are used except the specified target column(s)\n",
    "        \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        if omit:\n",
    "            r._change('_pt_columnx', [ c for c in self.columns if c not in columns ])\n",
    "        else:\n",
    "            r._change('_pt_columnx', list(columns) if len(columns) > 0 else None)\n",
    "        return r\n",
    "    \n",
    "    def category(self, *columns, sort=False):\n",
    "        \"\"\"\n",
    "        Converts the values in the targetted columns into indices, for example to use in lookup tables.\n",
    "        columns that are categorized are excluded from scaling. You cannot use this function together\n",
    "        with polynomials or bias.\n",
    "        \n",
    "        Note: PipeTorch only uses categories that are in the training set and uses category 0 as\n",
    "        an unknown category number for categories in the validation and test set that are not known during training.\n",
    "        This way, no future information is used. \n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            columns: str or list of str\n",
    "                list of columns that is to be converted into a category\n",
    "            sort: True/False (default False) \n",
    "                whether the unique values of these colums should be converted \n",
    "                to indices in sorted order.\n",
    "        \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        assert self._pt_polynomials is None, 'You cannot combine categories with polynomials'\n",
    "        assert self._pt_bias is None, 'You cannot combine categories with polynomials'\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_category', columns)\n",
    "        r._change('_pt_category_sort', sort)\n",
    "        return r\n",
    "    \n",
    "    def dummies(self, *columns):\n",
    "        \"\"\"\n",
    "        Converts the values in the targetted columns into dummy variables. This is an alernative to \n",
    "        pd.get_dummies, that only uses the train set to assess which values there are (as it should be),\n",
    "        setting all variables to 0 for valid/test items that contain an unknown label. \n",
    "        That way, no future information is used. Columns that are categorized are excluded from scaling. \n",
    "        You cannot use this function together with polynomials or bias.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Args:\n",
    "            columns: str or list of str\n",
    "                the columns that are to be converted into a category\n",
    "        \n",
    "        Returns: DFrame \n",
    "        \"\"\"\n",
    "        assert self._pt_polynomials is None, 'You cannot combine categories with polynomials'\n",
    "        assert self._pt_bias is None, 'You cannot combine categories with polynomials'\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_dummies', columns)\n",
    "        return r\n",
    "\n",
    "    def sequence(self, window, shift_y = 1):\n",
    "        \"\"\"\n",
    "        The rows in the DataFrame are considered to be a continuous sequence over time. \n",
    "        From this sequence, input samples are created that contain the features over a 'window'\n",
    "        of rows. Ths allows to learn a model to predict a target based on prior history. The\n",
    "        samples are generated so that their window overlaps.\n",
    "        Unlocks the DFrame, train and valid sets are therefore resampled.\n",
    "        \n",
    "        Samples with NaN's are automatically skipped. When DataFrames are grouped, samples will\n",
    "        be created only within groups.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            window: int\n",
    "                the number of rows that is used a an input\n",
    "            shift_y: int\n",
    "                how many rows into the future the target variable is placed, \n",
    "                e.g. with a window=2 and shift_y=1, X0 would contain [x[0], x[1]] \n",
    "                while y0 would contain y[2], \n",
    "                the next sample X1 would be [x[1], x[2]] and y[3].\n",
    "        \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_sequence_window', window)\n",
    "        r._change('_pt_sequence_shift_y', shift_y)\n",
    "        return r\n",
    "        \n",
    "    def filterna(self, filter=True):\n",
    "        \"\"\"\n",
    "        Returns: a DataFrame in which rows with missing values are filtered\n",
    "        from the train and valid set. The result is not visible in the DataFrame\n",
    "        but will be in the resulting train and valid sets.\n",
    "        \n",
    "        Args:\n",
    "            filter: bool (True)\n",
    "                Default is to drop rows in de DSet's that still have missing values.\n",
    "                Set to false to turn off\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_filterna', filter)\n",
    "        return r\n",
    "    \n",
    "    def columnfunction(self, **columnfunctions):\n",
    "        \"\"\"\n",
    "        Returns: a DataFrame in which column transformations are added to the\n",
    "        data pipeline, before scaling the data.\n",
    "        \n",
    "        Transforms effectively adds to the existing configured column functions,\n",
    "        while overriding functions that are newly defined. This function does not\n",
    "        remove any column functions, unless called with zero parameters\n",
    "        \n",
    "        Args:\n",
    "            columnfunctions: { columname: callable}\n",
    "                when empty, all existing column functions will be erased,\n",
    "                otherwise, the given column functions are added to the dictionary\n",
    "                of configured column functions, which are executed in the pipeline\n",
    "                just before str column names and a function\n",
    "                that will be performed on the indicated column.\n",
    "        \"\"\"\n",
    "\n",
    "        assert all([ c in self.columns for c in columnfunctions ]), 'Not all keys are columns'\n",
    "        r = self.copy(deep=False)\n",
    "        if r._pt_transform is None:\n",
    "            r._change('_pt_transform', columnfunctions)\n",
    "        else:\n",
    "            r._change('_pt_transform', r._pt_transform | columnfunctions)\n",
    "        return r\n",
    "    \n",
    "    def log(self, *columns):\n",
    "        \"\"\"\n",
    "        Returns: a DataFrame in which log column transformations are added to the\n",
    "        data pipeline, before scaling the data.\n",
    "        \n",
    "        Args:\n",
    "            columnfunctions: { columname: callable}\n",
    "                a dictionary of str column names and a function\n",
    "                that will be performed on the indicated column.\n",
    "        \"\"\"\n",
    "        return self.columnfunction( **{c:FunctionTransformer(np.log) for c in columns })\n",
    "        \n",
    "    def log1p(self, *columns):\n",
    "        \"\"\"\n",
    "        Returns: a DataFrame in which log1p column transformations are added to the\n",
    "        data pipeline, before scaling the data.\n",
    "        \n",
    "        Args:\n",
    "            columnfunctions: { columname: callable}\n",
    "                a dictionary of str column names and a function\n",
    "                that will be performed on the indicated column.\n",
    "        \"\"\"\n",
    "        return self.columnfunction( **{c:FunctionTransformer(np.log1p) for c in columns })\n",
    "    \n",
    "    def dataset_train_transforms(self, *transforms):\n",
    "        \"\"\"\n",
    "        Configure a (list of) transformation function(s) that is called from the DataSet class to prepare the \n",
    "        train data.\n",
    "        \n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            *transforms: [ callable ]\n",
    "                (list of) transformation function(s) that is called from the DataSet class to prepare the data.\n",
    "\n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_dataset_train_transforms', transforms)\n",
    "        return r\n",
    "    \n",
    "    def dataset_transforms(self, *transforms):\n",
    "        \"\"\"\n",
    "        Configure a (list of) transformation function(s) that is called when retrieving\n",
    "        items from PipeTorch' TransformableDataSet class to prepare the data. These function allow\n",
    "        to configure a pipeline for data augmentation that is used to train PyTorch models.\n",
    "        For example, to train of images or audio fragments that are read from disk.\n",
    "        \n",
    "        Note that these transformations are not used to prepare Numpy Arrays, and the DataSet class has\n",
    "        to call the tranformations (which the TransformableDataSet class does that is used by default).\n",
    "\n",
    "        This effect is not inplace, but configured to a copy that is returned. \n",
    "\n",
    "        Arguments:\n",
    "            *transforms: [ callable ]\n",
    "                (list of) transformation function(s) that is called from the DataSet class to prepare the data.\n",
    "                \n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        r = self.copy(deep=False)\n",
    "        r._change('_pt_dataset_transforms', transforms)\n",
    "        return r\n",
    "\n",
    "    def _dataset_train_transformation_parameters(self, train_dset):\n",
    "        \"\"\"\n",
    "        Placeholder to extend DFrame to configure transformations that are applied on the train DataSet,\n",
    "        but not on the validation or test set. This is used by\n",
    "        ImageDFrame to learn normalization parameters for images and to learn a vocabulary \n",
    "        for TextDFrame to tokenize text.\n",
    "        \n",
    "        Note that the mechanism assumes that the train DSet is generated first.\n",
    "        \n",
    "        Arguments: \n",
    "            train_dset: DSet\n",
    "                The train DSet to learn the transformation parameters on\n",
    "            \n",
    "        Returns: bool (None)\n",
    "            This function should return True when parameters were learned, to force DFrame to add a new\n",
    "            list of transformations to the generated DSet.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _dataset_pre_transforms(self):\n",
    "        \"\"\"\n",
    "        Placeholder for standard transformations that precede configured transformations.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def _dataset_post_transforms(self):\n",
    "        \"\"\"\n",
    "        Placeholder for standard transformations that succede configured transformations.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def _dataset_transforms(self, pre=True, train=True, standard=True, post=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            pre: bool (True) - whether to include pre transformations\n",
    "            train: bool (True) - whether to include train transformations\n",
    "            standard: bool (True) - whether to include standard transformations\n",
    "            post: bool (True) - whether to include post transformations\n",
    "            \n",
    "        Returns: [ callable ]\n",
    "            A list of transformations that is applied to the generated DSet \n",
    "        \"\"\"\n",
    "        t = []\n",
    "        try:\n",
    "            if pre:\n",
    "                t.extend(self._dataset_pre_transforms())\n",
    "        except: pass\n",
    "        try:\n",
    "            if train:\n",
    "                t.extend(self._pt_dataset_train_transforms)\n",
    "        except: pass\n",
    "        try:\n",
    "            t.extend(self._pt_dataset_transforms)\n",
    "        except: pass\n",
    "        try:\n",
    "            if post:\n",
    "                t.extend( self._dataset_post_transforms() )\n",
    "        except: pass\n",
    "        return t\n",
    "    \n",
    "    def inspect(self):\n",
    "        \"\"\"\n",
    "        Describe the data in the DFrame, specifically by reporting per column \n",
    "        - Datatype \n",
    "        - Missing: number and percetage of 'Missing' values\n",
    "        - Range: numeric types, are described as a range [min, max]\n",
    "                  and for non-numeric types the #number of unique values is given\n",
    "        - Values: the two most frequently occuring values (most frequent first).\n",
    "        \"\"\"\n",
    "\n",
    "        missing_count = self.isnull().sum() # the count of missing values\n",
    "        value_count = self.isnull().count() # the count of all values\n",
    "        missing_percentage = round(missing_count / value_count * 100,2) #the percentage of missing values\n",
    "\n",
    "        datatypes = self.dtypes\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Missing (#)': missing_count, \n",
    "            'Missing (%)': missing_percentage,\n",
    "            'Datatype': datatypes\n",
    "        }) #create a dataframe\n",
    "        df = df.sort_values(by=['Missing (#)'], ascending=False)\n",
    "\n",
    "        value_col = []\n",
    "        range_col = []\n",
    "        for index, row in df.iterrows():\n",
    "            u = self[index].value_counts().index.tolist()\n",
    "            if pd.api.types.is_numeric_dtype(row['Datatype']):\n",
    "                _range = f\"[{self[index].min()}, {self[index].max()}]\"\n",
    "            else:\n",
    "                _range = f\"#{len(u)}\"\n",
    "            if len(u) == 1:\n",
    "                _values = f'({u})'\n",
    "            elif len(u) == 2:\n",
    "                _values = f'({u[0]}, {u[1]})'\n",
    "            elif len(u) > 2:\n",
    "                _values = f'({u[0]}, {u[1]}, ...)'\n",
    "            else:\n",
    "                _values = ''\n",
    "            range_col.append(_range)\n",
    "            value_col.append(_values)\n",
    "        df[\"Range\"] = range_col\n",
    "        df[\"Values\"] = value_col\n",
    "        return df\n",
    "\n",
    "    def cross_validate_sklearn(self, model, *target, evaluator=None, annot={}, **kwargs):\n",
    "        \"\"\"\n",
    "        On a DFrame that is configured for n-fold cross validation (df.folds(n)), this function iterates\n",
    "        over the trials, fitting an SKLearn model and storing the targets for the train, valid and test subsets.\n",
    "        \n",
    "        Arguments:\n",
    "            model: object\n",
    "                a machine learning algorithm that support the fit(X, y) and predict(X) methods,\n",
    "                like the SKLearn models.\n",
    "            target: callable\n",
    "                one or more functions that return an evaluation metrics when called with\n",
    "                target(y_true, y_predict)\n",
    "            evaluator: Evaluator (None)\n",
    "                when provided, the results are added to this evaluator, otherwise\n",
    "                the evaluator of this DFrame is reset and a new one is used.\n",
    "            annot: {}\n",
    "                the annotations that are stored with the metrics. cross_validate will add a\n",
    "                'fold' metric to indicate the fold.\n",
    "                \n",
    "        \"\"\"\n",
    "        assert self._pt_folds is not None, 'You have to set df.folds before you can use cross validate'\n",
    "        assert self._pt_folds > 1, 'You have to set df.folds greater than 1 before you can use cross validate'\n",
    "        if reset_evaluator:\n",
    "            try:\n",
    "                del self._pt_evaluator\n",
    "            except: pass\n",
    "        evaluator = self.evaluator(*target)\n",
    "        study = evaluator.study(**kwargs)\n",
    "        data = self.iterfolds()\n",
    "        folds = self._pt_folds\n",
    "        test = len(self._test_indices) > 0\n",
    "                \n",
    "        def run(evaluator, trial):\n",
    "            df = next(data)\n",
    "            annot['fold'] = trial.number\n",
    "            model.fit(df.train_X, df.train_y)\n",
    "            evaluator._store_metrics(df.train_y, model.predict(df.train_X), \n",
    "                                          annot={'phase':'train', **annot})\n",
    "            metrics = evaluator._store_metrics(df.valid_y, model.predict(df.valid_X), \n",
    "                                                    annot={'phase':'valid', **annot})\n",
    "            if test:\n",
    "                metrics = evaluator._store_metrics(df.test_y, model.predict(df.test_X), \n",
    "                                              annot={'phase':'test', **annot})\n",
    "            \n",
    "            return [ metrics[t] for t in study.target ]\n",
    "        \n",
    "        study.optimize(run, n_trials=folds)\n",
    "        return study\n",
    "\n",
    "    def study(self, *target, evaluator=None, **kwargs):\n",
    "        \"\"\"\n",
    "        On a DFrame that is configured for n-fold cross validation (df.folds(n)), this function iterates\n",
    "        over the trials, fitting an SKLearn model and storing the targets for the train, valid and test subsets.\n",
    "        \n",
    "        Arguments:\n",
    "            target: callable\n",
    "                one or more functions that return an evaluation metrics when called with\n",
    "                target(y_true, y_predict)\n",
    "            evaluator: Evaluator (None)\n",
    "                when provided, the results are added to this evaluator, otherwise\n",
    "                the evaluator of this DFrame is reset and a new one is used.\n",
    "        \"\"\"\n",
    "        if evaluator is None:\n",
    "            try:\n",
    "                del self._pt_evaluator\n",
    "            except: pass\n",
    "            evaluator = self.evaluator(*target)\n",
    "        study = evaluator.study(**kwargs)\n",
    "        return study\n",
    "\n",
    "class DFrame(pd.DataFrame, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DFrame.__init__(self, data)\n",
    "        self._catch_change()\n",
    "\n",
    "    def _changed(self, f, *args, **kwargs):\n",
    "        #print(f'changed {f} {args} {kwargs}')\n",
    "        self._columns_changed()\n",
    "        return f(*args, **kwargs)\n",
    "\n",
    "    def _return_changed(self, f, *args, **kwargs):\n",
    "        r = f(*args, **kwargs)\n",
    "        r._columns_changed()\n",
    "        return r\n",
    "\n",
    "    def _change(self, key, value):\n",
    "        #print(f'changed_attr {key} {value}')\n",
    "        if key in self._config_indices_set:\n",
    "            self._index_changed()\n",
    "        elif key in self._config_set:\n",
    "            self._columns_changed()\n",
    "        return super().__setattr__(key, value)\n",
    "\n",
    "    def __changed_inplace(self, **kwargs):\n",
    "        try:\n",
    "            if kwargs['inplace']:\n",
    "                if r._unchanged():\n",
    "                    r._columns_changed()\n",
    "        except: pass\n",
    "        try:\n",
    "            if kwargs['copy'] == False:\n",
    "                if r._unchanged():\n",
    "                    r._columns_changed()\n",
    "        except: pass\n",
    "\n",
    "    \n",
    "    def _changed_inplace(self, f, *args, **kwargs):\n",
    "        #print(f'changed_inplace {f} {args} {kwargs}')\n",
    "        self.__changed_inplace(**kwargs)\n",
    "        r = f(*args, **kwargs)\n",
    "        r._columns_changed()\n",
    "        return r\n",
    "     \n",
    "    def _unchanged(self):\n",
    "        #print(f'process_changed {self._cached_changed} {self.index.equals(self._cached_index)}')\n",
    "        \n",
    "        if self._cached_index is not None and self.index.equals(self._cached_index):\n",
    "            return True\n",
    "        self._index_changed()\n",
    "        return False\n",
    "    \n",
    "    def _catch_change_f(self, f):\n",
    "        return functools.wraps(f)(functools.partial(self._changed, f))\n",
    "    \n",
    "    def _catch_return_change_f(self, f):\n",
    "        return functools.wraps(f)(functools.partial(self._return_changed, f))\n",
    "    \n",
    "    def _catch_change_inplace_f(self, f):\n",
    "        return functools.wraps(f)(functools.partial(self._changed_inplace, f))\n",
    "    \n",
    "    def _catch_change(self):\n",
    "        self._set_value = self._catch_change_f(self._set_value)\n",
    "        self._setitem_slice = self._catch_change_f(self._setitem_slice)\n",
    "        self._setitem_frame = self._catch_change_f(self._setitem_frame)\n",
    "        self._setitem_array = self._catch_change_f(self._setitem_array)\n",
    "        self._set_item = self._catch_change_f(self._set_item)\n",
    "        self.__getitem__ = self._catch_return_change_f(self.__getitem__)\n",
    "        self.update = self._catch_change_f(self.update)\n",
    "        self.insert = self._catch_change_f(self.insert)\n",
    "\n",
    "        self._mgr.idelete = self._catch_change_f(self._mgr.idelete)\n",
    "        self._mgr.iset = self._catch_change_f(self._mgr.iset)\n",
    "        self._mgr.insert = self._catch_change_f(self._mgr.insert)\n",
    "        #self._mgr.column_setitem = self._catch_change_f(self._mgr.column_setitem)\n",
    "        #self._mgr.reindex_axis = self._catch_change_f(self._mgr.reindex_axis)\n",
    "        #self._mgr.reindex_indexer = self._catch_change_f(self._mgr.reindex_indexer)\n",
    "\n",
    "        self.clip = self._catch_change_inplace_f(self.clip)\n",
    "        self.drop = self._catch_change_inplace_f(self.drop)\n",
    "        self.drop_duplicates = self._catch_change_inplace_f(self.drop_duplicates)\n",
    "        #self.dropna = self._catch_change_inplace_f(self.dropna)\n",
    "        self.eval = self._catch_change_inplace_f(self.eval)\n",
    "\n",
    "        self.fillna = self._catch_change_inplace_f(self.fillna)\n",
    "        self.interpolate = self._catch_change_inplace_f(self.interpolate)\n",
    "        self.mask = self._catch_change_inplace_f(self.mask)\n",
    "        self.pad = self._catch_change_inplace_f(self.pad)\n",
    "        self.query = self._catch_change_inplace_f(self.query)\n",
    "        self.replace = self._catch_change_inplace_f(self.replace)\n",
    "        self.reset_index = self._catch_change_inplace_f(self.reset_index)\n",
    "        self.set_axis = self._catch_change_inplace_f(self.set_axis)\n",
    "        self.set_index = self._catch_change_inplace_f(self.set_index)\n",
    "        self.sort_index = self._catch_change_inplace_f(self.sort_index)\n",
    "        self.sort_values = self._catch_change_inplace_f(self.sort_values)\n",
    "        self.where = self._catch_change_inplace_f(self.where)\n",
    "        self.astype = self._catch_change_inplace_f(self.astype)\n",
    "        self.reindex = self._catch_change_inplace_f(self.reindex)\n",
    "        self.rename = self._catch_change_inplace_f(self.rename)\n",
    "        #self.__setattr__ = self._changed_attr\n",
    "\n",
    "    def dropna(self, **kwargs):\n",
    "        if self._pt_test_indices is not None:\n",
    "            warnings.warn(\"Using dropna on a dataset with a fixed test set \" +\n",
    "                    \"which is probably a bad idea since this removes test data.\",\n",
    "                    RuntimeWarning,\n",
    "                    stacklevel=find_stack_level(),\n",
    "                )\n",
    "        self.__changed_inplace(**kwargs)\n",
    "        return pd.DataFrame.dropna(self, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def read_csv(cls, path, **kwargs):\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        return cls(df)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dfs(cls, *dfs, **kwargs):\n",
    "        return cls(pd.concat(dfs), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_train_test(cls, train, test, **kwargs):\n",
    "        r = cls(pd.concat([train, test], ignore_index=True))\n",
    "        r._pt_train_valid_indices = list(range(len(train)))\n",
    "        r._pt_test_indices = list(range(len(train), len(train)+len(test)))\n",
    "        return r\n",
    "    \n",
    "    @classmethod\n",
    "    def read_from_kaggle(cls, dataset, train=None, test=None, shared=True, force=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Reads a DFrame from a Kaggle dataset. The downloaded dataset is automatically stored so that the next time\n",
    "        it is read from file rather than downloaded. See `read_csv`. The dataset is stored by default in a folder\n",
    "        with the dataset name in `~/.pipetorchuser`.\n",
    "\n",
    "        If the dataset is not cached, this functions requires a valid .kaggle/kaggle.json file, that you can \n",
    "        create manually or with the function `create_kaggle_authentication()`.\n",
    "\n",
    "        Note: there is a difference between a Kaggle dataset and a Kaggle competition. For the latter, \n",
    "        you have to use `read_from_kaggle_competition`.\n",
    "\n",
    "        Example:\n",
    "            read_from_kaggle('uciml/autompg-dataset')\n",
    "                to read/download `https://www.kaggle.com/datasets/uciml/autompg-dataset`\n",
    "            read_from_kaggle('robmarkcole/occupancy-detection-data-set-uci', 'datatraining.txt', 'datatest.txt')\n",
    "            to combine a train and test set in a single DFrame\n",
    "\n",
    "        Arguments:\n",
    "            dataset: str\n",
    "                the username/dataset part of the kaggle url, e.g. uciml/autompg-dataset for \n",
    "\n",
    "            train: str (None)\n",
    "                the filename that is used as the train set, e.g. 'train.csv'\n",
    "            test: str (None)\n",
    "                the filename that is used as the test set, e.g. 'test.csv'\n",
    "            shared: bool (False)\n",
    "                save the dataset in ~/.pipetorch instead of ~/.pipetorchuser, allowing to share downloaded\n",
    "                files between users.\n",
    "            force: bool (False)\n",
    "                when True, the dataset is always downloaded\n",
    "            **kwargs:\n",
    "                additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "                you will have to set engine='python'.\n",
    "\n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        k = Kaggle(dataset, shared=shared)\n",
    "        if force:\n",
    "            k.remove_user()\n",
    "        train = k.read(train, **kwargs)\n",
    "        if test is not None:\n",
    "            test = k.read(test, **kwargs)\n",
    "            return cls.from_train_test(train, test)\n",
    "        return cls(train)\n",
    "           \n",
    "#     @classmethod\n",
    "#     def read_from_kaggle_competition(cls, dataset, train=None, test=None, shared=False, force=False, **kwargs):\n",
    "#         train = read_from_kaggle_competition(dataset, filename=train, shared=shared, force=force, **kwargs)\n",
    "#         if test is not None:\n",
    "#             test = read_from_kaggle_competition(dataset, filename=test, **kwargs)\n",
    "#             return cls.from_train_test(train, test)\n",
    "#         return cls(train)\n",
    "\n",
    "#     @classmethod\n",
    "#     def read_csv(cls, url, filename=None, path=None, save=False, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Reads a .csv file from cache or url. The place to store the file is indicated by path / filename\n",
    "#         and when a delimiter is used, this is also used to save the file so that the original delimiter is kept.\n",
    "#         The file is only downloaded using the url if it does not exsists on the filing system. If the file is\n",
    "#         downloaded and save=True, it is also stored for future use.\n",
    "\n",
    "#         Arguments:\n",
    "#             url: str\n",
    "#                 the url to download or a full path pointing to a .csv file\n",
    "#             filename: str (None)\n",
    "#                 the filename to store the downloaded file under. If None, the filename is extracted from the url.\n",
    "#             path: str (None)\n",
    "#                 the path in which the file is stored. If None, it will first check the ~/.pipetorch (for sharing\n",
    "#                 dataset between users) and then ~/.pipetorchuser (for user specific caching of datasets).\n",
    "#             save: bool (False)\n",
    "#                 whether to save a downloaded .csv\n",
    "#             **kwargs:\n",
    "#                 additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "#                 you will have to set engine='python'.\n",
    "\n",
    "#         Returns: DFrame\n",
    "#         \"\"\"\n",
    "#         return cls(read_csv(url, filename=filename, path=path, save=save, **kwargs))\n",
    "   \n",
    "    @classmethod\n",
    "    def read_from_package(cls, package, filename, **kwargs):\n",
    "        return cls(read_from_package(package, filename, **kwargs))\n",
    "    \n",
    "    @classmethod\n",
    "    def read_from_function(cls, filename, function, path=None, save=True, **kwargs):\n",
    "        \"\"\"\n",
    "        First checks if a .csv file is already stored, otherwise, calls the custom function to retrieve a \n",
    "        DataFrame. \n",
    "\n",
    "        The place to store the file is indicated by path / filename.\n",
    "        The file is only retrieved from the function if it does not exsists on the filing system. \n",
    "        If the file is retrieved and save=True, it is also stored for future use.\n",
    "\n",
    "        Arguments:\n",
    "            filename: str (None)\n",
    "                the filename to store the downloaded file under.\n",
    "            function: func\n",
    "                a function that is called to retrieve the DataFrame if the file does not exist.\n",
    "            path: str (None)\n",
    "                the path in which the file is stored. If None, it will first check the ~/.pipetorch (for sharing\n",
    "                dataset between users) and then ~/.pipetorchuser (for user specific caching of datasets).\n",
    "            save: bool (True)\n",
    "                whether to save a downloaded .csv\n",
    "            **kwargs:\n",
    "                additional parameters passed to pd.read_csv. For example, when a multichar delimiter is used\n",
    "                you will have to set engine='python'.\n",
    "\n",
    "        Returns: DFrame\n",
    "        \"\"\"\n",
    "        return cls(read_from_function(filename, function, path=path, save=save, **kwargs))\n",
    "\n",
    "    @classmethod\n",
    "    def read_excel(cls, path, filename=None, **kwargs):\n",
    "        return cls(read_excel(path, filename=filename, **kwargs))\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return DFrame\n",
    "    \n",
    "    def groupby(self, by, axis=0, level=None, as_index=True, sort=True, group_keys=True, observed=False, dropna=True):\n",
    "        \"\"\"\n",
    "        Groups a DFrame just like a Pandas DataFrame. This is useful for \n",
    "        learning sequences, since samples are only created using a sliding window\n",
    "        within groups.\n",
    "        \n",
    "        Arguments:\n",
    "            by: single column or list of columns\n",
    "            for the other arguments, see pandas.DataFrame.groupby \n",
    "        \n",
    "        Returns: pipetorch.PTGroupedDFrame\n",
    "        \"\"\"\n",
    "        r = super().groupby(by, axis=axis, level=level, as_index=as_index, sort=sort, group_keys=group_keys, observed=observed, dropna=dropna)\n",
    "        return self._copy_meta( GroupedDFrame(r) )\n",
    "\n",
    "     \n",
    "class DSeries(pd.Series, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DFrame.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return DSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return DFrame\n",
    "    \n",
    "class GroupedDSeries(SeriesGroupBy, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DFrame.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return GroupedDFrame\n",
    "    \n",
    "class GroupedDFrame(DataFrameGroupBy, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__(obj=data.obj, keys=data.keys, axis=data.axis, level=data.level, grouper=data.grouper, exclusions=data.exclusions,\n",
    "                selection=data._selection, as_index=data.as_index, sort=data.sort, group_keys=data.group_keys,\n",
    "                observed=data.observed, mutated=data.mutated, dropna=data.dropna)\n",
    "        _DFrame.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDFrame\n",
    "    \n",
    "    @property\n",
    "    def _constructor_sliced(self):\n",
    "        return GroupedDSeries\n",
    "    \n",
    "    def _unchanged(self):\n",
    "        return True\n",
    "    \n",
    "    def _index_changed(self):\n",
    "        pass\n",
    "        \n",
    "    def _columns_changed(self):\n",
    "        pass\n",
    "    \n",
    "    def get_group(self, namel, obj=None):\n",
    "        return self._dframe( super().get_group(name, obj=obj) )\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for group, subset in super().__iter__():\n",
    "            yield group, self._copy_meta(subset)\n",
    "        \n",
    "    def to_dataset(self):\n",
    "        \"\"\"\n",
    "        Convert a grouped DFrame into a PyTorch ConcatDataset over datasets\n",
    "        for every group contained.\n",
    "        \n",
    "        returns: train, valid, test DataSet\n",
    "        \"\"\"\n",
    "        from torch.utils.data import ConcatDataset\n",
    "        dss = []\n",
    "        for key, group in self:\n",
    "            dss.append( self.df_to_dataset(group) )\n",
    "\n",
    "        return [ConcatDataset(ds) for ds in zip(*dss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655beac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc47353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
