{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a0297b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dframe.py        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import linecache\n",
    "from ..evaluate.evaluate import Evaluator\n",
    "from .databunch import Databunch\n",
    "from .dset import DSet\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n",
    "from collections import defaultdict\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "class show_warning:\n",
    "    def __enter__(self):\n",
    "        self.warning = warnings.catch_warnings(record=True)\n",
    "        self.w = self.warning.__enter__()\n",
    "        warnings.filterwarnings('error')\n",
    "        warnings.simplefilter('default')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        for wi in self.w:\n",
    "            if wi.line is None:\n",
    "                print(wi.filename)\n",
    "                wi.line = linecache.getline(wi.filename, wi.lineno)\n",
    "        print(f'line number {wi.lineno}  line {wi.line}') \n",
    "        self.warning.__exit__(exc_type, exc_value, exc_traceback)\n",
    "\n",
    "class _DFrame:\n",
    "    _metadata = ['_pt_scale_columns', '_pt_scale_omit_interval', '_pt_scalertype', '_pt_columny', '_pt_columnx', \n",
    "                 '_pt_transposey', '_pt_bias', '_pt_polynomials', '_pt_dtype', '_pt_category', '_pt_category_sort', \n",
    "                 '_pt_dummies', '_pt_sequence_window', '_pt_sequence_shift_y', \n",
    "                 '_pt_shuffle', '_pt_split', '_pt_random_state', '_pt_balance', \n",
    "                 '_pt_len', '_pt_indices', '_pt_train_valid_indices', '_pt_test_indices',\n",
    "                 '_pt_dataset', '_pt_transforms', '_pt_train_transforms']\n",
    "\n",
    "    _locked_names = ['_pt__locked_indices', '_pt__locked_train_indices', \n",
    "                       '_pt__locked_valid_indices', '_pt__locked_test_indices',\n",
    "                       '_pt__locked_train', '_pt__locked_valid', \n",
    "                       '_pt__locked_scalerx', '_pt__locked_scalery',\n",
    "                       '_pt__locked_categoryx', '_pt__locked_categoryy', \n",
    "                       '_pt__locked_dummiesx', '_pt__locked_dummiesy' ]\n",
    "\n",
    "    _internal_names = pd.DataFrame._internal_names + _locked_names\n",
    "    \n",
    "    _internal_names_set = set( _internal_names )\n",
    "    \n",
    "    @classmethod\n",
    "    def read_csv(cls, path, **kwargs):\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        return cls(df)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dfs(cls, *dfs, **kwargs):\n",
    "        return cls(pd.concat(dfs), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_train_test(cls, train, test, **kwargs):\n",
    "        r = cls(pd.concat([train, test], ignore_index=True))\n",
    "        r._pt_train_valid_indices = list(range(len(train)))\n",
    "        r._pt_test_indices = list(range(len(train), len(train)+len(test)))\n",
    "        return r\n",
    "    \n",
    "    def __init__(self, data, **kwargs):\n",
    "        for m in self._metadata:\n",
    "            self.__setattr__(m, None)\n",
    "            self._pt_transposey = False\n",
    "            try:\n",
    "                self.__setattr__(m, getattr(data, m))\n",
    "            except: pass\n",
    "\n",
    "    def _copy_meta(self, r):\n",
    "        for c in self._metadata:\n",
    "            setattr(r, c, getattr(self, c))\n",
    "        return r\n",
    "            \n",
    "    def _copy_indices(self, r):\n",
    "        if self.is_locked:\n",
    "            r._pt__locked_indices = self._pt__locked_indices\n",
    "            r._pt__locked_train_indices = self._pt__locked_train_indices\n",
    "            r._pt__locked_valid_indices = self._pt__locked_valid_indices\n",
    "            r._pt__locked_test_indices = self._pt__locked_test_indices\n",
    "        return r\n",
    "        \n",
    "    def _dframe(self, data):\n",
    "        return self._copy_meta( DFrame(data) )\n",
    "    \n",
    "    def _copy_with_indices(self):\n",
    "        r = copy.copy(self)\n",
    "        self._copy_indices(r)\n",
    "        return r\n",
    "        \n",
    "    def _dset(self, data, indices=None, transforms=None):\n",
    "        if indices is None:\n",
    "            indices = list(range(len(data)))\n",
    "        return DSet.from_dframe(data, self, indices, transforms)\n",
    "    \n",
    "    @property\n",
    "    def is_locked(self):\n",
    "        try:\n",
    "            return self._pt__locked_indices is not None\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def lock(self):\n",
    "        \"\"\"\n",
    "        To provide stable sampling of the train, valid and test set, this locks the sampled indices so that\n",
    "        from this point, train, valid and test consistently produce the same subsets.\n",
    "        \n",
    "        Returns: DFrame\n",
    "            a copy of the DFrame for which the sampled indices are locked\n",
    "        \"\"\"\n",
    "        if not self.is_locked:\n",
    "            self._pt__locked_indices = self._indices\n",
    "            self._pt__locked_train_indices = self._train_indices\n",
    "            self._pt__locked_valid_indices = self._valid_indices\n",
    "            self._pt__locked_test_indices = self._test_indices\n",
    "    \n",
    "    @property\n",
    "    def _columny(self):\n",
    "        try:\n",
    "            if len(self._pt_columny) > 0:\n",
    "                return self._pt_columny\n",
    "        except: pass\n",
    "        return [ self.columns[-1] ]\n",
    "        \n",
    "    def astype(self, dtype, copy=True, errors='raise'):\n",
    "        self._pt_dtype = dtype\n",
    "        return super().astype(dtype, copy=copy, errors=errors)\n",
    "        \n",
    "    @property\n",
    "    def _columnx(self):\n",
    "        if self._pt_columnx is None:\n",
    "            return [ c for c in self.columns if c not in self._columny ]\n",
    "        return [ c for c in self._pt_columnx if c not in self._columny ]\n",
    "   \n",
    "    @property\n",
    "    def _all_columns(self):\n",
    "        return list(set(self._columnx).union(set(self._columny)))\n",
    "\n",
    "    @property\n",
    "    def _columnsx_scale_indices(self):\n",
    "        if self._pt_polynomials is not None and self._pt_scale_columns is not None:\n",
    "            X = self.train._x_polynomials\n",
    "            return [ i for i in range(X.shape[1]) if (X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1]) ]\n",
    "        columnx = self._columnx\n",
    "        cat = set(self._pt_category) if type(self._pt_category) == tuple else []\n",
    "        if self._pt_scale_columns == True or self._pt_scale_columns == 'x_only':\n",
    "            r = [ c for c in columnx if c not in cat]\n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            r = []\n",
    "        else:\n",
    "            r = [ c for c in columnx if c in self._pt_scale_columns and c not in cat ]\n",
    "        X = self.train._x_polynomials\n",
    "        r = [ columnx.index(c) for i, c in enumerate(columnx) if c in r and ((X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1])) ]\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def _columnsy_scale_indices(self):\n",
    "        columny = self._columny\n",
    "        cat = set(self._pt_category) if type(self._pt_category) == tuple else []\n",
    "        if self._pt_scale_columns == True:\n",
    "            y = self.train._y_numpy\n",
    "            r = [ c for i, c in enumerate(columny) if c not in cat and (y[:,i].min() < self._pt_scale_omit_interval[0] or y[:,i].max() > self._pt_scale_omit_interval[1]) ]\n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            r = []\n",
    "        else:\n",
    "            r = [ c for c in columny if c in self._pt_scale_columns and c not in cat ]\n",
    "        return [ columny.index(c) for c in r ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_scaler(scalertype, column):\n",
    "        scaler = scalertype()\n",
    "        scaler.fit(column)\n",
    "        return scaler\n",
    "    \n",
    "    @property\n",
    "    def _scalerx(self):\n",
    "        try:\n",
    "            return self._pt__locked_scalerx\n",
    "        except:  \n",
    "            X = self.train._x_polynomials\n",
    "            s = [ None ] * X.shape[1]\n",
    "            for i in self._columnsx_scale_indices:\n",
    "                s[i] = self._create_scaler(self._pt_scalertype, X[:, i:i+1])\n",
    "            self._pt__locked_scalerx = s\n",
    "            return self._pt__locked_scalerx\n",
    "        \n",
    "    @property\n",
    "    def _scalery(self):\n",
    "        try:\n",
    "            return self._pt__locked_scalery\n",
    "        except:\n",
    "            y = self.train._y_numpy\n",
    "            s = [ None ] * y.shape[1]\n",
    "            for i in self._columnsy_scale_indices:\n",
    "                s[i] = self._create_scaler(self._pt_scalertype, y[:, i:i+1])\n",
    "            self._pt__locked_scalery = s\n",
    "            return self._pt__locked_scalery\n",
    "    \n",
    "    def _create_category(self, column):\n",
    "        sort = self._pt_category_sort\n",
    "        class Category:\n",
    "            def fit(self, X):\n",
    "                s = X.unique()\n",
    "                if sort:\n",
    "                    s = sorted(s)\n",
    "                self.dict = defaultdict(lambda:0, { v:(i+1) for i, v in enumerate(s) })\n",
    "                self.inverse_dict = { (i+1):v for i, v in enumerate(s) }\n",
    "                self.inverse_dict[0] = np.NaN\n",
    "            \n",
    "            def transform(self, X):\n",
    "                return X.map(self.dict)\n",
    "            \n",
    "            def inverse_transform(self, X):\n",
    "                return X.map(self.inverse_dict)\n",
    "            \n",
    "        if column not in self._pt_category:\n",
    "            return None\n",
    "        \n",
    "        c = Category()\n",
    "        c.fit(self.train[column])\n",
    "        return c\n",
    "    \n",
    "    def _categoryx(self):\n",
    "        try:\n",
    "            return self._pt__locked_categoryx\n",
    "        except:\n",
    "            assert self.is_locked, '_categoryx can only be called on a locked DFrame to avoid consistency issues'\n",
    "            if self._pt_category is None or len(self._pt_category) == 0:\n",
    "                self._pt__locked_categoryx = None\n",
    "            else:\n",
    "                self._pt__locked_categoryx = [ self._create_category(c) for c in self._columnx ] \n",
    "            return self._pt__locked_categoryx\n",
    "    \n",
    "    def _categoryy(self):\n",
    "        try:\n",
    "            return self._pt__locked_categoryy\n",
    "        except:\n",
    "            assert self.is_locked, '_categoryy can only be called on a locked DFrame to avoid consistency issues'\n",
    "            if self._pt_category is None or len(self._pt_category) == 0:\n",
    "                self._pt__locked_categoryy = None\n",
    "            else:\n",
    "                self._pt__locked_categoryy = [ self._create_category(c) for c in self._columny ] \n",
    "            return self._pt__locked_categoryy\n",
    "\n",
    "    def _create_dummies(self, column):    \n",
    "        if column not in self._pt_dummies:\n",
    "            return None\n",
    "        \n",
    "        c = OneHotEncoder(handle_unknown='ignore')\n",
    "        c.fit(self.train[[column]])\n",
    "        return c\n",
    "    \n",
    "    def _dummiesx(self):\n",
    "        try:\n",
    "            return self._pt__locked_dummiesx\n",
    "        except:\n",
    "            assert self.is_locked, '_dummiesx can only be called on a locked DFrame to avoid consistency issues'\n",
    "            if self._pt_dummies is None or len(self._pt_dummies) == 0:\n",
    "                self._pt__locked_dummiesx = [ None ] * len(self._columnx)\n",
    "            else:\n",
    "                self._pt__locked_dummiesx = [ self._create_dummies(c) for c in self._columnx ]\n",
    "            return self._pt__locked_dummiesx\n",
    "    \n",
    "    def _dummiesy(self):\n",
    "        try:\n",
    "            return self._pt__locked_dummiesy\n",
    "        except:\n",
    "            assert self.is_locked, '_dummiesy can only be called on a locked DFrame to avoid consistency issues'\n",
    "            if self._pt_dummies is None or len(self._pt_dummies) == 0:\n",
    "                self._pt__locked_dummiesy = [ None ] * len(self._columny)\n",
    "            else:\n",
    "                self._pt__locked_dummiesy = [ self._create_dummies(c) for c in self._columny ]\n",
    "            return self._pt__locked_dummiesy\n",
    "\n",
    "    @property\n",
    "    def _shift_y(self):\n",
    "        if self._pt_sequence_shift_y is not None:\n",
    "            return self._pt_sequence_shift_y\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @property\n",
    "    def _sequence_window(self):\n",
    "        try:\n",
    "            if self._pt_sequence_window is not None:\n",
    "                return self._pt_sequence_window\n",
    "        except:pass\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def _sequence_index_y(self):\n",
    "        return self._sequence_window+self._shift_y-1\n",
    "    \n",
    "    @property\n",
    "    def _indices_unshuffled(self):\n",
    "        if self._pt_sequence_window is not None:\n",
    "            try:\n",
    "                self._pt_train_valid_indices = [ i for i in self._pt_train_valid_indices if i in self.index ]\n",
    "                indices = self._pt_train_valid_indices[:-(self._pt_sequence_window + self._pt_sequence_shift_y - 1)]\n",
    "            except:\n",
    "                indices = list(range(len(self) - (self._pt_sequence_window + self._pt_sequence_shift_y - 1)))\n",
    "            return indices\n",
    "        else:\n",
    "            try:\n",
    "                return [ i for i in self._pt_train_valid_indices if i in self.index ]\n",
    "            except:\n",
    "                return np.where(self[self._all_columns].notnull().all(1))[0]\n",
    "\n",
    "    @property\n",
    "    def _shuffle(self):\n",
    "        return ((self._pt_shuffle is None and self._pt_split is not None) or self._pt_shuffle) and \\\n",
    "               self._pt_sequence_window is None\n",
    "        \n",
    "    def _check_len(self):\n",
    "        \"\"\"\n",
    "        Internal method, to check if then length changed, to keep the split between the train/valid/test\n",
    "        unless the length changed to obtain stable results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self._pt_len == len(self._pt_indices):\n",
    "                return True\n",
    "        except: pass\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def _indices(self):\n",
    "        try:\n",
    "            if self._pt__locked_indices:\n",
    "                return self._pt__locked_indices\n",
    "        except: pass\n",
    "        try:\n",
    "            if self._check_len():\n",
    "                return self._pt_indices\n",
    "        except: pass\n",
    "        self._pt_indices = self._indices_unshuffled\n",
    "        self._pt_len = len(self._pt_indices)\n",
    "        if self._shuffle:\n",
    "            if self._pt_random_state is not None:\n",
    "                np.random.seed(self._pt_random_state)\n",
    "            np.random.shuffle(self._pt_indices)\n",
    "        return self._pt_indices\n",
    "        \n",
    "    @property\n",
    "    def _valid_begin(self):\n",
    "        try:\n",
    "            return int((1 - sum(self._pt_split))* len(self._indices))\n",
    "        except: \n",
    "            try:\n",
    "                return int((1 - self._pt_split)* len(self._indices))\n",
    "            except:\n",
    "                return len(self._indices)\n",
    "        \n",
    "    @property\n",
    "    def _test_begin(self):\n",
    "        try:\n",
    "            return int((1 - self._pt_split[1])* len(self._indices))\n",
    "        except:\n",
    "            return len(self._indices)\n",
    "\n",
    "    @property\n",
    "    def _train_indices_unbalanced(self):\n",
    "        return self._indices[:self._valid_begin]\n",
    "\n",
    "    def _pseudo_choose(self, indices, items):\n",
    "        r = indices[:items % len(indices)]\n",
    "        while items >= len(indices):\n",
    "            r = np.hstack([r, indices])\n",
    "            items -= len(indices)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _train_indices(self):\n",
    "        try:\n",
    "            if self._pt__locked_train_indices:\n",
    "                return self._pt__locked_train_indices\n",
    "        except: pass\n",
    "        indices = self._train_indices_unbalanced\n",
    "        if self._pt_balance is not None:\n",
    "            y = self.iloc[indices][self._columny]\n",
    "            classes = np.unique(y)\n",
    "            classindices = {c:np.where(y==c)[0] for c in classes}\n",
    "            classlengths = {c:len(indices) for c, indices in classindices.items()}\n",
    "            if self._pt_balance == True: # equal classes\n",
    "                n = max(classlengths.values())\n",
    "                mask = np.hstack([self._pseudo_choose(classindices[c], n) for c in classes])\n",
    "            else:                        # use given weights\n",
    "                weights = self._pt_balance\n",
    "                n = max([ int(math.ceil(classlengths[c] / w)) for c, w in weights.items() ])\n",
    "                mask = np.hstack([self._pseudo_choose(classindices[c], round(n*weights[c])) for c in classes])\n",
    "            indices = np.array(indices)[ mask ]\n",
    "        return indices\n",
    "\n",
    "    @property\n",
    "    def _valid_indices(self):\n",
    "        try:\n",
    "            if self._pt__locked_valid_indices:\n",
    "                return self._pt__locked_valid_indices\n",
    "        except: pass\n",
    "        return self._indices[self._valid_begin:self._test_begin]\n",
    "\n",
    "    @property\n",
    "    def _test_indices(self):\n",
    "        try:\n",
    "            if self._pt__locked_test_indices:\n",
    "                return self._pt__locked_test_indices\n",
    "        except: pass\n",
    "        try:\n",
    "            if self._pt_test_indices is not None:\n",
    "                return self._pt_test_indices\n",
    "        except: pass\n",
    "        return self._indices[self._test_begin:]\n",
    "\n",
    "    def df_to_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Converts the given df to a DataSet using the pipeline of this DFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame or DFrame\n",
    "                to convert into a DataSet\n",
    "        \n",
    "        returns: Converts the given df to a DataSet.\n",
    "        \"\"\"\n",
    "        assert self.is_locked, 'You can only use a locked DFrame, to prevent inconsistencies in the transformation'\n",
    "        return r.df_to_dset(df).to_dataset()\n",
    "        \n",
    "    def to_datasets(self, dataset=None):\n",
    "        \"\"\"\n",
    "        Locks the DFrame (for a consistent split) and \n",
    "        returns a list with a train, valid and (optionally) test DataSet. \n",
    "        \n",
    "        Arguments:\n",
    "            dataset: class (None)\n",
    "                The DataSet class to use\n",
    "        \n",
    "        Returns: list(DataSet)\n",
    "        \"\"\"\n",
    "        self.lock()\n",
    "        self._pt_dataset = dataset\n",
    "        res = [ self.train.to_dataset() ]\n",
    "        if len(self._valid_indices) > 0:\n",
    "            res.append(self.valid.to_dataset())\n",
    "        if len(self._test_indices) > 0:\n",
    "            res.append(self.test.to_dataset())\n",
    "        return res\n",
    "        \n",
    "    def df_to_dset(self, df):\n",
    "        \"\"\"\n",
    "        Converts a DataFrame to a DSet that has the same pipeline as this DFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame\n",
    "        \n",
    "        Returns: DSet\n",
    "        \"\"\"\n",
    "        assert self.is_locked, 'You can only use a locked DFrame, to prevent inconsistencies in the transformation'\n",
    "        return self._dset(df, range(len(df)))\n",
    "        \n",
    "    def to_databunch(self, dataset=None, batch_size=32, num_workers=0, shuffle=True, pin_memory=False, balance=False):\n",
    "        \"\"\"\n",
    "        returns: a Databunch that contains dataloaders for the train, valid and test part.\n",
    "        batch_size, num_workers, shuffle, pin_memory: see Databunch/Dataloader constructor\n",
    "        \"\"\"\n",
    "        return Databunch(self, *self.to_datasets(dataset=dataset), \n",
    "                         batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, \n",
    "                         pin_memory=pin_memory, balance=balance)    \n",
    "\n",
    "    def kfold(self, n_splits=5):\n",
    "        \"\"\"\n",
    "        Prepare the PipeTorch DataFrame for k-fold cross validation.\n",
    "        n_splits: the number of folds\n",
    "        return: a sequence of k PipeTorch DataFrames across which every item is used in the validation set once\n",
    "        and the training splits and validation splits are disjoint.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        for train_ind, valid_ind in kf.split(self._train_indices):\n",
    "            r = copy.copy(self)\n",
    "            r._train_indices = self._train_indices[train_ind]\n",
    "            r._valid_indices = self._train_indices[valid_ind]\n",
    "            yield r\n",
    "    \n",
    "    def evaluate(self, *metrics):\n",
    "        self.lock()\n",
    "        return Evaluator(self, *metrics)\n",
    "   \n",
    "    def from_numpy(self, x):\n",
    "        if x.shape[1] == len(self._columnx) + len(self._columny):\n",
    "            y = x[:,-len(self._columny):]\n",
    "            x = x[:,:-len(self._columny)]\n",
    "        elif x.shape[1] == len(self._columnx):\n",
    "            y = np.zeros((len(x), len(self._columny)))\n",
    "        else:\n",
    "            raise ValueError('x must either have as many columns in x or the entire df')\n",
    "        series = [ pd.Series(s.reshape(-1), name=c) for s, c in zip(x.T, self._columnx)]\n",
    "        series.extend([ pd.Series(s.reshape(-1), name=c) for s, c in zip(y.T, self._columny) ] )\n",
    "        df = pd.concat(series, axis=1)\n",
    "        return self._dset(df)\n",
    "    \n",
    "    def from_list(self, x):\n",
    "        return self.from_numpy(np.array(x))\n",
    "    \n",
    "    def _dset_indices(self, indices, transforms):\n",
    "        if self._pt_sequence_window is None:\n",
    "            return self._dset(self.iloc[indices], indices, transforms)\n",
    "        else:\n",
    "            try:\n",
    "                low, high = min(indices), max(indices) + self._sequence_window + self._shift_y - 1\n",
    "                return self._dset(self.iloc[low:high], list(range(low, high)), transforms)\n",
    "            except:\n",
    "                return self._dset(self.iloc[:0], [], transforms)\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Returns or creates a DSet and optionally trains transformation parameters, e.g. for image\n",
    "        normalization or text tokenization. Transformations are only supported for PyTorch DataSets.\n",
    "        \n",
    "        Returns: DSet\n",
    "            with the train subset of the DFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.is_locked:\n",
    "                return self._pt__locked_train\n",
    "        except: pass\n",
    "        self.lock()\n",
    "        self._pt__locked_train = self._dset_indices(self._train_indices, self._transforms())\n",
    "        if self._train_transformation_parameters(self._pt__locked_train):\n",
    "            self._pt__locked_train = self._dset_indices(self._train_indices, self._transforms())\n",
    "        return self._pt__locked_train\n",
    "    \n",
    "    @property\n",
    "    def raw_train(self):\n",
    "        self.lock()\n",
    "        return self._dset_indices(self._train_indices, self._transforms())\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        try:\n",
    "            if self.is_locked:\n",
    "                return self._pt__locked_valid\n",
    "        except: pass\n",
    "        self.lock()\n",
    "        self._pt__locked_valid = self._dset_indices(self._valid_indices, self._transforms(train=False))\n",
    "        return self._pt__locked_valid\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        self.lock()\n",
    "        if self._pt_sequence_window is None:\n",
    "            return DSet.df_to_testset(self.iloc[self._test_indices], self, self._test_indices, self._transforms(train=False))\n",
    "        else:\n",
    "            low, high = min(self._test_indices), max(self._test_indices) + self._sequence_window + self._shift_y - 1\n",
    "            return DSet.df_to_testset(self.iloc[low:high], self, list(range(low, high)), self._transforms(train=False))\n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.train.X\n",
    "            \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train.y\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.valid.X\n",
    "       \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid.y\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test.X\n",
    "            \n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test.y\n",
    "    \n",
    "    def loss_surface(self, model, loss, **kwargs):\n",
    "        self.evaluate(loss).loss_surface(model, loss, **kwargs)\n",
    "    \n",
    "    def inverse_scale_y(self, y):\n",
    "        \"\"\"\n",
    "        Inversely scale an output y vector.\n",
    "        \n",
    "        Arguments:\n",
    "        y: Numpy array or PyTorch tensor\n",
    "            with the output that where preprocessed by this DFrame or predicted by the model\n",
    "        \n",
    "        Return: Pandas DataFrame\n",
    "            That is reconstructed from Numpy arrays/Pytorch tensors\n",
    "            that are transformed back to the original scale. \n",
    "        \"\"\"\n",
    "        y = to_numpy(y)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1,1)\n",
    "        df = pd.DataFrame(self._inverse_scale(to_numpy(y), self._scalery, self._columny))\n",
    "        if self._categoryy() is not None:\n",
    "            for c, cat in zip(self._columny, self._categoryy()):\n",
    "                if cat is not None:\n",
    "                    df[c] = cat.inverse_transform(df[c])\n",
    "        return df\n",
    "\n",
    "    def add_column(self, y, indices, erase_y=True, columns=None):\n",
    "        \"\"\"\n",
    "        Adds a column with values for the target variable to the DataFrame. When applicable, the transformation\n",
    "        for the target variable is automatically inverted. This is useful to evaluate or visualize\n",
    "        results.\n",
    "        \n",
    "        Arguments:\n",
    "            y: Numpy/PyTorch array\n",
    "                values in the same for as those generated for the target variable \n",
    "                (possibly predictions) that will be added to the DataFrame\n",
    "            erase_y: bool (True)\n",
    "                whether the original target variable is removed \n",
    "        \n",
    "        Returns: copy of DFrame \n",
    "            to which y is added \n",
    "        \"\"\"\n",
    "        df_y = self.inverse_scale_y(y)\n",
    "        r = copy.deepcopy(self)\n",
    "        if columns is None:\n",
    "            columns = [ c + '_pred' for c in self._columny ]\n",
    "        r[columns] = np.NaN\n",
    "        r.loc[r.index[indices], columns] = df_y.values\n",
    "        return self._dframe(r)\n",
    "    \n",
    "    def inverse_scale_X(self, X):\n",
    "        \"\"\"\n",
    "        Inversely scale an input X matrix.\n",
    "        \n",
    "        Arguments:\n",
    "        X: Numpy array or PyTorch tensor\n",
    "            with the input features that where preprocessed by this DataFrame\n",
    "        \n",
    "        Return: Pandas DataFrame\n",
    "            That is reconstructed from Numpy arrays/Pytorch tensors\n",
    "            that are transformed back to the orignal scale. \n",
    "        \"\"\"\n",
    "        if self._pt_bias:\n",
    "            X = X[:, 1:]\n",
    "        if self._pt_polynomials is not None:\n",
    "            X = X[:, :len(self._columnx)]\n",
    "        df = self._inverse_scale(to_numpy(X), self._scalerx[:len(self._columnx)], self._columnx)\n",
    "        if self._categoryx() is not None:\n",
    "            for c, cat in zip(self._columnx, self._categoryx()):\n",
    "                if cat is not None:\n",
    "                    df[c] = cat.inverse_transform(df[c])\n",
    "        return df\n",
    "\n",
    "    def _inverse_scale(self, data, scalerlist, columns):\n",
    "        data = to_numpy(data)\n",
    "        if scalerlist is not None:\n",
    "            data = [ data[:, i:i+1] if scaler is None else scaler.inverse_transform(data[:,i:i+1]) for i, scaler in enumerate(scalerlist) ]\n",
    "        series = [ pd.Series(x.reshape(-1), name=c) for x, c in zip(data, columns)]\n",
    "        return pd.concat(series, axis=1)\n",
    "\n",
    "    def inverse_scale(self, X, y, y_pred = None, cum=None):\n",
    "        \"\"\"\n",
    "        Reconstructs a DSet from Numpy arrays/Pytorch tensors\n",
    "        that are scaled back to the original scale. \n",
    "        This is useful for evaluation or to visualize results.\n",
    "        \n",
    "        Arguments:\n",
    "            X: Numpy array or PyTorch tensor \n",
    "                with the same format as input features that were generated by the DataFrame\n",
    "            y: Numpy array or PyTorch tensor\n",
    "                with the same format as the target variable that was generated by the\n",
    "                DataFrame\n",
    "            pred_y (optional): Numpy array or PyTorch tensor\n",
    "                with the same format as the target variable that was generated by the\n",
    "                DataFrame\n",
    "            cum (optional): DSet\n",
    "                an dataset to add the results to, \n",
    "                to accumulate predictions over several mini-batches.\n",
    "        \n",
    "        Returns: DSet \n",
    "        \"\"\"\n",
    "        y = self.inverse_scale_y(y)\n",
    "        X = self.inverse_scale_X(X)\n",
    "        if y_pred is not None:\n",
    "            y_pred = self.inverse_scale_y(y_pred).add_suffix('_pred')\n",
    "            df = pd.concat([X, y, y_pred], axis=1)\n",
    "        else:\n",
    "            df = pd.concat([X, y], axis=1)\n",
    "        if cum is not None:\n",
    "            df = pd.concat([cum, df])\n",
    "        df = self._dset(df)\n",
    "        return df\n",
    "        \n",
    "    def plot_boundary(self, predict):\n",
    "        self.evaluate().plot_boundary(predict)\n",
    "\n",
    "    def balance(self, weights=True):\n",
    "        \"\"\"\n",
    "        Oversamples rows in the training set, so that the values of the target variable \n",
    "        are better balanced. Does not affect the valid/test set.\n",
    "        \n",
    "        Arguments:\n",
    "            weights: True or dict\n",
    "                when set to True, the target values of the training set are \n",
    "                uniformely distributed,\n",
    "                otherwise a dictionary can be passed that map target values to the \n",
    "                desired fraction of the training set (e.g. {0:0.4, 1:0.6}).\n",
    "        \n",
    "        Returns: copy of DFrame\n",
    "            schdules to balance the train set\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_balance = weights\n",
    "        return r    \n",
    "  \n",
    "    def scale(self, columns=True, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        \"\"\"\n",
    "        Scales the features and target variable in the DataFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            columns: True, str or list of str (True)\n",
    "                the columns to scale (True for all)\n",
    "            scalertype: an SKLearn type scaler (StandardScaler)\n",
    "            omit_interval: (-2,2) when colums is set to True\n",
    "                all columns whose values lie outside the omit_interval,\n",
    "        \n",
    "        Return: copy of DFrame\n",
    "            schedules scaling the indicated columns, \n",
    "            using a scaler that is fitted om the training set\n",
    "            and applied to train, valid and test set.\n",
    "        \"\"\"\n",
    "        if self._pt_polynomials and columns != 'x_only':\n",
    "            assert type(columns) != list or len(columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_scale_columns = columns\n",
    "        r._pt_scalertype = scalertype\n",
    "        r._pt_scale_omit_interval = omit_interval\n",
    "        return r\n",
    "    \n",
    "    def scalex(self, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        \"\"\"\n",
    "        Scale all input features.\n",
    "        \n",
    "        Arguments:\n",
    "            scalertype: SKLearn scaler class (StandardScaler)\n",
    "            omit_interval: (-2,2) features whose values lie within this interval are not scaled\n",
    "        \n",
    "        Returns: copy of DFrame \n",
    "            schedules all input features (with values outside omit_interval)\n",
    "            to be scaled (see scale)\n",
    "        \"\"\"\n",
    "        return self.scale(columns='x_only', scalertype=scalertype, omit_interval=omit_interval)\n",
    "    \n",
    "    def add_bias(self):\n",
    "        \"\"\"\n",
    "        Adds a bias column in the pipeline.\n",
    "        \n",
    "        Returns: copy of DFrame\n",
    "            schedules as bias column of 1's to be added to the input\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_bias = True\n",
    "        return r\n",
    "    \n",
    "    def split(self, split=0.2, shuffle=True, random_state=None):\n",
    "        \"\"\"\n",
    "        Split the data in a train/valid/(test) set.\n",
    "        The train and valid sets will be resampled.\n",
    "        \n",
    "        Arguments:\n",
    "            split: the fraction that is used for the validation set (and optionally test set). \n",
    "                When a single digit is given (default 0.2), that fraction of rows in the DataFrame will be \n",
    "                used for the validation set and the remainder for the training set. When a tuple of \n",
    "                fractions is given, those fractions of rows will be assigned to a validation and test set.\n",
    "            shuffle: shuffle the rows before splitting\n",
    "            random_state: set a random_state for reproducible results\n",
    "            \n",
    "        Returns: copy of DFrame \n",
    "            schedules the rows to be split into a train, valid and (optionally) test set.\n",
    "        \"\"\"\n",
    "        assert not self.is_locked, 'You cannot change the pipeline of a Locked DFrame'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_split = split\n",
    "        r._pt_shuffle = shuffle\n",
    "        r._pt_random_state = random_state\n",
    "        return r\n",
    "        \n",
    "    def polynomials(self, degree, include_bias=False):\n",
    "        \"\"\"\n",
    "        Adds (higher-order) polynomials to the data pipeline.\n",
    "        \n",
    "        Arguments:\n",
    "            degree: int - degree of the higher order polynomials (e.g. 2 for squared)\n",
    "            include_bias: bool (False) - whether to generate a bias column\n",
    "        \n",
    "        returns: copy of DFrame \n",
    "            schedules to generate higher order polynomials over the input features.\n",
    "        \"\"\"\n",
    "        assert type(self._pt_scale_columns) != list or len(self._pt_scale_columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_polynomials = PolynomialFeatures(degree, include_bias=include_bias)\n",
    "        return r\n",
    "    \n",
    "    def no_columny(self):\n",
    "        \"\"\"\n",
    "        PipeTorch cannot currently handle a dataset without a target variable, \n",
    "        however, it can work by simply assigning one of the used input features\n",
    "        as a target variable.\n",
    "        \n",
    "        Returns: copy of DFrame\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_columny = [self._pt_columnx[0]]\n",
    "        return r\n",
    "    \n",
    "    def columny(self, columns=None, transpose=False):\n",
    "        \"\"\"\n",
    "        By default, PipeTorch uses the last column as the target variable and transposes it to become a row vector.\n",
    "        This function can alter this default behavior. Transposing y is the default for single variable targets, \n",
    "        since most loss functions and metrics cannot handle column vectors. The set target variables are \n",
    "        automatically excluded from the input X.\n",
    "        \n",
    "        Arguments:\n",
    "            columns: str or list of str\n",
    "                single column name or list of columns that is to be used as target column. \n",
    "                None: use the last column\n",
    "            transpose: bool (False)\n",
    "                whether to transpose y. \n",
    "                When using a single target variable targets, setting this to True\n",
    "                allows to transpose the generated target vector.\n",
    "        \n",
    "        Returns: DFrame \n",
    "            schedules the given column(s) to be used as target columns, \n",
    "            and marks whether the target variable is to be transposed.\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self._copy_with_indices()\n",
    "        if columns is not None:\n",
    "            r._pt_columny = [columns] if type(columns) == str else columns\n",
    "        assert r._pt_columny is None or len(r._pt_columny) == 1 or not transpose, 'You cannot transpose multiple target columns'\n",
    "        r._pt_transposey = transpose\n",
    "        return r\n",
    "\n",
    "    def columnx(self, *columns, omit=False):\n",
    "        \"\"\"\n",
    "        Specify which columns to use as input. The target variable is always excluded\n",
    "        so if you want to add that (for sequence learning), you should copy the\n",
    "        target column. \n",
    "        Unlocks the DFrame, train and valid sets are therefore resampled.\n",
    "        \n",
    "        Arguments:\n",
    "            columns: str or list of str\n",
    "                columns to be used as input features\n",
    "            omit: bool (False)\n",
    "                when True, all columns are used except the specified target column(s)\n",
    "        \n",
    "        Returns: copy of DFrame\n",
    "            schedules the given columns to use as input.\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        if omit:\n",
    "            r._pt_columnx = [ c for c in self.columns if c not in columns ]\n",
    "        else:\n",
    "            r._pt_columnx = list(columns) if len(columns) > 0 else None\n",
    "        return r\n",
    "    \n",
    "    def category(self, *columns, sort=False):\n",
    "        \"\"\"\n",
    "        Converts the values in the targetted columns into indices, for example to use in lookup tables.\n",
    "        columns that are categorized are excluded from scaling. You cannot use this function together\n",
    "        with polynomials or bias.\n",
    "        Unlocks the DFrame, train and valid sets are therefore resampled.\n",
    "        \n",
    "        Note: PipeTorch only uses categories that are in the training set and uses category 0 as\n",
    "        an unknown category number for categories in the validation and test set that are not known during training.\n",
    "        This way, no future information is used. \n",
    "        \n",
    "        Arguments:\n",
    "            columns: str or list of str\n",
    "                list of columns that is to be converted into a category\n",
    "            sort: True/False (default False) \n",
    "                whether the unique values of these colums should be converted \n",
    "                to indices in sorted order.\n",
    "        \n",
    "        Returns: copy of the PipeTorch DataFrame\n",
    "            the columns are scheduled for conversion into categories, \n",
    "            by converting every unique value into a unique index starting from 0\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self._pt_polynomials is None, 'You cannot combine categories with polynomials'\n",
    "        assert self._pt_bias is None, 'You cannot combine categories with polynomials'\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_category = columns\n",
    "        r._pt_category_sort = sort\n",
    "        return r\n",
    "    \n",
    "    def dummies(self, *columns):\n",
    "        \"\"\"\n",
    "        Converts the values in the targetted columns into dummy variables.\n",
    "        columns that are categorized are excluded from scaling. You cannot use this function together\n",
    "        with polynomials or bias.\n",
    "        Unlocks the DFrame, train and valid sets are therefore resampled.\n",
    "        \n",
    "        Note: PipeTorch only uses categories that are in the training set and uses category 0 as\n",
    "        an unknown category number for categories in the validation and test set that are not known during training.\n",
    "        This way, no future information is used. \n",
    "        \n",
    "        Arguments:\n",
    "            columns: str or list of str\n",
    "                the columns that are to be converted into a category\n",
    "            sort: bool (False)\n",
    "                whether the unique values of these colums should be converted \n",
    "                to indices in sorted order.\n",
    "        \n",
    "        Returns: copy of DFrame \n",
    "            schedules to convert the columns into categories, \n",
    "            for which every unique value is converted into a unique index starting from 0\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self._pt_polynomials is None, 'You cannot combine categories with polynomials'\n",
    "        assert self._pt_bias is None, 'You cannot combine categories with polynomials'\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_dummies = columns\n",
    "        return r\n",
    "\n",
    "    def sequence(self, window, shift_y = 1):\n",
    "        \"\"\"\n",
    "        The rows in the DataFrame are considered to be a continuous sequence over time. \n",
    "        From this sequence, input samples are created that contain the features over a 'window'\n",
    "        of rows. Ths allows to learn a model to predict a target based on prior history. The\n",
    "        samples are generated so that their window overlaps.\n",
    "        Unlocks the DFrame, train and valid sets are therefore resampled.\n",
    "        \n",
    "        Samples with NaN's are automatically skipped. When DataFrames are grouped, samples will\n",
    "        be created only within groups.\n",
    "        \n",
    "        Arguments:\n",
    "        window: int\n",
    "            the number of rows that is used a an input\n",
    "        shift_y: int\n",
    "            how many rows into the future the target variable is placed, \n",
    "            e.g. with a window=2 and shift_y=1, X0 would contain [x[0], x[1]] \n",
    "            while y0 would contain y[2], \n",
    "            the next sample X1 would be [x[1], x[2]] and y[3].\n",
    "        \n",
    "        Returns: copy of DFrame\n",
    "            schedules generating samples of sequences, using a sliding window \n",
    "            for learning on sequences\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_sequence_window = window\n",
    "        r._pt_sequence_shift_y = shift_y\n",
    "        return r\n",
    "    \n",
    "    def train_transforms(self, *transforms):\n",
    "        \"\"\"\n",
    "        Configure a (list of) transformation function(s) that is called from the DataSet class to prepare the \n",
    "        train data.\n",
    "        \n",
    "        Arguments:\n",
    "            *transforms: [ callable ]\n",
    "                (list of) transformation function(s) that is called from the DataSet class to prepare the data.\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_train_transforms = transforms\n",
    "        return r\n",
    "    \n",
    "    def transforms(self, *transforms):\n",
    "        \"\"\"\n",
    "        Configure a (list of) transformation function(s) that is called from the DataSet class to prepare the data.\n",
    "\n",
    "        Arguments:\n",
    "            *transforms: [ callable ]\n",
    "                (list of) transformation function(s) that is called from the DataSet class to prepare the data.\n",
    "        \"\"\"\n",
    "        r = self._copy_with_indices()\n",
    "        r._pt_transforms = transforms\n",
    "        return r\n",
    "\n",
    "    def _train_transformation_parameters(self, train_dset):\n",
    "        \"\"\"\n",
    "        Placeholder to extend DFrame to train transformations on the train DSet. This is used by\n",
    "        ImageDFrame to normalize images and TextDFrame to tokenize text.\n",
    "        Note that the mechanism assumes that the train DSet is generated first.\n",
    "        \n",
    "        Arguments: \n",
    "            train_dset: DSet\n",
    "                The train DSet to learn the transformation parameters on\n",
    "            \n",
    "        Returns: bool (None)\n",
    "            This function should return True when parameters were learned, to force DFrame to add a new\n",
    "            list of transformations to the generated DSet.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _pre_transforms(self):\n",
    "        \"\"\"\n",
    "        Placeholder for standard transformations that precede configured transformations.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def _post_transforms(self):\n",
    "        \"\"\"\n",
    "        Placeholder for standard transformations that succede configured transformations.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def _transforms(self, pre=True, train=True, standard=True, post=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            pre: bool (True) - whether to include pre transformations\n",
    "            train: bool (True) - whether to include train transformations\n",
    "            standard: bool (True) - whether to include standard transformations\n",
    "            post: bool (True) - whether to include post transformations\n",
    "            \n",
    "        Returns: [ callable ]\n",
    "            A list of transformations that is applied to the generated train DataSet \n",
    "        \"\"\"\n",
    "        t = []\n",
    "        try:\n",
    "            if pre:\n",
    "                t.extend(self._pre_transforms())\n",
    "        except: pass\n",
    "        try:\n",
    "            if train:\n",
    "                t.extend(self._pt_train_transforms)\n",
    "        except: pass\n",
    "        try:\n",
    "            t.extend(self._pt_transforms)\n",
    "        except: pass\n",
    "        try:\n",
    "            if post:\n",
    "                t.extend( self._post_transforms() )\n",
    "        except: pass\n",
    "        return t\n",
    "    \n",
    "class DFrame(pd.DataFrame, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DFrame.__init__(self, data)\n",
    "        \n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return DFrame\n",
    "    \n",
    "    def groupby(self, by, axis=0, level=None, as_index=True, sort=True, group_keys=True, observed=False, dropna=True):\n",
    "        \"\"\"\n",
    "        Groups a DFrame just like a Pandas DataFrame. This is useful for \n",
    "        learning sequences, since samples are only created using a sliding window\n",
    "        within groups.\n",
    "        \n",
    "        Arguments:\n",
    "            by: single column or list of columns\n",
    "            for the other arguments, see pandas.DataFrame.groupby \n",
    "        \n",
    "        Returns: pipetorch.PTGroupedDFrame\n",
    "        \"\"\"\n",
    "        r = super().groupby(by, axis=axis, level=level, as_index=as_index, sort=sort, group_keys=group_keys, observed=observed, dropna=dropna)\n",
    "        return self._copy_meta( GroupedDFrame(r) )\n",
    "\n",
    "     \n",
    "class DSeries(pd.Series, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DFrame.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return DSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return DFrame\n",
    "    \n",
    "class GroupedDSeries(SeriesGroupBy, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DFrame.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return GroupedDFrame\n",
    "    \n",
    "class GroupedDFrame(DataFrameGroupBy, _DFrame):\n",
    "    _metadata = _DFrame._metadata\n",
    "    _internal_names = _DFrame._internal_names\n",
    "    _internal_names_set = _DFrame._internal_names_set\n",
    "\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__(obj=data.obj, keys=data.keys, axis=data.axis, level=data.level, grouper=data.grouper, exclusions=data.exclusions,\n",
    "                selection=data._selection, as_index=data.as_index, sort=data.sort, group_keys=data.group_keys,\n",
    "                observed=data.observed, mutated=data.mutated, dropna=data.dropna)\n",
    "        _DFrame.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDFrame\n",
    "    \n",
    "    @property\n",
    "    def _constructor_sliced(self):\n",
    "        return GroupedDSeries\n",
    "    \n",
    "    def astype(self, dtype, copy=True, errors='raise'):\n",
    "        \"\"\"\n",
    "        see: pipetorch.DFrame.astype\n",
    "        \"\"\"\n",
    "        DFrame.astype(self, dtype, copy=copy, errors=errors)\n",
    "\n",
    "    def get_group(self, namel, obj=None):\n",
    "        return self._dframe( super().get_group(name, obj=obj) )\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for group, sub0set in super().__iter__():\n",
    "            yield group, self._copy_meta(subset)\n",
    "        \n",
    "    def to_dataset(self):\n",
    "        \"\"\"\n",
    "        Convert 0a grouped DFrame into a PyTorch ConcatDataset over datasets\n",
    "        for every group contained.\n",
    "        \n",
    "        returns: train, valid, test DataSet\n",
    "        \"\"\"\n",
    "        from torch.utils.data import ConcatDataset\n",
    "        dss = []\n",
    "        for key, group in self:\n",
    "            dss.append( self._dframe(group).to_dataset())\n",
    "\n",
    "        return [ConcatDataset(ds) for ds in zip(*dss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655beac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc47353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
