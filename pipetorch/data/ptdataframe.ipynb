{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7a0297b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ptdataframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ptdataframe.py        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import linecache\n",
    "from ..evaluate.evaluate import Evaluator\n",
    "from .databunch import Databunch\n",
    "from .ptdataset import PTDataSet\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n",
    "from collections import defaultdict\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "class show_warning:\n",
    "    def __enter__(self):\n",
    "        self.warning = warnings.catch_warnings(record=True)\n",
    "        self.w = self.warning.__enter__()\n",
    "        warnings.filterwarnings('error')\n",
    "        warnings.simplefilter('default')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        for wi in self.w:\n",
    "            if wi.line is None:\n",
    "                print(wi.filename)\n",
    "                wi.line = linecache.getline(wi.filename, wi.lineno)\n",
    "        print(f'line number {wi.lineno}  line {wi.line}') \n",
    "        self.warning.__exit__(exc_type, exc_value, exc_traceback)\n",
    "\n",
    "class PT:\n",
    "    _metadata = ['_pt_scale_columns', '_pt_scale_omit_interval', '_pt_scalertype', '_pt_columny', '_pt_columnx', '_pt_transposey', '_pt_bias', '_pt_polynomials', '_pt_dtype', '_pt_category', '_pt_category_sort', '_pt_dummies', '_pt_sequence_window', '_pt_sequence_shift_y', '_pt_shuffle', '_pt_split', '_pt_random_state', '_pt_balance', '_pt_len', '_pt_indices', '_pt_train_valid_indices', '_pt_test_indices']\n",
    "\n",
    "    @classmethod\n",
    "    def read_csv(cls, path, **kwargs):\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        return cls(df)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dfs(cls, *dfs, **kwargs):\n",
    "        return cls(pd.concat(dfs), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_train_test(cls, train, test, **kwargs):\n",
    "        r = cls(pd.concat([train, test], ignore_index=True))\n",
    "        r._pt_train_valid_indices = list(range(len(train)))\n",
    "        r._pt_test_indices = list(range(len(train), len(train)+len(test)))\n",
    "        return r\n",
    "    \n",
    "    def __init__(self, data, **kwargs):\n",
    "        for m in self._metadata:\n",
    "            self.__setattr__(m, None)\n",
    "            try:\n",
    "                self.__setattr__(m, getattr(data, m))\n",
    "            except: pass\n",
    "\n",
    "    def _copy_meta(self, r):\n",
    "        for c in self._metadata:\n",
    "            setattr(r, c, getattr(self, c))\n",
    "        return r\n",
    "                    \n",
    "    def _ptdataframe(self, data):\n",
    "        return self._copy_meta( PTDataFrame(data) )\n",
    "\n",
    "    def _ptlockeddataframe(self, data):\n",
    "        return self._copy_meta( PTLockedDataFrame(data) )\n",
    "\n",
    "    def locked(self):\n",
    "        return self._ptlockeddataframe(self)\n",
    "    \n",
    "    def _ptdataset(self, data, indices=None):\n",
    "        if indices is None:\n",
    "            indices = list(range(len(data)))\n",
    "        return PTDataSet.from_ptdataframe(data, self, indices)\n",
    "    \n",
    "    @property\n",
    "    def _columny(self):\n",
    "        try:\n",
    "            if len(self._pt_columny) > 0:\n",
    "                return self._pt_columny\n",
    "        except: pass\n",
    "        return [ self.columns[-1] ]\n",
    "        \n",
    "    @property\n",
    "    def _transposey(self):\n",
    "        return self._pt_transposey\n",
    "        \n",
    "    def astype(self, dtype, copy=True, errors='raise'):\n",
    "        self._pt_dtype = dtype\n",
    "        return super().astype(dtype, copy=copy, errors=errors)\n",
    "        \n",
    "    @property\n",
    "    def _columnx(self):\n",
    "        if self._pt_columnx is None:\n",
    "            return [ c for c in self.columns if c not in self._columny ]\n",
    "        return [ c for c in self._pt_columnx if c not in self._columny ]\n",
    "   \n",
    "    @property\n",
    "    def _all_columns(self):\n",
    "        return list(set(self._columnx).union(set(self._columny)))\n",
    "\n",
    "    @property\n",
    "    def _columnsx_scale_indices(self):\n",
    "        if self._pt_polynomials is not None and self._pt_scale_columns is not None:\n",
    "            X = self.train._x_polynomials\n",
    "            return [ i for i in range(X.shape[1]) if (X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1]) ]\n",
    "        columnx = self._columnx\n",
    "        cat = set(self._pt_category) if type(self._pt_category) == tuple else []\n",
    "        if self._pt_scale_columns == True or self._pt_scale_columns == 'x_only':\n",
    "            r = [ c for c in columnx if c not in cat]\n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            r = []\n",
    "        else:\n",
    "            r = [ c for c in columnx if c in self._pt_scale_columns and c not in cat ]\n",
    "        X = self.train._x_polynomials\n",
    "        r = [ columnx.index(c) for i, c in enumerate(columnx) if c in r and ((X[:,i].min() < self._pt_scale_omit_interval[0] or X[:,i].max() > self._pt_scale_omit_interval[1])) ]\n",
    "        return r\n",
    "        \n",
    "    @property\n",
    "    def _columnsy_scale_indices(self):\n",
    "        columny = self._columny\n",
    "        cat = set(self._pt_category) if type(self._pt_category) == tuple else []\n",
    "        if self._pt_scale_columns == True:\n",
    "            y = self.train._y_numpy\n",
    "            r = [ c for i, c in enumerate(columny) if c not in cat and (y[:,i].min() < self._pt_scale_omit_interval[0] or y[:,i].max() > self._pt_scale_omit_interval[1]) ]\n",
    "        elif self._pt_scale_columns == False or self._pt_scale_columns is None or len(self._pt_scale_columns) == 0:\n",
    "            r = []\n",
    "        else:\n",
    "            r = [ c for c in columny if c in self._pt_scale_columns and c not in cat ]\n",
    "        return [ columny.index(c) for c in r ]\n",
    "\n",
    "    @property\n",
    "    def _scalerx(self):\n",
    "        X = self.train._x_polynomials\n",
    "        s = [ None ] * X.shape[1]\n",
    "        for i in self._columnsx_scale_indices:\n",
    "            s[i] = self._create_scaler(self._pt_scalertype, X[:, i:i+1])\n",
    "        return s\n",
    "        \n",
    "    @property\n",
    "    def _scalery(self):\n",
    "        y = self.train._y_numpy\n",
    "        s = [ None ] * y.shape[1]\n",
    "        for i in self._columnsy_scale_indices:\n",
    "            s[i] = self._create_scaler(self._pt_scalertype, y[:, i:i+1])\n",
    "        return s\n",
    "    \n",
    "    def _categoryx(self):\n",
    "        try:\n",
    "            if self._pt_category is None or len(self._pt_category) == 0:\n",
    "                return None\n",
    "        except: pass\n",
    "        return [ self._create_category(c) for c in self._columnx ]          \n",
    "    \n",
    "    def _categoryy(self):\n",
    "        try:\n",
    "            if self._pt_category is None or len(self._pt_category) == 0:\n",
    "                return None\n",
    "        except: pass\n",
    "        return [ self._create_category(c) for c in self._columny ]\n",
    "\n",
    "    def _create_category(self, column):\n",
    "        sort = self._pt_category_sort\n",
    "        class Category:\n",
    "            def fit(self, X):\n",
    "                s = X.unique()\n",
    "                if sort:\n",
    "                    s = sorted(s)\n",
    "                self.dict = defaultdict(lambda:0, { v:(i+1) for i, v in enumerate(s) })\n",
    "                self.inverse_dict = { (i+1):v for i, v in enumerate(s) } + {0:np.NaN}\n",
    "            \n",
    "            def transform(self, X):\n",
    "                return X.map(self.dict)\n",
    "            \n",
    "            def inverse_transform(self, X):\n",
    "                return X.map(self.inverse_dict)\n",
    "            \n",
    "        if column not in self._pt_category:\n",
    "            return None\n",
    "        \n",
    "        c = Category()\n",
    "        c.fit(self.train[column])\n",
    "        return c\n",
    "    \n",
    "    def _dummiesx(self):\n",
    "        try:\n",
    "            if self._pt_dummies is None or len(self._pt_dummies) == 0:\n",
    "                return None\n",
    "        except: pass\n",
    "        return [ self._create_dummies(c) for c in self._columnx ]          \n",
    "    \n",
    "    def _dummiesy(self):\n",
    "        try:\n",
    "            if self._pt_dummies is None or len(self._pt_dummies) == 0:\n",
    "                return None\n",
    "        except: pass\n",
    "        return [ self._create_dummies(c) for c in self._columny ]\n",
    "\n",
    "    def _create_dummies(self, column):    \n",
    "        if column not in self._pt_dummies:\n",
    "            return None\n",
    "        \n",
    "        c = OneHotEncoder(handle_unknown='ignore')\n",
    "        c.fit(self.train[[column]])\n",
    "        return c\n",
    "    \n",
    "    @property\n",
    "    def _shift_y(self):\n",
    "        if self._pt_sequence_shift_y is not None:\n",
    "            return self._pt_sequence_shift_y\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @property\n",
    "    def _sequence_window(self):\n",
    "        try:\n",
    "            if self._pt_sequence_window is not None:\n",
    "                return self._pt_sequence_window\n",
    "        except:pass\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def _sequence_index_y(self):\n",
    "        return self._sequence_window+self._shift_y-1\n",
    "    \n",
    "    @property\n",
    "    def _indices_unshuffled(self):\n",
    "        if self._pt_sequence_window is not None:\n",
    "            try:\n",
    "                self._pt_train_valid_indices = [ i for i in self._pt_train_valid_indices if i in self.index ]\n",
    "                indices = self._pt_train_valid_indices[:-(self._pt_sequence_window + self._pt_sequence_shift_y - 1)]\n",
    "            except:\n",
    "                indices = list(range(len(self) - (self._pt_sequence_window + self._pt_sequence_shift_y - 1)))\n",
    "            return indices\n",
    "        else:\n",
    "            try:\n",
    "                return [ i for i in self._pt_train_valid_indices if i in self.index ]\n",
    "            except:\n",
    "                return np.where(self[self._all_columns].notnull().all(1))[0]\n",
    "\n",
    "    @property\n",
    "    def _shuffle(self):\n",
    "        return ((self._pt_shuffle is None and self._pt_split is not None) or self._pt_shuffle) and \\\n",
    "               self._pt_sequence_window is None\n",
    "        \n",
    "    def _check_len(self):\n",
    "        \"\"\"\n",
    "        Internal method, to check if then length changed, to keep the split between the train/valid/test\n",
    "        unless the length changed to obtain stable results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self._pt_len == len(self._pt_indices):\n",
    "                return True\n",
    "        except: pass\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def _indices(self):\n",
    "        try:\n",
    "            if self._check_len():\n",
    "                return self._pt_indices\n",
    "        except: pass\n",
    "        self._pt_indices = self._indices_unshuffled\n",
    "        self._pt_len = len(self._pt_indices)\n",
    "        if self._shuffle:\n",
    "            if self._pt_random_state is not None:\n",
    "                np.random.seed(self._pt_random_state)\n",
    "            np.random.shuffle(self._pt_indices)\n",
    "        return self._pt_indices\n",
    "        \n",
    "    @property\n",
    "    def _valid_begin(self):\n",
    "        try:\n",
    "            return int((1 - sum(self._pt_split))* len(self._indices))\n",
    "        except: \n",
    "            try:\n",
    "                return int((1 - self._pt_split)* len(self._indices))\n",
    "            except:\n",
    "                return len(self._indices)\n",
    "        \n",
    "    @property\n",
    "    def _test_begin(self):\n",
    "        try:\n",
    "            return int((1 - self._pt_split[1])* len(self._indices))\n",
    "        except:\n",
    "            return len(self._indices)\n",
    "\n",
    "    @property\n",
    "    def _train_indices_unbalanced(self):\n",
    "        return self._indices[:self._valid_begin]\n",
    "\n",
    "    def _pseudo_choose(self, indices, items):\n",
    "        r = indices[:items % len(indices)]\n",
    "        while items >= len(indices):\n",
    "            r = np.hstack([r, indices])\n",
    "            items -= len(indices)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _train_indices(self):\n",
    "        indices = self._train_indices_unbalanced\n",
    "        if self._pt_balance is not None:\n",
    "            y = self.iloc[indices][self._columny]\n",
    "            classes = np.unique(y)\n",
    "            classindices = {c:np.where(y==c)[0] for c in classes}\n",
    "            classlengths = {c:len(indices) for c, indices in classindices.items()}\n",
    "            if self._pt_balance == True: # equal classes\n",
    "                n = max(classlengths.values())\n",
    "                mask = np.hstack([self._pseudo_choose(classindices[c], n) for c in classes])\n",
    "            else:                        # use given weights\n",
    "                weights = self._pt_balance\n",
    "                n = max([ int(math.ceil(classlengths[c] / w)) for c, w in weights.items() ])\n",
    "                mask = np.hstack([self._pseudo_choose(classindices[c], round(n*weights[c])) for c in classes])\n",
    "            indices = np.array(indices)[ mask ]\n",
    "        return indices\n",
    "\n",
    "    @property\n",
    "    def _valid_indices(self):\n",
    "        return self._indices[self._valid_begin:self._test_begin]\n",
    "\n",
    "    @property\n",
    "    def _test_indices(self):\n",
    "        try:\n",
    "            if self._pt_test_indices is not None:\n",
    "                return self._pt_test_indices\n",
    "        except: pass\n",
    "        return self._indices[self._test_begin:]\n",
    "\n",
    "    def to_dataset(self, *dfs):\n",
    "        \"\"\"\n",
    "        returns: a list with a train, valid and test DataSet. Every DataSet contains an X and y, where the \n",
    "        input data matrix X contains all columns but the last, and the target y contains the last column\n",
    "        columns: list of columns to convert, the last column is always the target. default=None means all columns.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        r = PTLockedDataFrame(self)\n",
    "        if r._pt_transposey is None:\n",
    "            r._pt_transposey = False\n",
    "        res = [ r.train.to_dataset() ]\n",
    "        if len(self._valid_indices) > 0:\n",
    "            res.append(r.valid.to_dataset())\n",
    "        if len(self._test_indices) > 0:\n",
    "            res.append(r.test.to_dataset())\n",
    "        for df in dfs:\n",
    "            res.append(r.to_subset(df).to_dataset())\n",
    "        assert len(res) < 4, 'You cannot have more than a train, valid and test set'\n",
    "        return res\n",
    "        \n",
    "    def to_subset(self, df):\n",
    "        return self._ptdataset(df, range(len(df)))\n",
    "        \n",
    "    def to_databunch(self, *dfs, batch_size=32, num_workers=0, shuffle=True, pin_memory=False, balance=False):\n",
    "        \"\"\"\n",
    "        returns: a Databunch that contains dataloaders for the train, valid and test part.\n",
    "        batch_size, num_workers, shuffle, pin_memory: see Databunch/Dataloader constructor\n",
    "        \"\"\"\n",
    "        return Databunch(self, *self.to_dataset(*dfs), batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=pin_memory, balance=balance)    \n",
    "\n",
    "    def kfold(self, n_splits=5):\n",
    "        \"\"\"\n",
    "        Prepare the PipeTorch DataFrame for k-fold cross validation.\n",
    "        n_splits: the number of folds\n",
    "        return: a sequence of k PipeTorch DataFrames across which every item is used in the validation set once\n",
    "        and the training splits and validation splits are disjoint.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        for train_ind, valid_ind in kf.split(self._train_indices):\n",
    "            r = copy.copy(self)\n",
    "            r._train_indices = self._train_indices[train_ind]\n",
    "            r._valid_indices = self._train_indices[valid_ind]\n",
    "            yield r\n",
    "    \n",
    "    def evaluate(self, *metrics):\n",
    "        return Evaluator(self, *metrics)\n",
    "   \n",
    "    def from_numpy(self, x):\n",
    "        if x.shape[1] == len(self._columnx) + len(self._columny):\n",
    "            y = x[:,-len(self._columny):]\n",
    "            x = x[:,:-len(self._columny)]\n",
    "        elif x.shape[1] == len(self._columnx):\n",
    "            y = np.zeros((len(x), len(self._columny)))\n",
    "        else:\n",
    "            raise ValueError('x must either have as many columns in x or the entire df')\n",
    "        series = [ pd.Series(s.reshape(-1), name=c) for s, c in zip(x.T, self._columnx)]\n",
    "        series.extend([ pd.Series(s.reshape(-1), name=c) for s, c in zip(y.T, self._columny) ] )\n",
    "        df = pd.concat(series, axis=1)\n",
    "        return self._ptdataset(df)\n",
    "    \n",
    "    def from_list(self, x):\n",
    "        return self.from_numpy(np.array(x))\n",
    "    \n",
    "    def _ptdataset_indices(self, indices):\n",
    "        if self._pt_sequence_window is None:\n",
    "            return self._ptdataset(self.iloc[indices], indices)\n",
    "        else:\n",
    "            try:\n",
    "                low, high = min(indices), max(indices) + self._sequence_window + self._shift_y - 1\n",
    "                return self._ptdataset(self.iloc[low:high], list(range(low, high)))\n",
    "            except:\n",
    "                return self._ptdataset(self.iloc[:0], [])\n",
    "    \n",
    "    @property\n",
    "    def full(self):\n",
    "        return self._ptdataset_indices(np.concatenate([self._train_indices, self._valid_indices]))\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._ptdataset_indices(self._train_indices)\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        return self._ptdataset_indices(self._valid_indices)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        if self._pt_sequence_window is None:\n",
    "            return PTDataSet.df_to_testset(self.iloc[self._test_indices], self, self._test_indices)\n",
    "        else:\n",
    "            low, high = min(self._test_indices), max(self._test_indices) + self._sequence_window + self._shift_y - 1\n",
    "            return PTDataSet.df_to_testset(self.iloc[low:high], self, list(range(low, high)))\n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.train.X\n",
    "            \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train.y\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.valid.X\n",
    "       \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid.y\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return self.test.X\n",
    "            \n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return self.test.y\n",
    "    \n",
    "    @property\n",
    "    def full_X(self):\n",
    "        return self.full.X\n",
    "            \n",
    "    @property\n",
    "    def full_y(self):\n",
    "        return self.full.y\n",
    "\n",
    "    def scalex(self, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        return self.scale(columns='x_only', scalertype=scalertype, omit_interval=omit_interval)\n",
    "    \n",
    "    def loss_surface(self, model, loss, **kwargs):\n",
    "        self.evaluate(loss).loss_surface(model, loss, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_scaler(scalertype, column):\n",
    "        scaler = scalertype()\n",
    "        scaler.fit(column)\n",
    "        return scaler\n",
    "\n",
    "    def inverse_transform_y(self, y):\n",
    "        y = to_numpy(y)\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1,1)\n",
    "        df = pd.DataFrame(self._inverse_transform(to_numpy(y), self._scalery, self._columny))\n",
    "        if self._categoryy is not None:\n",
    "            for c, cat in zip(self._columny, self._categoryy):\n",
    "                if cat is not None:\n",
    "                    df[c] = cat.inverse_transform(df[c])\n",
    "        return df\n",
    "\n",
    "    def add_column(self, y, indices, erase_y=True, columns=None):\n",
    "        df_y = self.inverse_transform_y(y)\n",
    "        r = copy.deepcopy(self)\n",
    "        if columns is None:\n",
    "            columns = [ c + '_pred' for c in self._columny ]\n",
    "        r[columns] = np.NaN\n",
    "        r.loc[r.index[indices], columns] = df_y.values\n",
    "        return self._ptdataframe(r)\n",
    "    \n",
    "    def inverse_transform_X(self, X):\n",
    "        if self._pt_bias:\n",
    "            X = X[:, 1:]\n",
    "        if self._pt_polynomials is not None:\n",
    "            X = X[:, :len(self._columnx)]\n",
    "        df = self._inverse_transform(to_numpy(X), self._scalerx[:len(self._columnx)], self._columnx)\n",
    "        if self._categoryx is not None:\n",
    "            for c, cat in zip(self._columnx, self._categoryx):\n",
    "                if cat is not None:\n",
    "                    df[c] = cat.inverse_transform(df[c])\n",
    "        return df\n",
    "\n",
    "    def _inverse_transform(self, data, scalerlist, columns):\n",
    "        data = to_numpy(data)\n",
    "        if scalerlist is not None:\n",
    "            data = [ data[:, i:i+1] if scaler is None else scaler.inverse_transform(data[:,i:i+1]) for i, scaler in enumerate(scalerlist) ]\n",
    "        series = [ pd.Series(x.reshape(-1), name=c) for x, c in zip(data, columns)]\n",
    "        return pd.concat(series, axis=1)\n",
    "\n",
    "    def inverse_transform(self, X, y, y_pred = None, cum=None):\n",
    "        y = self.inverse_transform_y(y)\n",
    "        X = self.inverse_transform_X(X)\n",
    "        if y_pred is not None:\n",
    "            y_pred = self.inverse_transform_y(y_pred).add_suffix('_pred')\n",
    "            df = pd.concat([X, y, y_pred], axis=1)\n",
    "        else:\n",
    "            df = pd.concat([X, y], axis=1)\n",
    "        if cum is not None:\n",
    "            df = pd.concat([cum, df])\n",
    "        df = self._ptdataset(df)\n",
    "        return df\n",
    "        \n",
    "    def plot_boundary(self, predict):\n",
    "        self.evaluate().plot_boundary(predict)\n",
    "\n",
    "class PTSet:\n",
    "    def balance(self, weights=True):\n",
    "        r = copy.copy(self)\n",
    "        r._pt_balance = weights\n",
    "        return r    \n",
    "  \n",
    "    def scale(self, columns=True, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        if self._pt_polynomials and columns != 'x_only':\n",
    "            assert type(columns) != list or len(columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_scale_columns = columns\n",
    "        r._pt_scalertype = scalertype\n",
    "        r._pt_scale_omit_interval = omit_interval\n",
    "        return r\n",
    "    \n",
    "    def scalex(self, scalertype=StandardScaler, omit_interval=(-2,2)):\n",
    "        return self.scale(columns='x_only')\n",
    "    \n",
    "    def add_bias(self):\n",
    "        r = copy.copy(self)\n",
    "        r._pt_bias = True\n",
    "        return r\n",
    "    \n",
    "    def split(self, split=0.2, shuffle=True, random_state=None):\n",
    "        r = copy.copy(self)\n",
    "        r._pt_split = split\n",
    "        r._pt_shuffle = shuffle\n",
    "        r._pt_random_state = random_state\n",
    "        return r\n",
    "        \n",
    "    def polynomials(self, degree, include_bias=False):\n",
    "        assert type(self._pt_scale_columns) != list or len(self._pt_scale_columns) == 0, 'You cannot combine polynomials with column specific scaling'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_polynomials = PolynomialFeatures(degree, include_bias=include_bias)\n",
    "        return r\n",
    "    \n",
    "    def no_columny(self):\n",
    "        r = copy.deepcopy(self)\n",
    "        self._pt_columny = [self._pt_columnx[0]]\n",
    "        return r\n",
    "    \n",
    "    def columny(self, columns=None, transpose=None):\n",
    "        \"\"\"\n",
    "        By default, PipeTorch uses the last column as the target variable and transposes it to become a row vector.\n",
    "        This function can alter this default behavior. Transposing y is the default for single variable targets, \n",
    "        since most loss functions and metrics cannot handle column vectors. The set target variables are \n",
    "        automatically excluded from the input X.\n",
    "        columns: single column name or list of columns that is to be used as target column. \n",
    "        None means use the last column\n",
    "        transpose: True/False whether to transpose y. This is the default for single variable targets, since\n",
    "        most loss functions and metrics expect a row vector. When a list of columns is used as a target\n",
    "        transpose always has to be False.\n",
    "        return: dataframe for which the given columns are marked as target columns, and marks whether the \n",
    "        target variable is to be transposed.\n",
    "        \"\"\"\n",
    "        r = copy.deepcopy(self)\n",
    "        if columns is not None:\n",
    "            r._pt_columny = [columns] if type(columns) == str else columns\n",
    "        if r._pt_columny is not None and len(r._pt_columny) > 1:\n",
    "            transpose = False\n",
    "        if transpose is not None:\n",
    "            r._pt_transposey = transpose\n",
    "        return r\n",
    "\n",
    "    def columnx(self, *columns, omit=False):\n",
    "        \"\"\"\n",
    "        Specify which columns to use as input. The target variable is always excluded\n",
    "        so if you want to add that (for sequence learning), you should copy the\n",
    "        target column. \n",
    "        omit: when True, all columns are used except the specified columns\n",
    "        return: a new PipeTorch DataFrame with the given input specified.\n",
    "        \"\"\"\n",
    "        r = copy.deepcopy(self)\n",
    "        if omit:\n",
    "            r._pt_columnx = [ c for c in self.columns if c not in columns ]\n",
    "        else:\n",
    "            r._pt_columnx = list(columns) if len(columns) > 0 else None\n",
    "        return r\n",
    "    \n",
    "    def category(self, *columns, sort=False):\n",
    "        \"\"\"\n",
    "        Converts the values in the targetted columns into indices, for example to use in lookup tables.\n",
    "        columns that are categorized are excluded from scaling. You cannot use this function together\n",
    "        with polynomials or bias.\n",
    "        \n",
    "        Note: PipeTorch only uses categories that are in the training set and uses category 0 as\n",
    "        an unknown category number for categories in the validation and test set that are not known during training.\n",
    "        This way, no future information is used. \n",
    "        \n",
    "        columns: list of columns that is to be converted into a category\n",
    "        sort: True/False (default False) whether the unique values of these colums should be converted to indices in sorted order.\n",
    "        return: dataframe where the columns are converted into categories, \n",
    "        for which every unique value is converted into a unique index starting from 0\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self._pt_polynomials is None, 'You cannot combine categories with polynomials'\n",
    "        assert self._pt_bias is None, 'You cannot combine categories with polynomials'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_category = columns\n",
    "        r._pt_category_sort = sort\n",
    "        return r\n",
    "    \n",
    "    def dummies(self, *columns):\n",
    "        \"\"\"\n",
    "        Converts the values in the targetted columns into dummy variables.\n",
    "        columns that are categorized are excluded from scaling. You cannot use this function together\n",
    "        with polynomials or bias.\n",
    "        \n",
    "        Note: PipeTorch only uses categories that are in the training set and uses category 0 as\n",
    "        an unknown category number for categories in the validation and test set that are not known during training.\n",
    "        This way, no future information is used. \n",
    "        \n",
    "        columns: list of columns that is to be converted into a category\n",
    "        sort: True/False (default False) whether the unique values of these colums should be converted to indices in sorted order.\n",
    "        return: dataframe where the columns are converted into categories, \n",
    "        for which every unique value is converted into a unique index starting from 0\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self._pt_polynomials is None, 'You cannot combine categories with polynomials'\n",
    "        assert self._pt_bias is None, 'You cannot combine categories with polynomials'\n",
    "        r = copy.copy(self)\n",
    "        r._pt_dummies = columns\n",
    "        return r\n",
    "\n",
    "    def sequence(self, window, shift_y = 1):\n",
    "        r = copy.copy(self)\n",
    "        r._pt_sequence_window = window\n",
    "        r._pt_sequence_shift_y = shift_y\n",
    "        return r\n",
    "    \n",
    "class PTDataFrame(pd.DataFrame, PT, PTSet):\n",
    "    _metadata = PT._metadata\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        PT.__init__(self, data)\n",
    "        \n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTDataFrame\n",
    "    \n",
    "    def groupby(self, by, axis=0, level=None, as_index=True, sort=True, group_keys=True, observed=False, dropna=True):\n",
    "        r = super().groupby(by, axis=axis, level=level, as_index=as_index, sort=sort, group_keys=group_keys, observed=observed, dropna=dropna)\n",
    "        return self._copy_meta( PTGroupedDataFrame(r) )\n",
    "\n",
    "class PTLockedDataFrame(pd.DataFrame, PT):\n",
    "    _internal_names = ['_pt__scale_columns', '_pt__train', '_pt__valid', '_pt__test', '_pt__full', '_pt__scalerx', '_pt__scalery', '_pt__train_x', '_pt__train_y', '_pt__valid_x', '_pt__valid_y', '_pt__categoryx', '_pt__categoryy', '_pt__dummiesx', '_pt__dummiesy', '_pt__train_indices', '_pt__valid_indices', '_pt__test_indices']\n",
    "    _metadata = PT._metadata + _internal_names\n",
    "\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(data, **kwargs)\n",
    "        for m in self._metadata:\n",
    "            try:\n",
    "                self.__setattr__(m, getattr(data, m))\n",
    "            except: pass\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTLockedDataFrame\n",
    "    \n",
    "    @property\n",
    "    def _scalerx(self):\n",
    "        try:\n",
    "            if self._pt__scalerx is not None:\n",
    "                return self._pt__scalerx\n",
    "        except: pass\n",
    "        self._pt__scalerx = super()._scalerx\n",
    "        return self._pt__scalerx\n",
    "        \n",
    "    @property\n",
    "    def _scalery(self):\n",
    "        try:\n",
    "            if self._pt__scalery is not None:\n",
    "                return self._pt__scalery\n",
    "        except: pass\n",
    "        self._pt__scalery = super()._scalery\n",
    "        return self._pt__scaler\n",
    "    \n",
    "    @property\n",
    "    def _categoryx(self):\n",
    "        try:\n",
    "            if self._pt__categoryx is not None:\n",
    "                return self._pt__categoryx\n",
    "        except: pass\n",
    "        self._pt__categoryx = super()._categoryx\n",
    "        return self._pt__categoryx            \n",
    "    \n",
    "    @property\n",
    "    def _categoryy(self):\n",
    "        try:\n",
    "            if self._pt__categoryy is not None:\n",
    "                return self._pt__categoryy\n",
    "        except: pass\n",
    "        self._pt__categoryy = super()._categoryy\n",
    "        return self._pt__categoryy            \n",
    "\n",
    "    @property\n",
    "    def _dummiesy(self):\n",
    "        try:\n",
    "            if self._pt__dummiesy is not None:\n",
    "                return self._pt__dummiesy\n",
    "        except: pass\n",
    "        self._pt__dummiesy = super()._dummiesy\n",
    "        return self._pt__dummiesy            \n",
    "\n",
    "    @property\n",
    "    def _dummiesx(self):\n",
    "        try:\n",
    "            if self._pt__dummiesx is not None:\n",
    "                return self._pt__dummiesx\n",
    "        except: pass\n",
    "        self._pt__dummiesx = super()._dummiesx\n",
    "        return self._pt__dummiesx            \n",
    "    \n",
    "    @property\n",
    "    def full(self):\n",
    "        try:\n",
    "            return self._pt__full\n",
    "        except:\n",
    "            self._pt__full = super().full\n",
    "            return self._pt__full\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        try:\n",
    "            return self._pt__train\n",
    "        except:\n",
    "            self._pt__train = super().train\n",
    "            return self._pt__train\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        try:\n",
    "            return self._pt__valid\n",
    "        except:\n",
    "            self._pt__valid = super().valid\n",
    "            return self._pt__valid\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        try:\n",
    "            return self._pt__test\n",
    "        except:\n",
    "            self._pt__test = super().test\n",
    "            return self._pt__test\n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        try:\n",
    "            return self._pt__train_x\n",
    "        except:\n",
    "            self._pt__train_x = self.train.X\n",
    "            return self._pt__train_x\n",
    "            \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        try:\n",
    "            return self._pt__train_y\n",
    "        except:\n",
    "            self._pt__train_y = self.train.y\n",
    "        return self._pt__train_y\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        try:\n",
    "            return self._pt__valid_x\n",
    "        except:\n",
    "            self._pt__valid_x = self.valid.X\n",
    "        return self._pt__valid_x\n",
    "       \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        try:\n",
    "            return self._pt__valid_y\n",
    "        except:\n",
    "            self._pt__valid_y = self.valid.y\n",
    "        return self._pt__valid_y\n",
    "        \n",
    "class PTSeries(pd.Series, PT):\n",
    "    _metadata = PT._metadata\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        PT.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return PTDataFrame\n",
    "    \n",
    "class PTGroupedSeries(SeriesGroupBy, PT):\n",
    "    _metadata = PT._metadata\n",
    "\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        PT.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTGroupedSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return PTGroupedDataFrame\n",
    "    \n",
    "    \n",
    "class PTGroupedDataFrame(DataFrameGroupBy, PT):\n",
    "    _metadata = PT._metadata\n",
    "\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__(obj=data.obj, keys=data.keys, axis=data.axis, level=data.level, grouper=data.grouper, exclusions=data.exclusions,\n",
    "                selection=data._selection, as_index=data.as_index, sort=data.sort, group_keys=data.group_keys,\n",
    "                observed=data.observed, mutated=data.mutated, dropna=data.dropna)\n",
    "        PT.__init__(self, data)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return PTGroupedDataFrame\n",
    "    \n",
    "    @property\n",
    "    def _constructor_sliced(self):\n",
    "        return PTGroupedSeries\n",
    "    \n",
    "    def astype(self, dtype, copy=True, errors='raise'):\n",
    "        PTDataFrame.astype(self, dtype, copy=copy, errors=errors)\n",
    "\n",
    "    def get_group(self, name, obj=None):\n",
    "        return self._ptdataframe( super().get_group(name, obj=obj) )\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for group, subset in super().__iter__():\n",
    "            yield group, self._copy_meta(subset)\n",
    "        \n",
    "    def to_dataset(self):\n",
    "        from torch.utils.data import ConcatDataset\n",
    "        dss = []\n",
    "        for key, group in self:\n",
    "            dss.append( self._ptdataframe(group).to_dataset())\n",
    "\n",
    "        return [ConcatDataset(ds) for ds in zip(*dss)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655beac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
