{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4928407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting databunch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile databunch.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, ConcatDataset\n",
    "from torchvision.datasets import VisionDataset\n",
    "from .transformabledataset import TransformableDataset\n",
    "from ..evaluate.evaluate import Evaluator\n",
    "\n",
    "def traverse_dataset(ds):\n",
    "    if isinstance(ds, Subset):\n",
    "        return traverse_dataset(ds.dataset)\n",
    "    if isinstance(ds, TransformableDataset):\n",
    "        return traverse_dataset(ds.dataset)\n",
    "    if isinstance(ds, TensorDataset):\n",
    "        return ds.tensors\n",
    "    if isinstance(ds, VisionDataset):\n",
    "        try:\n",
    "            return ds.data, ds.targets\n",
    "        except:\n",
    "            raise ValueError('Cannot traverse a VisionDataset')\n",
    "    raise ValueError('Cannot traverse this type of Dataset')\n",
    "\n",
    "class Databunch:\n",
    "    \"\"\"\n",
    "    Following the idea of databunches from the Fast.ai library, \n",
    "    a Databunch is a convenient wrapper for a train and valid DataLoader in one object.\n",
    "    This makes it less redundant to configure the dataloaders and ensures that the dataloaders for\n",
    "    DFrame are always paired.\n",
    "    \n",
    "    Args:\n",
    "        df: DFrame\n",
    "            the source\n",
    "        \n",
    "        train_ds: DataSet\n",
    "            A PyTorch DataSet for the train part\n",
    "            \n",
    "        test_ds: DataSet\n",
    "            A PyTorch DataSet for the test part\n",
    "            \n",
    "        valid_ds: DataSet\n",
    "            A PyTorch DataSet for the valid part, when None, test_ds is used for validation\n",
    "            \n",
    "        batch_size: int (32)\n",
    "            the batch size that is used to generate a PyTorch DataLoader for the train set\n",
    "            \n",
    "        test_batch_size: int (None)\n",
    "            the batch size that is used to generate a PyTorch DataLoader for the test and valid set.\n",
    "            A higher size generally improves the processing speed and does not affect training.\n",
    "            When None, batch_size is used.\n",
    "            \n",
    "        num_workers: int (2)\n",
    "            the number of CPU cores that are assigned by the DataLoader to prepare data\n",
    "            \n",
    "        shuffle: bool (True)\n",
    "            where the data is shuffled every iteration (in general, shuffling is a good idea)\n",
    "            \n",
    "        pin_memory: bool (False)\n",
    "            is passed to Dataloader, pinning memory can sometimes help to speed up data preparation by\n",
    "            keeping data in memory, however this only works when there is a sufficient amount of RAM and\n",
    "            may not work together with num_workers > 1.\n",
    "            \n",
    "        collate: callable (None)\n",
    "            a collate function that is used to prepare the mini-batches by the Dataloader\n",
    "\n",
    "        balance: dict (None)\n",
    "            the training data is balanced accoriding to the sampling probability assigned\n",
    "            to each y-label in the given dictionary. \n",
    "            e.g. {'M':0.2, 'F':0.8} would make datapoints with y='F' 4 times as likely to be chosen\n",
    "            as a training sample than datapoints with y='M'. This way, the data can be balanced.\n",
    "            If weights=True, the weights are chosen so that all labels are equally likely to be picked.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, train_ds, test_ds=None, valid_ds=None, batch_size=32, test_batch_size=None, \n",
    "                 num_workers=2, shuffle=True, pin_memory=False, collate=None, balance=None):\n",
    "        self.df = df\n",
    "        self.train_ds = train_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.valid_ds = valid_ds if valid_ds is not None else test_ds\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.pin_memory = pin_memory\n",
    "        self.collate = collate\n",
    "        self._balance = balance\n",
    "        \n",
    "    def _dataloader(self, ds, batch_size=32, shuffle=True, **kwargs):\n",
    "        if self.collate is not None:\n",
    "            kwargs['collate_fn'] = self.collate\n",
    "        return DataLoader(ds, batch_size=batch_size, num_workers=self.num_workers, shuffle=shuffle, pin_memory=self.pin_memory, **kwargs)\n",
    "        \n",
    "    def copy(self):\n",
    "        return Databunch(self.df, self.train_ds, self.test_ds, self.valid_ds, self.batch_size, self.test_batch_size,\n",
    "                   self.num_workers, self.shuffle, self.pin_memory, self.collate, self.sampler)\n",
    "        \n",
    "    @property\n",
    "    def train_dl(self):\n",
    "        try:\n",
    "            return self._train_dl\n",
    "        except:\n",
    "            kwargs = {}\n",
    "            if self._balance is not None and self._balance is not False:\n",
    "                target = self.train_ds.tensors[1].numpy().squeeze()\n",
    "                if self._balance == True:\n",
    "                    weights = {t:(1. / c) for t, c in zip(*np.unique(target, return_counts=True))}\n",
    "                else:\n",
    "                    weights = self._balance\n",
    "                samples_weight = np.array([weights[t] for t in target])\n",
    "                samples_weight = torch.from_numpy(samples_weight)\n",
    "                kwargs['sampler'] = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "                shuffle = False\n",
    "            else:\n",
    "                shuffle = self.shuffle\n",
    "            self._train_dl = self._dataloader(self.train_ds, batch_size=self.batch_size, shuffle=shuffle, **kwargs)\n",
    "            return self._train_dl\n",
    "\n",
    "    @property\n",
    "    def valid_dl(self):\n",
    "        try:\n",
    "            return self._valid_dl\n",
    "        except:\n",
    "            self._valid_dl = self._dataloader(self.valid_ds, batch_size=self.test_batch_size, shuffle=False)\n",
    "            return self._valid_dl\n",
    "\n",
    "    @property\n",
    "    def test_dl(self):\n",
    "        try:\n",
    "            return self._test_dl\n",
    "        except:\n",
    "            self._test_dl = self._dataloader(self.test_ds, batch_size=self.test_batch_size, shuffle=False)\n",
    "            return self._test_dl\n",
    "\n",
    "    @property\n",
    "    def folds(self):\n",
    "        return self.df._pt_folds\n",
    "        \n",
    "    def iter_folds(self):\n",
    "        if self.folds is None:\n",
    "            yield self.train_dl, self.test_dl, self.valid_dl\n",
    "        else:\n",
    "            for fold in range(self.folds):\n",
    "                yield self.fold(fold)\n",
    "\n",
    "    def fold(self, fold):\n",
    "        db = self.df.fold(fold).to_databunch(batch_size=self.batch_size, num_workers=self.num_workers, \n",
    "                                            shuffle=self.shuffle, pin_memory=self.pin_memory, balance=self._balance,\n",
    "                                            test_batch_size=self.test_batch_size)\n",
    "        return db.train_dl, db.test_dl, db.valid_dl\n",
    "                \n",
    "    def reset(self):\n",
    "        try:\n",
    "            del self._valid_dl\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._train_dl\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._test_dl\n",
    "        except: pass\n",
    "                \n",
    "    def balance(self, weights=True):\n",
    "        \"\"\"\n",
    "        Return a this Databunch, in which the training set is balanced according to the \n",
    "        given weights. See torch.utils.data.WeightedRandomSampler\n",
    "        \n",
    "        Args:\n",
    "            weights: dict label:fraction (True)\n",
    "                A dictionary that defines the likelyhood of a datapoint with that label to be picked.\n",
    "                e.g. {'M':0.2, 'F':0.8} would make datapoints with y='F' 4 times as likely to be chosen\n",
    "                as a training sample than datapoints with y='M'. This way, the data can be balanced.\n",
    "                If weights=True, the weights are chosen so that all labels are equally likely to be picked.\n",
    "                \n",
    "        Returns: self\n",
    "        \"\"\"\n",
    "        self._balance = weights\n",
    "        try:\n",
    "            del self._train_dl\n",
    "        except: pass\n",
    "        return self\n",
    "        \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "        \n",
    "    @batch_size.setter\n",
    "    def batch_size(self, value):\n",
    "        if value is not None:\n",
    "            self._batch_size = min(value, len(self.train_ds))\n",
    "            self.reset()\n",
    "\n",
    "    @property\n",
    "    def valid_batch_size(self):\n",
    "        try:\n",
    "            return min(self._valid_batch_size, len(self.valid_ds))\n",
    "        except:\n",
    "            return len(self.valid_ds)\n",
    "        \n",
    "    @property\n",
    "    def test_batch_size(self):\n",
    "        try:\n",
    "            return min(self._test_batch_size, len(self.valid_ds))\n",
    "        except:\n",
    "            return len(self.valid_ds)\n",
    "        \n",
    "    @test_batch_size.setter\n",
    "    def test_batch_size(self, value):\n",
    "        if value is not None:\n",
    "            self._test_batch_size = value\n",
    "            self._valid_batch_size = value\n",
    "            self.reset()\n",
    "\n",
    "    @property\n",
    "    def shuffle(self):\n",
    "        return self._shuffle\n",
    "        \n",
    "    @shuffle.setter\n",
    "    def shuffle(self, value):\n",
    "        if value is not None:\n",
    "            self._shuffle = value\n",
    "\n",
    "    @property\n",
    "    def collate(self):\n",
    "        try:\n",
    "            return self._collate\n",
    "        except: pass\n",
    "\n",
    "    @collate.setter\n",
    "    def collate(self, value):\n",
    "        if value is not None:\n",
    "            self._collate = value\n",
    "            \n",
    "    @property\n",
    "    def num_workers(self):\n",
    "        return self._num_workers\n",
    "\n",
    "    @num_workers.setter\n",
    "    def num_workers(self, value):\n",
    "        if value is not None:\n",
    "            self._num_workers = value\n",
    "            self.reset()\n",
    "    \n",
    "    def inverse_y(self, y):\n",
    "        return self.df.inverse_y(y)\n",
    "    \n",
    "    def inverse_scale_y(self, y):\n",
    "        return self.df.inverse_scale_y(y)\n",
    "    \n",
    "    def inverse_scale(self, X, y, y_pred, cum=None):\n",
    "        return self.df.inverse_scale(X, y, y_pred, cum=cum)\n",
    "    \n",
    "    def from_numpy(self, X):\n",
    "        X = self.df.from_numpy(X)\n",
    "    \n",
    "    def predict(self, model, dl, device=None):\n",
    "        import torch\n",
    "        model.eval()\n",
    "        prev = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(False)\n",
    "        if device is None:\n",
    "            try:\n",
    "                device = model.device\n",
    "            except: pass\n",
    "        df = None\n",
    "        for *X, y in dl:\n",
    "            if device is not None:\n",
    "                X = [ x.to(device) for x in X ]\n",
    "            y_pred = model(*X)\n",
    "            df = self.inverse_scale(*X, y, y_pred, df)\n",
    "        torch.set_grad_enabled(prev)\n",
    "        return df\n",
    "    \n",
    "    def predict_train(self, model, device=None):\n",
    "        return self.predict(model, self.train_dl, device=device)\n",
    "    \n",
    "    def predict_valid(self, model, device=None):\n",
    "        return self.predict(model, self.valid_dl, device=device)\n",
    "\n",
    "    def predict_test(self, model, device=None):\n",
    "        return self.predict(model, self.test_dl, device=device)\n",
    "\n",
    "    def sample(self, device=None, valid=False, test=False):\n",
    "        \"\"\"\n",
    "        returns a single batch from the DataLoader.\n",
    "        \n",
    "        Args:\n",
    "            device: torch.device (None)\n",
    "                transfers the data to the given device, e.g. db.sample(torch.device('cuda:0'))\n",
    "            \n",
    "            valid: bool (False)\n",
    "                retrieves a sample from the validation set\n",
    "                \n",
    "            test: bool (False)\n",
    "                retrieves a sample from the test set\n",
    "                \n",
    "        Returns: (tensor, tensor)\n",
    "            containing the input and output features for a retrieved batch from the dataset\n",
    "        \"\"\"\n",
    "        arrays = next(iter(self.train_dl))\n",
    "        if device is not None:\n",
    "            arrays = [ a.to(device) for a in arrays ]\n",
    "        return arrays\n",
    "\n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return traverse_dataset(self.train_ds)[0]\n",
    "\n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return traverse_dataset(self.valid_ds)[0]\n",
    "\n",
    "    @property\n",
    "    def test_X(self):\n",
    "        return traverse_dataset(self.test_ds)[0]\n",
    "\n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return traverse_dataset(self.train_ds)[-1]\n",
    "\n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return traverse_dataset(self.valid_ds)[-1]\n",
    "\n",
    "    @property\n",
    "    def test_y(self):\n",
    "        return traverse_dataset(self.test_ds)[-1]\n",
    "    \n",
    "    def to_evaluator(self, *metrics):\n",
    "        return Evaluator(self.df, *metrics)\n",
    "    \n",
    "    def df_to_dset(self, df):\n",
    "        \"\"\"\n",
    "        Converts a DataFrame to a DSet that has the pipeline as this DataBunch.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame\n",
    "        \n",
    "        Returns: DSet\n",
    "        \"\"\"\n",
    "        return self.df.df_to_dset(df)\n",
    "\n",
    "    def df_to_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Converts the given df to a DataSet using the pipeline of this DataBunch.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame or DFrame\n",
    "                to convert into a DataSet\n",
    "        \n",
    "        returns: DataSet.\n",
    "        \"\"\"\n",
    "        return self.df.df_to_dset(df).to_dataset()\n",
    "    \n",
    "    def df_to_dataloader(self, df):\n",
    "        \"\"\"\n",
    "        Converts the given df to a DataLoader using the pipeline of this DataBunch.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame or DFrame\n",
    "                to convert into a DataSet\n",
    "        \n",
    "        returns: DataSet.\n",
    "        \"\"\"\n",
    "        return self._dataloader(self.df_to_dataset(df))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa079a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdafa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
