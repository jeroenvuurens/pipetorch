{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209951df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dset.py        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from ..data.transformabledataset import TransformableDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.values\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "class _DSet:\n",
    "    _metadata = ['_df', '_pt_categoryx', '_pt_categoryy', \n",
    "                 '_pt_dummiesx', '_pt_dummiesy',\n",
    "                 '_pt_scalerx', '_pt_scalery', \n",
    "                 '_pt_columntransformerx', '_pt_columntransformery',\n",
    "                 '_pt_columny', '_pt_columnx', \n",
    "                 '_pt_vectory', '_pt_bias', '_pt_polynomials', \n",
    "                 '_pt_dtype', \n",
    "                 '_pt_sequence_window', '_pt_sequence_shift_y', '_pt_is_test', \n",
    "                 '_pt_datasetclass', '_pt_dataset_transforms', '_pt_filterna']\n",
    "    _internal_names = pd.DataFrame._internal_names + [\"_pt__indices\", \"_pt__x_sequence\"]\n",
    "    _internal_names_set = set(_internal_names)\n",
    "    \n",
    "    def __init__(self, data, **kwargs):\n",
    "        for m in self._metadata:\n",
    "            self.__setattr__(m, None)\n",
    "            \n",
    "    def to_dframe(self):\n",
    "        cls = self._df.__class__\n",
    "        r = cls(self)\n",
    "\n",
    "        r._pt_columnx = self._pt_columnx\n",
    "        r._pt_columny = self._pt_columny\n",
    "        r._pt_vectory = self._pt_vectory\n",
    "        r._pt_bias = self._pt_bias\n",
    "        r._pt_polynomials = self._pt_polynomials\n",
    "        r._pt_sequence_window = self._pt_sequence_window\n",
    "        r._pt_sequence_shift_y = self._pt_sequence_shift_y\n",
    "        r._pt_filterna = self._pt_filterna\n",
    "        r._pt_dataset_transforms = self._pt_dataset_transforms\n",
    "        \n",
    "        r._pt__train = self\n",
    "        r._pt__valid = None\n",
    "        r._pt__test = None\n",
    "        r._categoryx = self._categoryx\n",
    "        r._categoryy = self._categoryy\n",
    "        r._dummiesx = self._dummiesx\n",
    "        r._dummiesy = self._dummiesy\n",
    "        r._scalerx = self._scalerx\n",
    "        r._scalery = self._scalery\n",
    "        r._pt_split = None\n",
    "        r._pt_random_state = None\n",
    "        r._pt_balance = None\n",
    "        r._pt_shuffle = False\n",
    "        return r\n",
    "\n",
    "    def _copy_meta(self, r):\n",
    "        r._df = self._df\n",
    "        r._pt_categoryx = self._categoryx\n",
    "        r._pt_categoryy = self._categoryy\n",
    "        r._pt_dummiesx = self._dummiesx\n",
    "        r._pt_dummiesy = self._dummiesy\n",
    "        r._pt_scalerx = self._scalerx\n",
    "        r._pt_scalery = self._scalery\n",
    "        r._pt_columny = self._pt_columny\n",
    "        r._pt_columnx = self._pt_columnx\n",
    "        r._pt_columntransformery = self._columntransformery\n",
    "        r._pt_columntransformerx = self._columntransformerx\n",
    "        r._pt_is_test = self._pt_is_test\n",
    "        r._pt_vectory = self._pt_vectory\n",
    "        r._pt_polynomials = self._pt_polynomials\n",
    "        r._pt_dataset_transforms = self._pt_dataset_transforms\n",
    "        r._pt_datasetclass = self._pt_datasetclass\n",
    "        r._pt_bias = self._pt_bias\n",
    "        r._pt_dtype = self._pt_dtype\n",
    "        r._pt_sequence_window = self._pt_sequence_window\n",
    "        r._pt_sequence_shift_y = self._pt_sequence_shift_y\n",
    "        r._pt_filterna = self._pt_filterna\n",
    "        return r\n",
    "    \n",
    "    def _dset(self, data):\n",
    "        return self._copy_meta( DSet(data) )\n",
    "    \n",
    "    def df_to_dset(self, df):\n",
    "        \"\"\"\n",
    "        Converts a DataFrame to a DSet that has the pipeline as this DSet.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame\n",
    "        \n",
    "        Returns: DSet\n",
    "        \"\"\"\n",
    "        return self._df.df_to_dset(df)\n",
    "\n",
    "    def df_to_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Converts the given df to a DataSet using the pipeline of this DSet.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame or DFrame\n",
    "                to convert into a DataSet\n",
    "        \n",
    "        returns: DataSet.\n",
    "        \"\"\"\n",
    "        return self.df_to_dset(df).to_dataset(self._pt_datasetclass)    \n",
    "\n",
    "    @property\n",
    "    def _dtype(self):\n",
    "        return self._pt_dtype\n",
    "    \n",
    "    def _not_nan_y_transposed(self):\n",
    "        y = self._y_transposed\n",
    "        if len(y.shape) == 1:\n",
    "            mask = np.isnan(self._y_transposed)\n",
    "        else:\n",
    "            mask = np.any(np.isnan(self._y_transposed))\n",
    "        return ~mask\n",
    "\n",
    "    def _not_nan_x_sequence(self):\n",
    "        mask = np.any(np.isnan(self._x_sequence), axis=1)\n",
    "        return ~mask\n",
    "\n",
    "    def _not_nan_x_numpy(self):\n",
    "        mask = np.any(np.isnan(self._x_numpy), axis=1)\n",
    "        return ~mask\n",
    "    \n",
    "    @property\n",
    "    def indices(self):\n",
    "        try:\n",
    "            return self._pt__indices\n",
    "        except:\n",
    "            if self._pt_filterna:\n",
    "                if self._pt_is_test:\n",
    "                    if self._is_sequence:\n",
    "                        self._pt__indices = self._not_nan_x_sequence()\n",
    "                    else:\n",
    "                        self._pt__indices = self._not_nan_x_numpy()\n",
    "                else:\n",
    "                    if self._is_sequence:\n",
    "                        self._pt__indices = self._not_nan_y_transposed() & self._not_nan_x_sequence()\n",
    "                    else:\n",
    "                        self._pt__indices = self._not_nan_y_transposed() & self._not_nan_x_numpy()\n",
    "            else:\n",
    "                self._pt__indices = [ True ] * len(self._y_transposed)\n",
    "            return self._pt__indices\n",
    "        \n",
    "    @property\n",
    "    def length(self):\n",
    "        return sum(self.indices)\n",
    "    \n",
    "    @property\n",
    "    def _shift_y(self):\n",
    "        if self._pt_sequence_shift_y is not None:\n",
    "            return self._pt_sequence_shift_y\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @property\n",
    "    def _sequence_window(self):\n",
    "        try:\n",
    "            if self._is_sequence:\n",
    "                return self._pt_sequence_window\n",
    "        except:pass\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def _start_index_y(self):\n",
    "        if self._is_sequence:\n",
    "            return self._pt_sequence_window+self._shift_y-1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    @property\n",
    "    def _columny(self):\n",
    "        return [ self.columns[-1] ] if self._pt_columny is None else self._pt_columny\n",
    "        \n",
    "    @property\n",
    "    def _vectory(self):\n",
    "        return True if self._pt_vectory is None else self._pt_vectory\n",
    "            \n",
    "    @property\n",
    "    def _columnx(self):\n",
    "        if self._pt_columnx is None:\n",
    "            return [ c for c in self.columns if c not in self._columny ]\n",
    "        return self._pt_columnx\n",
    "   \n",
    "    @property\n",
    "    def _scalerx(self):\n",
    "        if self._pt_scalerx is not None:\n",
    "            return self._pt_scalerx\n",
    "        self._pt_scalerx = self._df._scalerx\n",
    "        return self._pt_scalerx\n",
    "\n",
    "    @_scalerx.setter\n",
    "    def _scalerx(self, value):\n",
    "        self._pt_scalerx = value\n",
    "\n",
    "    @property\n",
    "    def _scalery(self):\n",
    "        if self._pt_scalery is not None:\n",
    "            return self._pt_scalery\n",
    "        self._pt_scalery = self._df._scalery\n",
    "        return self._pt_scalery\n",
    "\n",
    "    @_scalery.setter\n",
    "    def _scalery(self, value):\n",
    "        self._pt_scalery = value\n",
    "\n",
    "    @property\n",
    "    def _categoryx(self):\n",
    "        if self._pt_categoryx is not None:\n",
    "            return self._pt_categoryx\n",
    "        self._pt_categoryx = self._df._categoryx()\n",
    "        return self._pt_categoryx\n",
    "\n",
    "    @_categoryx.setter\n",
    "    def _categoryx(self, value):\n",
    "        self._pt_categoryx = value\n",
    "\n",
    "    @property\n",
    "    def _categoryy(self):\n",
    "        if self._pt_categoryy is not None:\n",
    "            return self._pt_categoryy\n",
    "        self._pt_categoryy = self._df._categoryy()\n",
    "        return self._pt_categoryy\n",
    "\n",
    "    @_categoryy.setter\n",
    "    def _categoryy(self, value):\n",
    "        self._pt_categoryy = value\n",
    "\n",
    "    @property\n",
    "    def _columntransformerx(self):\n",
    "        if self._pt_columntransformerx is not None:\n",
    "            return self._pt_columntransformerx\n",
    "        self._pt_columntransformerx = self._df._columntransformerx()\n",
    "        return self._pt_columntransformerx\n",
    "\n",
    "    @_columntransformerx.setter\n",
    "    def _columntransformerx(self, value):\n",
    "        self._pt_columntransformerx = value\n",
    "\n",
    "    @property\n",
    "    def _columntransformery(self):\n",
    "        if self._pt_columntransformery is not None:\n",
    "            return self._pt_columntransformery\n",
    "        self._pt_columntransformery = self._df._columntransformery()\n",
    "        return self._pt_columntransformery\n",
    "\n",
    "    @_columntransformery.setter\n",
    "    def _columntransformery(self, value):\n",
    "        self._pt_columntransformery = value\n",
    "\n",
    "    @property\n",
    "    def _dummiesx(self):\n",
    "        if self._pt_dummiesx is not None:\n",
    "            return self._pt_dummiesx\n",
    "        self._pt_dummiesx = self._df._dummiesx()\n",
    "        return self._pt_dummiesx\n",
    "\n",
    "    @_dummiesx.setter\n",
    "    def _dummiesx(self, value):\n",
    "        self._pt_dummiesx = value\n",
    "\n",
    "    @property\n",
    "    def _dummiesy(self):\n",
    "        if self._pt_dummiesy is not None:\n",
    "            return self._pt_dummiesy\n",
    "        self._pt_dummiesy = self._df._dummiesy()\n",
    "        return self._pt_dummiesy\n",
    "\n",
    "    @_dummiesy.setter\n",
    "    def _dummiesy(self, value):\n",
    "        self._pt_dummiesy = value\n",
    "\n",
    "    @property\n",
    "    def _polynomials(self):\n",
    "        return self._pt_polynomials\n",
    "       \n",
    "    @property\n",
    "    def _bias(self):\n",
    "        return self._pt_bias\n",
    "    \n",
    "    def _transform(self, scalers, array):\n",
    "        out = []\n",
    "        for i, scaler in enumerate(scalers):\n",
    "            if scaler is not None:\n",
    "                out.append(scaler.transform(array[:, i:i+1]))\n",
    "            else:\n",
    "                out.append(array[:, i:i+1])\n",
    "        return np.concatenate(out, axis=1)\n",
    "\n",
    "    def resample_rows(self, n=True):\n",
    "        \"\"\"\n",
    "        Resamples this DSet with replacement to support bootstrapping.\n",
    "        \n",
    "        Arguments:\n",
    "            n: int (True)\n",
    "                The number of samples to take, or the size of the dataset if n equals True.\n",
    "        \n",
    "        Returns: DSet\n",
    "            A resampled DSet\n",
    "        \"\"\"\n",
    "        r = self._dset(self)\n",
    "        if n == True:\n",
    "            n = len(r)\n",
    "        if n < 1:\n",
    "            n = n * len(r)\n",
    "        return r.iloc[resample(list(range(len(r))), n_samples = int(n))]\n",
    "    \n",
    "    def interpolate_factor(self, factor=2, sortcolumn=None):\n",
    "        \"\"\"\n",
    "        Interpolates the data between the rows. This function was created to simplify drawing\n",
    "        of higher order polynomial functions, and may be useful for other situations, but it is\n",
    "        limited to simple interplation between two consecutive points.\n",
    "        \n",
    "        Arguments:\n",
    "            factor: int (2)\n",
    "                interpolates by ading 2 ** factor - 1 values. In other words, a factor of 1 interpolates\n",
    "                1 value betweem every 2 existing values and a factor of 2 will interpolate 3 values between \n",
    "                every two values.\n",
    "            sortcolumn: str (None)\n",
    "                Before interpolating, the DSet is first sorted by this column, or the first column if None is\n",
    "                provided.\n",
    "                \n",
    "        Returns: DSet\n",
    "            In which the values are interpolated.\n",
    "        \"\"\"\n",
    "        if not sortcolumn:\n",
    "            sortcolumn = self.columns[0]\n",
    "        df = self.sort_values(by=sortcolumn)\n",
    "        for i in range(factor):\n",
    "            i = df.rolling(2).sum()[1:] / 2.0\n",
    "            df = pd.concat([df, i], axis=0)\n",
    "            df = df.sort_values(by=sortcolumn)\n",
    "        return self._df._dset(df).reset_index(drop=True)\n",
    "    \n",
    "    @property\n",
    "    def _x_transformed(self):\n",
    "        if self._is_sequence:\n",
    "            self = self.iloc[:-self._shift_y]\n",
    "        if self._columntransformerx is None:\n",
    "            return self[self._columnx]\n",
    "        r = copy.copy(self[self._columnx])\n",
    "        for c, t in zip(r._columnx, r._columntransformerx):\n",
    "            if t is not None:\n",
    "                r[c] = t.transform(r[c])\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _x_category(self):\n",
    "        if self._categoryx is None:\n",
    "            return self._x_transformed\n",
    "        r = copy.copy(self._x_transformed)\n",
    "        for c, cat in zip(r._columnx, r._categoryx):\n",
    "            if cat is not None:\n",
    "                r[c] = cat.transform(r[c])\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _x_dummies(self):\n",
    "        if self._dummiesx is None:\n",
    "            return self._x_category\n",
    "        r = copy.copy(self._x_category)\n",
    "        r1 = []\n",
    "        for d, onehot in zip(r._columnx, r._dummiesx):\n",
    "            if onehot is not None:\n",
    "                a = onehot.transform(r[[d]])\n",
    "                r1.append( pd.DataFrame(a.toarray(), columns=onehot.get_feature_names_out([d])) )\n",
    "                r = r.drop(columns = d)\n",
    "        r1.insert(0, r.reset_index(drop=True))\n",
    "        r = pd.concat(r1, axis=1)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _x_numpy(self):\n",
    "        return self._x_dummies.to_numpy()\n",
    "    \n",
    "    @property\n",
    "    def _x_polynomials(self):\n",
    "        try:\n",
    "            return self._polynomials.fit_transform(self._x_numpy)\n",
    "        except:\n",
    "            return self._x_numpy\n",
    "\n",
    "    @property\n",
    "    def _x_scaled(self):\n",
    "        if len(self) > 0:\n",
    "            return self._transform(self._scalerx, self._x_polynomials)\n",
    "        return self._x_polynomials\n",
    "            \n",
    "    @property\n",
    "    def _x_biased(self):\n",
    "        a = self._x_scaled\n",
    "        if self._bias:\n",
    "            return np.concatenate([np.ones((len(a),1)), a], axis=1)\n",
    "        return a\n",
    "    \n",
    "    @property\n",
    "    def _x_sequence(self):\n",
    "        try:\n",
    "            return self._pt__x_sequence\n",
    "        except:\n",
    "            if not self._is_sequence:\n",
    "                self._pt__x_sequence = self._x_biased\n",
    "            else:\n",
    "                X = self._x_biased\n",
    "                window = self._sequence_window\n",
    "                len_seq_mode = max(0, len(X) - window + 1)\n",
    "                self._pt__x_sequence =  np.concatenate([np.expand_dims(X[ii:ii+window], axis=0) for ii in range(len_seq_mode)], axis=0)        \n",
    "            return self._pt__x_sequence\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"\n",
    "        Constructs a Numpy array in which the values for X are transformed according to the configured pipeline.\n",
    "        \n",
    "        Returns: Numpy array\n",
    "        \"\"\"\n",
    "        return self._x_sequence[self.indices]\n",
    "\n",
    "    @property\n",
    "    def X_tensor(self):\n",
    "        \"\"\"\n",
    "        Constructs a PyTorch tensor in which the values for X are transformed according to the configured pipeline.\n",
    "        A difference with X() is that the data is transformed to a single datatype (by default torch.FloatTensor).\n",
    "        \n",
    "        Returns: PyTorch tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        import torch\n",
    "        if self._pt_dtype is None:\n",
    "            return torch.tensor(self.X).type(torch.FloatTensor)\n",
    "        if self._pt_dtype and np.issubdtype(self._pt_dtype, np.number):\n",
    "            return torch.tensor(self.X.astype(self._pt_dtype))\n",
    "        return torch.tensor(self.X)\n",
    "\n",
    "    @property\n",
    "    def y_tensor(self):\n",
    "        \"\"\"\n",
    "        Constructs a PyTorch tensor in which the values for y are transformed according to the configured pipeline.\n",
    "        A difference with y() is that the data is transformed to a single datatype (by default torch.FloatTensor).\n",
    "        \n",
    "        Returns: PyTorch tensor\n",
    "        \"\"\"\n",
    "\n",
    "        import torch\n",
    "        y = self._y_scaled[self.indices] if self._pt_vectory is None else self._y\n",
    "        if self._pt_dtype is None:\n",
    "            return torch.tensor(y).type(torch.FloatTensor)\n",
    "        if self._pt_dtype and np.issubdtype(self._pt_dtype, np.number):\n",
    "            return torch.tensor(y.astype(self._pt_dtype))\n",
    "        else:\n",
    "            return torch.tensor(y)\n",
    " \n",
    "    @property\n",
    "    def _is_sequence(self):\n",
    "        return self._pt_sequence_window is not None\n",
    "        \n",
    "    @property\n",
    "    def tensors(self):\n",
    "        \"\"\"\n",
    "        Combines X_tensor() and y_tensor()\n",
    "        \n",
    "        Returns: tensor, tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return self.X_tensor, self.y_tensor\n",
    "            \n",
    "    @property\n",
    "    def _range_y(self):\n",
    "        stop = len(self) if self._is_sequence and self._shift_y >= 0 else len(self) + self._shift_y\n",
    "        start = min(stop, self._start_index_y)\n",
    "        return slice(start, stop)\n",
    "\n",
    "    @property\n",
    "    def _y_transformed(self):\n",
    "        if self._is_sequence:\n",
    "            self = self.iloc[self._range_y]\n",
    "        if self._categoryy is None:\n",
    "            return self[self._columny]\n",
    "        if self._columntransformery is None:\n",
    "            return self[self._columny]\n",
    "        r = copy.copy(self[self._columny])\n",
    "        for c, t in zip(r._columny, r._columntransformery):\n",
    "            if t is not None:\n",
    "                r[c] = t.transform(r[c])\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _y_category(self):\n",
    "        if self._categoryy is None:\n",
    "            return self._y_transformed\n",
    "        r = copy.copy(self._y_transformed)\n",
    "        for c, cat in zip(r._columny, r._categoryy):\n",
    "            if cat is not None:\n",
    "                r[c] = cat.transform(r[c])\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _y_dummies(self):\n",
    "        if self._dummiesy is None:\n",
    "            return self._y_category\n",
    "        r = copy.copy(self._y_category)\n",
    "        r1 = []\n",
    "        for d, onehot in zip(r._columny, r._dummiesy):\n",
    "            if onehot is not None:\n",
    "                a = onehot.transform(r[[d]])\n",
    "                r1.append( pd.DataFrame(a.toarray(), columns=onehot.get_feature_names_out([d])) )\n",
    "                r = r.drop(columns = d)\n",
    "        r1.insert(0, r.reset_index(drop=True))\n",
    "        r = pd.concat(r1, axis=1)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _y_numpy(self):\n",
    "        return self._y_dummies.to_numpy()\n",
    "    \n",
    "    @property\n",
    "    def _y_scaled(self):\n",
    "        if len(self) > 0:\n",
    "            return self._transform(self._scalery, self._y_numpy)\n",
    "        return self._y_numpy\n",
    "            \n",
    "    @property\n",
    "    def _y_transposed(self):\n",
    "        return self._y_scaled.squeeze(axis=1) if self._vectory else self._y_scaled\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        \"\"\"\n",
    "        Constructs a Numpy array in which the values for y are transformed according to the configured pipeline.\n",
    "        \n",
    "        Returns: Numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._y_transposed[self.indices]\n",
    "        \n",
    "    def replace_y(self, new_y):\n",
    "        \"\"\"\n",
    "        Returns a copy of this DSet, in which y is replaced with alternative values.\n",
    "        \n",
    "        Arguments:\n",
    "            new_y: Numpy array, PyTorch tensor or callable\n",
    "                If new_y is a callable, it will be called with this DSet's X to generate predicted values of y\n",
    "        \n",
    "        Returns: DSet\n",
    "            A copy of this DSet, in which the values for y are replaced by the given values new_y\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self._predict(new_y)\n",
    "        indices = self.indices\n",
    "        offset = self._range_y.start\n",
    "        if offset > 0:\n",
    "            indices = [False] * offset + indices[:len(self) - offset]\n",
    "        assert len(y_pred) == sum(indices), f'The number of predictions ({len(y_pred)}) does not match the number of samples ({len(indices)})'\n",
    "        r = copy.deepcopy(self)\n",
    "        columns = [r.columns.get_loc(c) for c in self._columny]\n",
    "        #r.iloc[indices, columns] = np.NaN\n",
    "        r.iloc[indices, columns] = to_numpy(y_pred)\n",
    "        return r\n",
    "    \n",
    "    def to_dataset(self, datasetclass=None):\n",
    "        \"\"\"\n",
    "        Converts this DSet into a PyTorch DataSet.\n",
    "        \n",
    "        Arguments: \n",
    "            datasetclass: class (TensorDataset)\n",
    "                the class to use to instantiate the dataset\n",
    "        \n",
    "        returns: DataSet\n",
    "            A PyTorch DataSet over X_tensor and y_tensor\n",
    "        \"\"\"\n",
    "        self._pt_datasetclass = datasetclass or self._pt_datasetclass\n",
    "        if self._pt_datasetclass is not None:\n",
    "            r = self._pt_datasetclass(*self.tensors)\n",
    "        elif self._pt_dtype is str or self._pt_dtype == False:\n",
    "            r = NumpyDataset(self.X, self.y)\n",
    "        else:\n",
    "            from torch.utils.data import TensorDataset\n",
    "            r = TensorDataset(*self.tensors)\n",
    "        if self._pt_dataset_transforms is not None:\n",
    "            r = TransformableDataset(r, self._pt_dtype, *self._pt_dataset_transforms)\n",
    "        return r\n",
    "    \n",
    "    def to_dataloader(self, batch_size=32, shuffle=True, collate_fn=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Converts this DSet into a PyTorch DataLoader.\n",
    "        \n",
    "        Arguments: \n",
    "            batch_size: int (32)\n",
    "            \n",
    "            shuffle: bool (True)\n",
    "            \n",
    "            collate_fn: callable (None)\n",
    "            \n",
    "            **kwargs: dict\n",
    "                passed on to DataLoader\n",
    "        \n",
    "        returns: DataLoader\n",
    "            A PyTorch DataLoader over X_tensor and y_tensor\n",
    "        \"\"\"\n",
    "        if collate_fn is not None:\n",
    "            kwargs['collate_fn'] = collate_fn\n",
    "        from torch.utils.data import DataLoader\n",
    "        return DataLoader(self.to_dataset(), batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    \n",
    "    def _predict_y(self, predict):\n",
    "        if not callable(predict):\n",
    "            return predict\n",
    "        try:\n",
    "            from torch import nn\n",
    "            import torch\n",
    "            with torch.set_grad_enabled(False):\n",
    "                return to_numpy(predict(self.X_tensor)).reshape(len(self))\n",
    "        except:\n",
    "            raise\n",
    "        try:\n",
    "            return predict(self.X).reshape(len(self))\n",
    "        except:\n",
    "            raise\n",
    "            raise ValueError('predict mus be a function that works on Numpy arrays or PyTorch tensors')\n",
    "\n",
    "    def _predict(self, predict):\n",
    "        return self.inverse_scale_y(self._predict_y(predict))\n",
    "\n",
    "    def predict(self, predict, drop=True):\n",
    "        y_pred = self._predict_y(predict)\n",
    "        if drop:\n",
    "            return self._df.inverse_scale(self.X, y_pred)\n",
    "        return self._df.inverse_scale(self.X, self.y, y_pred)\n",
    "    \n",
    "    def add_column(self, y_pred, *columns, inplace=False):\n",
    "        \"\"\"\n",
    "        Fills the given predictions into the columns.\n",
    "        \n",
    "        Depending on inplace, True means the values are placed in the original \n",
    "        DFrame from which this subset was generated, and False means that a new\n",
    "        copy of the subset (DSet) is returned. See fill_column().\n",
    "        \n",
    "        The predictions are automatically converted (scaled) back using \n",
    "        inverse_transform to the original scale.\n",
    "        \n",
    "        Args:\n",
    "            y_pred: 2D Array like data\n",
    "                the values to be stored in the Dataframe. The number of rows must\n",
    "                match the number of values for the target variable in the subset.\n",
    "            *columns: str (None)\n",
    "                the names of columns in which to store the values. The number of\n",
    "                columns must match the number of columns in values. If None,\n",
    "                new columns are added with the name of the target variable(s) and \n",
    "                the suffix '_pred'\n",
    "            inplace: bool (False)\n",
    "                whether to write the values in a local copy of the subset or to\n",
    "                write them in the original DFrame that generated this subset. Read\n",
    "                the warnings above.\n",
    "            offeset: int (0)\n",
    "                offset can be set when sequences are used to set values on the same\n",
    "                row as the target variable.\n",
    "                \n",
    "        Returns: DSet or None\n",
    "            Either a DSet with modified values (inplace=False) or None (inplace=True)\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = to_numpy(y_pred)\n",
    "        offset = self._range_y.start\n",
    "        y_pred = self.inverse_scale_y(y_pred)\n",
    "        if len(columns) == 0:\n",
    "            columns = [ c + '_pred' for c in self._columny ]\n",
    "        \n",
    "        return self.fill_column(y_pred, *columns, offset=offset, inplace=inplace)\n",
    "\n",
    "    def fill_column(self, values, *columns, inplace=False, offset=0):\n",
    "        \"\"\"\n",
    "        Fills the given columns with the given values.\n",
    "        \n",
    "        Depending on inplace, True means the values are placed in the original \n",
    "        DFrame from which this subset was generated, and False means that a new\n",
    "        copy of the subset (DSet) is returned. \n",
    "        \n",
    "        Both variants come with a caution:\n",
    "        INPLACE=True means that the original DFrame will have to regenerate all subsets\n",
    "        (which it will do automatically) and any copy of a subset will become invalid.\n",
    "        Additionally, columns that are added will also be added to the entire DFrame\n",
    "        and therefore also any subset, while only this subset will be seeded with\n",
    "        values. Use this when the data is to be used for further processing with\n",
    "        PipeTorch.\n",
    "        \n",
    "        INPLACE=False means that the changes are lost when the subsets are regenerated \n",
    "        (e.g. when you reconfigure the pipeline). Also, the connection between the\n",
    "        DFrame and the subset are broken, therefore preparing data may not be possible\n",
    "        anymore. Use this to add data to a subset to process with Pandas for reporting.\n",
    "\n",
    "        Args:\n",
    "            values: 2D Array like data\n",
    "                the values to be stored in the Dataframe. The number of rows must\n",
    "                match the number of rows in the subset.\n",
    "            *columns: str\n",
    "                the names of columns in which to store the values. The number of\n",
    "                columns must match the number of columns in values.\n",
    "            inplace: bool (False)\n",
    "                whether to write the values in a local copy of the subset or to\n",
    "                write them in the original DFrame that generated this subset. Read\n",
    "                the warnings above.\n",
    "            offeset: int (0)\n",
    "                offset can be set when sequences are used to set values on the same\n",
    "                row as the target variable.\n",
    "                \n",
    "        Returns: DSet or None\n",
    "            Either a DSet with modified values (inplace=False) or None (inplace=True)\n",
    "        \"\"\"\n",
    "        indices = self.indices\n",
    "        if offset > 0:\n",
    "            indices = [False] * offset + indices[:len(self)-offset]\n",
    "        if np.isscalar(values):\n",
    "            v = np.empty((sum(indices), len(columns)))\n",
    "            v[:,:] = values\n",
    "            values = v\n",
    "        else:\n",
    "            values = to_numpy(values)\n",
    "\n",
    "        assert len(values) == sum(indices), f'The number of values ({len(values)}) does not match the number of samples ({sum(indices)})'\n",
    "        \n",
    "        if inplace:\n",
    "            try:\n",
    "                dfindices = self.index[indices]\n",
    "            except:\n",
    "                raise TypeError('You can only use fill_column inplace if an index is present.')\n",
    "\n",
    "            for c in columns:\n",
    "                if c not in self._df.columns:\n",
    "                    self._df[c] = np.NaN\n",
    "        #columns = [self._df.columns.get_loc(c) for c in columns]\n",
    "            self._df.loc[ dfindices, columns ] = values\n",
    "            self._df._columns_changed()\n",
    "        else:\n",
    "            r = copy.deepcopy(self)\n",
    "            for c in columns:\n",
    "                r[c] = np.NaN\n",
    "            #columns = [r.columns.get_loc(c) for c in columns]\n",
    "            r.loc[ indices, columns] = values\n",
    "            return r\n",
    "\n",
    "    def inverse_scale_y(self, y_pred):\n",
    "        return self._df.inverse_scale_y(y_pred)\n",
    "    \n",
    "    def line(self, x=None, y=None, xlabel = None, ylabel = None, title = None, **kwargs ):\n",
    "        \"\"\"\n",
    "        Plots a line graph of this dataset using matplotlib.\n",
    "        \n",
    "        Args:\n",
    "            x: str (None)\n",
    "                the column to use on the x-axis. If None, the first input feature is used.\n",
    "                \n",
    "            y: str, Array or function (None)\n",
    "                the column to use in the y-axis. If None, the first target feature is used.\n",
    "                You may also pass an array with values (for example model predictions), but these\n",
    "                must be paired with the dataset rows. Alernatively, pass a function(X) that is called on X \n",
    "                to generate y. PipeTorch will attempt to first use a tensor (in case of a PyTorch model) \n",
    "                and when that fails with a Numpy Array.\n",
    "                \n",
    "            xlabel: str (None)\n",
    "                the label to use on the x-axis. If None, the name of x is used.\n",
    "                \n",
    "            ylabel: str (none)\n",
    "                the label to use on the y-axis. If None, the name of y is used.\n",
    "                \n",
    "            title: str (None)\n",
    "                the title used for the figure\n",
    "                \n",
    "            kwargs: dict\n",
    "                arguments that are passed to plt.plot\n",
    "        \"\"\"\n",
    "        \n",
    "        self._df._evaluator().line(x=x, y=y, xlabel=xlabel, ylabel=ylabel, title=title, df=self, **kwargs)\n",
    "    \n",
    "    def scatter(self, x=None, y=None, xlabel = None, ylabel = None, title = None, **kwargs ):\n",
    "        \"\"\"\n",
    "        Plots a scatter graph of this dataset using matplotlib.\n",
    "        \n",
    "        Args:\n",
    "            x: str (None)\n",
    "                the column to use on the x-axis. If None, the first input feature is used.\n",
    "                \n",
    "            y: str, Array or function (None)\n",
    "                the column to use in the y-axis. If None, the first target feature is used.\n",
    "                You may also pass an array with values (for example model predictions), but these\n",
    "                must be paired with the dataset rows. Alernatively, pass a function(X) that is called on X \n",
    "                to generate y. PipeTorch will attempt to first use a tensor (in case of a PyTorch model) \n",
    "                and when that fails with a Numpy Array.\n",
    "                \n",
    "            xlabel: str (None)\n",
    "                the label to use on the x-axis. If None, the name of x is used.\n",
    "                \n",
    "            ylabel: str (none)\n",
    "                the label to use on the y-axis. If None, the name of y is used.\n",
    "                \n",
    "            title: str (None)\n",
    "                the title used for the figure\n",
    "                \n",
    "            kwargs: dict\n",
    "                arguments that are passed to plt.plot\n",
    "        \"\"\"\n",
    "        self._df._evaluator().scatter(x=x, y=y, xlabel=xlabel, ylabel=ylabel, title=title, df=self, **kwargs)\n",
    "    \n",
    "    def scatter2d_class(self, x1=None, x2=None, y=None, xlabel=None, ylabel=None, title=None, loc='upper right', noise=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Plots a 2d scatter graph of this dataset using matplotlib. The y-label is used as a class label.\n",
    "        \n",
    "        Args:\n",
    "            x1: str (None)\n",
    "                the column to use on the x-axis. If None, the first input feature is used.\n",
    "                \n",
    "            x2: str (None)\n",
    "                the column to use on the x-axis. If None, the second input feature is used.\n",
    "                \n",
    "            y: str, Array or function (None)\n",
    "                the column to use as the series for the plot. If None, the first target feature is used.\n",
    "                You may also pass an array with values (for example model predictions), but these\n",
    "                must be paired to the dataset rows. Alernatively, pass a function(X) that is called on X \n",
    "                to generate y. PipeTorch will attempt to first use a tensor (in case of a PyTorch model) \n",
    "                and when that fails with a Numpy Array.\n",
    "                \n",
    "            xlabel: str (None)\n",
    "                the label to use on the x-axis. If None, the name of x is used.\n",
    "                \n",
    "            ylabel: str (none)\n",
    "                the label to use on the y-axis. If None, the name of y is used.\n",
    "                \n",
    "            title: str (None)\n",
    "                the title used for the figure\n",
    "                \n",
    "            loc: str ('upper right')\n",
    "                passed to plt.legend to place the legend in a certain position\n",
    "                \n",
    "            noise: 0 (float)\n",
    "                transforms s0 that x1 and x2 are incremented with noise multiplied by a random number\n",
    "                from their respecrive standard deviation. This allows better visualization of discrete data.\n",
    "                \n",
    "            kwargs: dict\n",
    "                arguments that are passed to plt.plot\n",
    "        \"\"\"\n",
    "        self._df._evaluator().scatter2d_class(x1=x1, x2=x2, y=y, xlabel=xlabel, ylabel=ylabel, title=title, loc=loc, noise=noise, df=self, **kwargs)\n",
    "\n",
    "    def scatter2d_color(self, x1=None, x2=None, c=None, xlabel=None, ylabel=None, title=None, noise=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Plots a 2d scatter graph of this dataset using matplotlib. The y-label is used to color the points.\n",
    "        \n",
    "        Args:\n",
    "            x1: str (None)\n",
    "                the column to use on the x-axis. If None, the first input feature is used.\n",
    "                \n",
    "            x2: str (None)\n",
    "                the column to use on the x-axis. If None, the second input feature is used.\n",
    "                \n",
    "            y: str, Array or function (None)\n",
    "                the column to use as the series for the plot. If None, the first target feature is used.\n",
    "                You may also pass an array with values (for example model predictions), but these\n",
    "                must be paired to the dataset rows. Alernatively, pass a function(X) that is called on X \n",
    "                to generate y. PipeTorch will attempt to first use a tensor (in case of a PyTorch model) \n",
    "                and when that fails with a Numpy Array.\n",
    "                \n",
    "            xlabel: str (None)\n",
    "                the label to use on the x-axis. If None, the name of x is used.\n",
    "                \n",
    "            ylabel: str (none)\n",
    "                the label to use on the y-axis. If None, the name of y is used.\n",
    "                \n",
    "            title: str (None)\n",
    "                the title used for the figure\n",
    "                \n",
    "            loc: str ('upper right')\n",
    "                passed to plt.legend to place the legend in a certain position\n",
    "                \n",
    "            noise: 0 (float)\n",
    "                transforms s0 that x1 and x2 are incremented with noise multiplied by a random number\n",
    "                from their respecrive standard deviation. This allows better visualization of discrete data.\n",
    "                \n",
    "            kwargs: dict\n",
    "                arguments that are passed to plt.plot\n",
    "        \"\"\"\n",
    "        self._df._evaluator().scatter2d_color(x1=x1, x2=x2, c=c, xlabel=xlabel, ylabel=ylabel, title=title, noise=noise, df=self, **kwargs)\n",
    "\n",
    "    def scatter2d_size(self, x1=None, x2=None, s=None, xlabel=None, ylabel=None, title=None, noise=0, **kwargs):\n",
    "        \"\"\"\n",
    "        Plots a 2d scatter graph of this dataset using matplotlib. The y-label is used as the point size.\n",
    "        \n",
    "        Args:\n",
    "            x1: str (None)\n",
    "                the column to use on the x-axis. If None, the first input feature is used.\n",
    "                \n",
    "            x2: str (None)\n",
    "                the column to use on the x-axis. If None, the second input feature is used.\n",
    "                \n",
    "            y: str, Array or function (None)\n",
    "                the column to use as the series for the plot. If None, the first target feature is used.\n",
    "                You may also pass an array with values (for example model predictions), but these\n",
    "                must be paired to the dataset rows. Alernatively, pass a function(X) that is called on X \n",
    "                to generate y. PipeTorch will attempt to first use a tensor (in case of a PyTorch model) \n",
    "                and when that fails with a Numpy Array.\n",
    "                \n",
    "            xlabel: str (None)\n",
    "                the label to use on the x-axis. If None, the name of x is used.\n",
    "                \n",
    "            ylabel: str (none)\n",
    "                the label to use on the y-axis. If None, the name of y is used.\n",
    "                \n",
    "            title: str (None)\n",
    "                the title used for the figure\n",
    "                \n",
    "            loc: str ('upper right')\n",
    "                passed to plt.legend to place the legend in a certain position\n",
    "                \n",
    "            noise: 0 (float)\n",
    "                transforms s0 that x1 and x2 are incremented with noise multiplied by a random number\n",
    "                from their respecrive standard deviation. This allows better visualization of discrete data.\n",
    "                \n",
    "            kwargs: dict\n",
    "                arguments that are passed to plt.plot\n",
    "        \"\"\"\n",
    "        self._df._evaluator().scatter2d_size(x1=x1, x2=x2, s=s, xlabel=xlabel, ylabel=ylabel, title=title, noise=noise, df=self, **kwargs)\n",
    "\n",
    "    def plot_boundary(self, predict, levels=[0.5]):\n",
    "        \"\"\"\n",
    "        Plots a decision boundary for classification models that use exactly two input features. \n",
    "        Prior to calling this function, you should already scatter_plot the dataset, beacuse this\n",
    "        function uses the minimum and maximum values on the axis to do a grid search. It will then\n",
    "        overlay the decision boundary on the existing plot.\n",
    "        \n",
    "        Args:\n",
    "            predict: function (None)\n",
    "                a function(X) that is called to classify an X with two features \n",
    "                PipeTorch will attempt to first use a tensor (in case of a PyTorch model) \n",
    "                and when that fails with a Numpy Array.\n",
    "                \n",
    "            levels: [ float ] ([0.5])\n",
    "                the levels of the decision boundaries to plot. Pass multiple values or None\n",
    "                to generate a contour plot.\n",
    "                \n",
    "            kwargs: dict\n",
    "                arguments that are passed to plt.plot\n",
    "        \"\"\"\n",
    "        self._df._evaluator().plot_boundary(predict, levels=levels)\n",
    "\n",
    "    def inspect(self):\n",
    "        \"\"\"\n",
    "        Describe the data in the DFrame, specifically by reporting per column \n",
    "        - Datatype \n",
    "        - Missing: number and percetage of 'Missing' values\n",
    "        - Range: numeric types, are described as a range [min, max]\n",
    "                  and for non-numeric types the #number of unique values is given\n",
    "        - Values: the two most frequently occuring values (most frequent first).\n",
    "        \"\"\"\n",
    "\n",
    "        missing_count = self.isnull().sum() # the count of missing values\n",
    "        value_count = self.isnull().count() # the count of all values\n",
    "        missing_percentage = round(missing_count / value_count * 100,2) #the percentage of missing values\n",
    "\n",
    "        datatypes = self.dtypes\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Missing (#)': missing_count, \n",
    "            'Missing (%)': missing_percentage,\n",
    "            'Datatype': datatypes\n",
    "        }) #create a dataframe\n",
    "        df = df.sort_values(by=['Missing (#)'], ascending=False)\n",
    "\n",
    "        value_col = []\n",
    "        range_col = []\n",
    "        for index, row in df.iterrows():\n",
    "            u = self[index].value_counts().index.tolist()\n",
    "            if pd.api.types.is_numeric_dtype(row['Datatype']):\n",
    "                _range = f\"[{self[index].min()}, {self[index].max()}]\"\n",
    "            else:\n",
    "                _range = f\"#{len(u)}\"\n",
    "            if len(u) == 1:\n",
    "                _values = f'({u})'\n",
    "            elif len(u) == 2:\n",
    "                _values = f'({u[0]}, {u[1]})'\n",
    "            elif len(u) > 2:\n",
    "                _values = f'({u[0]}, {u[1]}, ...)'\n",
    "            else:\n",
    "                _values = ''\n",
    "            range_col.append(_range)\n",
    "            value_col.append(_values)\n",
    "        df[\"Range\"] = range_col\n",
    "        df[\"Values\"] = value_col\n",
    "        return df\n",
    "     \n",
    "class DSet(pd.DataFrame, _DSet):\n",
    "    _metadata = _DSet._metadata\n",
    "    _internal_names = _DSet._internal_names\n",
    "    _internal_names_set = _DSet._internal_names_set\n",
    "    \n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(data, *args, **kwargs)\n",
    "        _DSet.__init__(self, data)\n",
    "        \n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return DSet\n",
    "           \n",
    "    @classmethod\n",
    "    def from_dframe(cls, data, df, transforms):\n",
    "        r = cls(data)\n",
    "        r._df = df\n",
    "        #r._dfindices = np.array(dfindices)\n",
    "        r._pt_columny = df._columny\n",
    "        r._pt_columnx = df._columnx\n",
    "        #r._pt_dataset = df._pt_dataset\n",
    "        r._pt_dataset_transforms = transforms\n",
    "        r._pt_vectory = df._pt_vectory\n",
    "        r._pt_polynomials = df._pt_polynomials\n",
    "        r._pt_bias = df._pt_bias\n",
    "        r._pt_dtype = df._pt_dtype\n",
    "        r._pt_is_test = False\n",
    "        r._pt_sequence_window = df._pt_sequence_window\n",
    "        r._pt_sequence_shift_y = df._pt_sequence_shift_y\n",
    "        r._pt_filterna = df._pt_filterna\n",
    "        return r\n",
    "    \n",
    "    @classmethod\n",
    "    def df_to_testset(cls, data, df, transforms):\n",
    "        r = cls.from_dframe(data, df, transforms)\n",
    "        r._pt_is_test = True\n",
    "        return r\n",
    "    \n",
    "    def groupby(self, by, axis=0, level=None, as_index=True, sort=True, group_keys=True, observed=False, dropna=True):\n",
    "        r = super().groupby(by, axis=axis, level=level, as_index=as_index, sort=sort, group_keys=group_keys, observed=observed, dropna=dropna)\n",
    "        return self._copy_meta( GroupedDSet(r) )\n",
    "\n",
    "class NumpyDataset(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "class GroupedDSetSeries(SeriesGroupBy, _DSet):\n",
    "    _metadata = _DSet._metadata\n",
    "    #_internal_names = PTDS._internal_names\n",
    "    #_internal_names_set = PTDS._internal_names_set\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDSetSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return GroupedDFrame\n",
    "    \n",
    "class GroupedDSet(DataFrameGroupBy, _DSet):\n",
    "    _metadata = _DSet._metadata\n",
    "    #_internal_names = PTDS._internal_names\n",
    "    #_internal_names_set = PTDS._internal_names_set\n",
    "\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__(obj=data.obj, keys=data.keys, axis=data.axis, level=data.level, grouper=data.grouper, exclusions=data.exclusions,\n",
    "                selection=data._selection, as_index=data.as_index, sort=data.sort, group_keys=data.group_keys,\n",
    "                observed=data.observed, mutated=data.mutated, dropna=data.dropna)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDSet\n",
    "    \n",
    "    @property\n",
    "    def _constructor_sliced(self):\n",
    "        return GroupedDSetSeries\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for group, subset in super().__iter__():\n",
    "            yield group, selfl._copy_meta(subset)\n",
    "            \n",
    "    def get_group(self, name, obj=None):\n",
    "        return self._dset( super().get_group(name, obj=obj) )\n",
    "        \n",
    "    def to_dataset(self):\n",
    "        from torch.utils.data import ConcatDataset\n",
    "        dss = []\n",
    "        for key, group in self:\n",
    "            dss.append( group.to_dataset())\n",
    "\n",
    "        return ConcatDataset(dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
