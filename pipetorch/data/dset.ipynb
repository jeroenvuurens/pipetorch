{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209951df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dset.py        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy, SeriesGroupBy\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from ..data.transformabledataset import TransformableDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "import copy\n",
    "import os\n",
    "\n",
    "def to_numpy(arr):\n",
    "    try:\n",
    "        return arr.data.cpu().numpy()\n",
    "    except: pass\n",
    "    try:\n",
    "        return arr.to_numpy()\n",
    "    except: pass\n",
    "    return arr\n",
    "\n",
    "class _DSet:\n",
    "    _metadata = ['_df', '_dfindices', '_pt_categoryx', '_pt_categoryy', '_pt_dummiesx', '_pt_dummiesy', \n",
    "                 '_pt_columny', '_pt_columnx', '_pt_transposey', '_pt_bias', '_pt_polynomials', \n",
    "                 '_pt_dtype', '_pt_sequence_window', '_pt_sequence_shift_y', '_pt_is_test', \n",
    "                 '_pt_dataset', '_pt_transforms']\n",
    "    _internal_names = pd.DataFrame._internal_names + [\"_pt__indices\", \"_pt__x_sequence\"]\n",
    "    _internal_names_set = set(_internal_names)\n",
    "    \n",
    "    def to_dframe(self):\n",
    "        cls = self._df.__class__\n",
    "        r = cls(self)\n",
    "\n",
    "        r._pt_columnx = self._pt_columnx\n",
    "        r._pt_columny = self._pt_columny\n",
    "        r._pt_transposey = self._pt_transposey\n",
    "        r._pt_bias = self._pt_bias\n",
    "        r._pt_polynomials = self._pt_polynomials\n",
    "        r._pt_sequence_window = self._pt_sequence_window\n",
    "        r._pt_sequence_shift_y = self._pt_sequence_shift_y\n",
    "        r._pt_dataset = self._pt_dataset\n",
    "        r._pt_transforms = self._pt_transforms\n",
    "        \n",
    "        r._pt__train = self\n",
    "        r._pt__valid = None\n",
    "        r._pt__test = None\n",
    "        r._pt_locked_indices = list(range(len(self)))\n",
    "        r._pt_locked_train_indices = r._pt_locked_indices\n",
    "        r._pt_locked_valid_indices = []\n",
    "        r._pt_locked_test_indices = []\n",
    "        r._pt_locked_categoryx = self._pt_categoryx\n",
    "        r._pt_locked_categoryy = self._pt_categoryy\n",
    "        r._pt_locked_dummiesx = self._pt_dummiesx\n",
    "        r._pt_locked_dummiesy = self._pt_dummiesy\n",
    "        r._pt_split = None\n",
    "        r._pt_random_state = None\n",
    "        r._pt_balance = None\n",
    "        r._pt_shuffle = False\n",
    "        return r\n",
    "\n",
    "    def _copy_meta(self, r):\n",
    "        r._df = self._df\n",
    "        r._dfindices = self._dfindices\n",
    "        r._pt_categoryx = self._pt_categoryx\n",
    "        r._pt_categoryy = self._pt_categoryy\n",
    "        r._pt_dummiesx = self._pt_dummiesx\n",
    "        r._pt_dummiesy = self._pt_dummiesy\n",
    "        r._pt_columny = self._pt_columny\n",
    "        r._pt_columnx = self._pt_columnx\n",
    "        r._pt_is_test = self._pt_is_test\n",
    "        r._pt_transposey = self._pt_transposey\n",
    "        r._pt_polynomials = self._pt_polynomials\n",
    "        r._pt_transforms = self._pt_transforms\n",
    "        r._pt_dataset = self._pt_dataset\n",
    "        r._pt_bias = self._pt_bias\n",
    "        r._pt_dtype = self._pt_dtype\n",
    "        r._pt_sequence_window = self._pt_sequence_window\n",
    "        r._pt_sequence_shift_y = self._pt_sequence_shift_y\n",
    "        return r\n",
    "    \n",
    "    def _dset(self, data):\n",
    "        return self._copy_meta( DSet(data) )\n",
    "    \n",
    "    def _not_nan(self, a):\n",
    "        a = pd.isnull(a)\n",
    "        while len(a.shape) > 1:\n",
    "            a = np.any(a, -1)\n",
    "        return np.where(~a)[0]\n",
    "    \n",
    "    def df_to_dset(self, df):\n",
    "        \"\"\"\n",
    "        Converts a DataFrame to a DSet that has the pipeline as this DSet.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame\n",
    "        \n",
    "        Returns: DSet\n",
    "        \"\"\"\n",
    "        return self._df.df_to_dset(df)\n",
    "\n",
    "    def df_to_dataset(self, df):\n",
    "        \"\"\"\n",
    "        Converts the given df to a DataSet using the pipeline of this DSet.\n",
    "        \n",
    "        Arguments:\n",
    "            df: DataFrame or DFrame\n",
    "                to convert into a DataSet\n",
    "        \n",
    "        returns: DataSet.\n",
    "        \"\"\"\n",
    "        return self.df_to_dset(df).to_dataset()    \n",
    "\n",
    "    @property\n",
    "    def _dtype(self):\n",
    "        return self._pt_dtype\n",
    "    \n",
    "    @property\n",
    "    def indices(self):\n",
    "        try:\n",
    "            return self._pt__indices\n",
    "        except:\n",
    "            if self._pt_is_test:\n",
    "                self._pt__indices = self._not_nan(self._x_sequence)\n",
    "            else:\n",
    "                s = set(self._not_nan(self._y_transposed))\n",
    "                self._pt__indices = [ i for i in self._not_nan(self._x_sequence) if i in s]\n",
    "            return self._pt__indices\n",
    "    \n",
    "    @property\n",
    "    def _scalerx(self):\n",
    "        return self._df._scalerx\n",
    "        \n",
    "    @property\n",
    "    def _scalery(self):\n",
    "        return self._df._scalery\n",
    "\n",
    "    @property\n",
    "    def _categoryx(self):\n",
    "        return self._pt_categoryx()\n",
    "        \n",
    "    @property\n",
    "    def _categoryy(self):\n",
    "        return self._pt_categoryy()\n",
    "\n",
    "    @property\n",
    "    def _dummiesx(self):\n",
    "        return self._pt_dummiesx()\n",
    "        \n",
    "    @property\n",
    "    def _dummiesy(self):\n",
    "        return self._pt_dummiesy()\n",
    "\n",
    "    @property\n",
    "    def _shift_y(self):\n",
    "        if self._pt_sequence_shift_y is not None:\n",
    "            return self._pt_sequence_shift_y\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @property\n",
    "    def _sequence_window(self):\n",
    "        try:\n",
    "            if self._is_sequence:\n",
    "                return self._pt_sequence_window\n",
    "        except:pass\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def _sequence_index_y(self):\n",
    "        return self._pt_sequence_window+self._shift_y-1\n",
    "\n",
    "    @property\n",
    "    def _columny(self):\n",
    "        return [ self.columns[-1] ] if self._pt_columny is None else self._pt_columny\n",
    "        \n",
    "    @property\n",
    "    def _transposey(self):\n",
    "        return True if self._pt_transposey is None else self._pt_transposey\n",
    "            \n",
    "    @property\n",
    "    def _columnx(self):\n",
    "        if self._pt_columnx is None:\n",
    "            return [ c for c in self.columns if c not in self._columny ]\n",
    "        return self._pt_columnx\n",
    "   \n",
    "    @property\n",
    "    def _polynomials(self):\n",
    "        return self._pt_polynomials\n",
    "       \n",
    "    @property\n",
    "    def _bias(self):\n",
    "        return self._pt_bias\n",
    "    \n",
    "    def _transform(self, scalers, array):\n",
    "        out = []\n",
    "        for i, scaler in enumerate(scalers):\n",
    "            if scaler is not None:\n",
    "                out.append(scaler.transform(array[:, i:i+1]))\n",
    "            else:\n",
    "                out.append(array[:, i:i+1])\n",
    "        return np.concatenate(out, axis=1)\n",
    "\n",
    "    def resample_rows(self, n=True):\n",
    "        \"\"\"\n",
    "        Resamples this DSet with replacement to support bootstrapping.\n",
    "        \n",
    "        Arguments:\n",
    "            n: int (True)\n",
    "                The number of samples to take, or the size of the dataset if n equals True.\n",
    "        \n",
    "        Returns: DSet\n",
    "            A resampled DSet\n",
    "        \"\"\"\n",
    "        r = self._dset(self)\n",
    "        if n == True:\n",
    "            n = len(r)\n",
    "        if n < 1:\n",
    "            n = n * len(r)\n",
    "        return r.iloc[resample(list(range(len(r))), n_samples = int(n))]\n",
    "    \n",
    "    def interpolate_factor(self, factor=2, sortcolumn=None):\n",
    "        \"\"\"\n",
    "        Interpolates the data between the rows. This function was created to simplify drawing\n",
    "        of higher order polynomial functions, and may be useful for other situations, but it is\n",
    "        limited to simple interplation between two consecutive points.\n",
    "        \n",
    "        Arguments:\n",
    "            factor: int (2)\n",
    "                interpolates by ading 2 ** factor - 1 values. In other words, a factor of 1 interpolates\n",
    "                1 value betweem every 2 existing values and a factor of 2 will interpolate 3 values between \n",
    "                every two values.\n",
    "            sortcolumn: str (None)\n",
    "                Before interpolating, the DSet is first sorted by this column, or the first column if None is\n",
    "                provided.\n",
    "                \n",
    "        Returns: DSet\n",
    "            In which the values are interpolated.\n",
    "        \"\"\"\n",
    "        if not sortcolumn:\n",
    "            sortcolumn = self.columns[0]\n",
    "        df = self.sort_values(by=sortcolumn)\n",
    "        for i in range(factor):\n",
    "            i = df.rolling(2).sum()[1:] / 2.0\n",
    "            df = pd.concat([df, i], axis=0)\n",
    "            df = df.sort_values(by=sortcolumn)\n",
    "        return self._df._dset(df).reset_index(drop=True)\n",
    "    \n",
    "    @property\n",
    "    def _x_category(self):\n",
    "        if self._is_sequence:\n",
    "            self = self.iloc[:-self._shift_y]\n",
    "        if self._categoryx is None:\n",
    "            return self[self._columnx]\n",
    "        r = copy.copy(self[self._columnx])\n",
    "        for c, cat in zip(r._columnx, r._categoryx):\n",
    "            if cat is not None:\n",
    "                r[c] = cat.transform(r[c])\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _x_dummies(self):\n",
    "        if self._dummiesx is None:\n",
    "            return self._x_category\n",
    "        r = copy.copy(self._x_category)\n",
    "        r1 = []\n",
    "        for d, onehot in zip(r._columnx, r._dummiesx):\n",
    "            if onehot is not None:\n",
    "                a = onehot.transform(r[[d]])\n",
    "                r1.append( pd.DataFrame(a.toarray(), columns=onehot.get_feature_names_out([d])) )\n",
    "                r = r.drop(columns = d)\n",
    "        r1.insert(0, r.reset_index(drop=True))\n",
    "        r = pd.concat(r1, axis=1)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _x_numpy(self):\n",
    "        return self._x_dummies.to_numpy()\n",
    "    \n",
    "    @property\n",
    "    def _x_polynomials(self):\n",
    "        try:\n",
    "            return self._polynomials.fit_transform(self._x_numpy)\n",
    "        except:\n",
    "            return self._x_numpy\n",
    "\n",
    "    @property\n",
    "    def _x_scaled(self):\n",
    "        if len(self) > 0:\n",
    "            return self._transform(self._scalerx, self._x_polynomials)\n",
    "        return self._x_polynomials\n",
    "            \n",
    "    @property\n",
    "    def _x_biased(self):\n",
    "        a = self._x_scaled\n",
    "        if self._bias:\n",
    "            return np.concatenate([np.ones((len(a),1)), a], axis=1)\n",
    "        return a\n",
    "    \n",
    "    @property\n",
    "    def _x_sequence(self):\n",
    "        try:\n",
    "            return self._pt__x_sequence\n",
    "        except:\n",
    "            if not self._is_sequence:\n",
    "                self._pt__x_sequence = self._x_biased\n",
    "            else:\n",
    "                X = self._x_biased\n",
    "                window = self._sequence_window\n",
    "                len_seq_mode = max(0, len(X) - window + 1)\n",
    "                self._pt__x_sequence =  np.concatenate([np.expand_dims(X[ii:ii+window], axis=0) for ii in range(len_seq_mode)], axis=0)        \n",
    "            return self._pt__x_sequence\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        \"\"\"\n",
    "        Constructs a Numpy array in which the values for X are transformed according to the configured pipeline.\n",
    "        \n",
    "        Returns: Numpy array\n",
    "        \"\"\"\n",
    "        return self._x_sequence[self.indices]\n",
    "\n",
    "    @property\n",
    "    def X_tensor(self):\n",
    "        \"\"\"\n",
    "        Constructs a PyTorch tensor in which the values for X are transformed according to the configured pipeline.\n",
    "        A difference with X() is that the data is transformed to a single datatype (by default torch.FloatTensor).\n",
    "        \n",
    "        Returns: PyTorch tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        import torch\n",
    "        if self._dtype is None:\n",
    "            return torch.tensor(self.X).type(torch.FloatTensor)\n",
    "        else:\n",
    "            return torch.tensor(self.X)\n",
    "\n",
    "    @property\n",
    "    def y_tensor(self):\n",
    "        \"\"\"\n",
    "        Constructs a PyTorch tensor in which the values for y are transformed according to the configured pipeline.\n",
    "        A difference with y() is that the data is transformed to a single datatype (by default torch.FloatTensor).\n",
    "        \n",
    "        Returns: PyTorch tensor\n",
    "        \"\"\"\n",
    "\n",
    "        import torch\n",
    "        if self._dtype is None:\n",
    "            return torch.tensor(self.y).type(torch.FloatTensor)\n",
    "        else:\n",
    "            return torch.tensor(self.y)\n",
    " \n",
    "    @property\n",
    "    def _is_sequence(self):\n",
    "        return self._pt_sequence_window is not None\n",
    "        \n",
    "    @property\n",
    "    def tensors(self):\n",
    "        \"\"\"\n",
    "        Combines X_tensor() and y_tensor()\n",
    "        \n",
    "        Returns: tensor, tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return self.X_tensor, self.y_tensor\n",
    "            \n",
    "    @property\n",
    "    def _range_y(self):\n",
    "        stop = len(self) if self._shift_y >= 0 else len(self) + self._shift_y\n",
    "        start = min(stop, self._sequence_window + self._shift_y - 1)\n",
    "        return slice(start, stop)\n",
    "        \n",
    "    @property\n",
    "    def _y_category(self):\n",
    "        if self._is_sequence:\n",
    "            self = self.iloc[self._range_y]\n",
    "        if self._categoryy is None:\n",
    "            return self[self._columny]\n",
    "        r = copy.copy(self[self._columny])\n",
    "        for d, onehot in zip(r._columny, r._dummiesy):\n",
    "            if onehot is not None:\n",
    "                r[c] = cat.transform(r[c])\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _y_dummies(self):\n",
    "        if self._dummiesy is None:\n",
    "            return self._y_category\n",
    "        r = copy.copy(self._y_category)\n",
    "        r1 = []\n",
    "        for d, onehot in zip(r._columny, r._dummiesy):\n",
    "            if onehot is not None:\n",
    "                a = onehot.transform(r[[d]])\n",
    "                r1.append( pd.DataFrame(a.toarray(), columns=onehot.get_feature_names_out([d])) )\n",
    "                r = r.drop(columns = d)\n",
    "        r1.insert(0, r.reset_index(drop=True))\n",
    "        r = pd.concat(r1, axis=1)\n",
    "        return r\n",
    "    \n",
    "    @property\n",
    "    def _y_numpy(self):\n",
    "        return self._y_dummies.to_numpy()\n",
    "    \n",
    "    @property\n",
    "    def _y_scaled(self):\n",
    "        if len(self) > 0:\n",
    "            return self._transform(self._scalery, self._y_numpy)\n",
    "        return self._y_numpy\n",
    "            \n",
    "    @property\n",
    "    def _y_transposed(self):\n",
    "        return self._y_scaled.squeeze() if self._transposey else self._y_scaled\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        \"\"\"\n",
    "        Constructs a Numpy array in which the values for y are transformed according to the configured pipeline.\n",
    "        \n",
    "        Returns: Numpy array\n",
    "        \"\"\"\n",
    "        \n",
    "        return self._y_transposed[self.indices]\n",
    "    \n",
    "    def replace_y(self, new_y):\n",
    "        \"\"\"\n",
    "        Returns a copy of this DSet, in which y is replaced with alternative values.\n",
    "        \n",
    "        Arguments:\n",
    "            new_y: Numpy array, PyTorch tensor or callable\n",
    "                If new_y is a callable, it will be called with this DSet's X to generate predicted values of y\n",
    "        \n",
    "        Returns: DSet\n",
    "            A copy of this DSet, in which the values for y are replaced by the given values new_y\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self._predict(new_y)\n",
    "        offset = self._range_y.start\n",
    "        indices = [ i + offset for i in self.indices ]\n",
    "        assert len(y_pred) == len(indices), f'The number of predictions ({len(y_pred)}) does not match the number of samples ({len(indices)})'\n",
    "        r = copy.deepcopy(self)\n",
    "        r[self._columny] = np.NaN\n",
    "        columns = [r.columns.get_loc(c) for c in self._columny]\n",
    "        r.iloc[indices, columns] = y_pred.values\n",
    "        return r\n",
    "    \n",
    "    def to_dataset(self, dataset=None):\n",
    "        \"\"\"\n",
    "        Converts this DSet into a PyTorch DataSet.\n",
    "        \n",
    "        Arguments: \n",
    "            dataset: class (TensorDataset)\n",
    "                the class to use to instantiate the dataset\n",
    "        \n",
    "        returns: DataSet\n",
    "            A PyTorch DataSet over X_tensor and y_tensor\n",
    "        \"\"\"\n",
    "        self._pt_dataset = dataset or self._pt_dataset\n",
    "        if self._pt_dtype is str:\n",
    "            r = NumpyDataset(self.X, self.y)\n",
    "        elif self._pt_dataset is None:\n",
    "            from torch.utils.data import TensorDataset\n",
    "            r = TensorDataset(*self.tensors)\n",
    "        else:\n",
    "            r = self._pt_dataset(*self.tensors)\n",
    "        if self._pt_transforms is not None:\n",
    "            r = TransformableDataset(r, *self._pt_transforms)\n",
    "        return r\n",
    "    \n",
    "    def _dataloader(self, batch_size=32, shuffle=True, collate_fn=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Converts this DSet into a PyTorch DataLoader.\n",
    "        \n",
    "        Arguments: \n",
    "            batch_size: int (32) - batch_size to use\n",
    "            \n",
    "            shuffle: bool (True) - whether to shuffle the data\n",
    "            \n",
    "            collate_fn: callable (None) - function that is used to construct batches from data points\n",
    "            \n",
    "            **kwargs: any named parameter that is passed on to DataLoader\n",
    "        \n",
    "        returns: DataLoader\n",
    "            A PyTorch DataLoader over X_tensor and y_tensor\n",
    "        \"\"\"\n",
    "        if collate_fn is not None:\n",
    "            kwargs['collate_fn'] = collate_fn\n",
    "        return DataLoader(self.to_dataset(), batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "    \n",
    "    def _predict_y(self, predict):\n",
    "        if not callable(predict):\n",
    "            return predict\n",
    "        try:\n",
    "            from torch import nn\n",
    "            import torch\n",
    "            with torch.set_grad_enabled(False):\n",
    "                return to_numpy(predict(self.X_tensor)).reshape(len(self))\n",
    "        except:\n",
    "            raise\n",
    "        try:\n",
    "            return predict(self.X).reshape(len(self))\n",
    "        except:\n",
    "            raise\n",
    "            raise ValueError('predict mus be a function that works on Numpy arrays or PyTorch tensors')\n",
    "\n",
    "    def _predict(self, predict):\n",
    "        return self.inverse_scale_y(self._predict_y(predict))\n",
    "\n",
    "    def predict(self, predict, drop=True):\n",
    "        y_pred = self._predict_y(predict)\n",
    "        if drop:\n",
    "            return self._df.inverse_scale(self.X, y_pred)\n",
    "        return self._df.inverse_scale(self.X, self.y, y_pred)\n",
    "    \n",
    "    def add_column(self, y_pred, *columns):\n",
    "        y_pred = to_numpy(y_pred)\n",
    "        offset = self._range_y.start\n",
    "        indices = [ i + offset for i in self.indices ]\n",
    "\n",
    "        assert len(y_pred) == len(indices), f'The number of predictions ({len(y_pred)}) does not match the number of samples ({len(indices)})'\n",
    "        r = copy.deepcopy(self)\n",
    "        y_pred = self.inverse_scale_y(y_pred)\n",
    "        if len(columns) == 0:\n",
    "            columns = [ c + '_pred' for c in self._columny ]\n",
    "        for c in columns:\n",
    "            r[c] = np.NaN\n",
    "        columns = [r.columns.get_loc(c) for c in columns]\n",
    "        r.iloc[indices, columns] = y_pred.values\n",
    "        return r\n",
    "\n",
    "    def inverse_scale_y(self, y_pred):\n",
    "        return self._df.inverse_scale_y(y_pred)\n",
    "    \n",
    "    def line(self, x=None, y=None, xlabel = None, ylabel = None, title = None, **kwargs ):\n",
    "        self._df.evaluate().line(x=x, y=y, xlabel=xlabel, ylabel=ylabel, title=title, df=self, **kwargs)\n",
    "    \n",
    "    def scatter(self, x=None, y=None, xlabel = None, ylabel = None, title = None, **kwargs ):\n",
    "        self._df.evaluate().scatter(x=x, y=y, xlabel=xlabel, ylabel=ylabel, title=title, df=self, **kwargs)\n",
    "    \n",
    "    def scatter2d_class(self, x1=None, x2=None, y=None, xlabel=None, ylabel=None, title=None, loc='upper right', noise=0, **kwargs):\n",
    "        self._df.evaluate().scatter2d_class(x1=x1, x2=x2, y=y, xlabel=xlabel, ylabel=ylabel, title=title, loc=loc, noise=noise, df=self, **kwargs)\n",
    "\n",
    "    def scatter2d_color(self, x1=None, x2=None, c=None, xlabel=None, ylabel=None, title=None, noise=0, **kwargs):\n",
    "        self._df.evaluate().scatter2d_color(x1=x1, x2=x2, c=c, xlabel=xlabel, ylabel=ylabel, title=title, noise=noise, df=self, **kwargs)\n",
    "\n",
    "    def scatter2d_size(self, x1=None, x2=None, s=None, xlabel=None, ylabel=None, title=None, noise=0, **kwargs):\n",
    "        self._df.evaluate().scatter2d_size(x1=x1, x2=x2, s=s, xlabel=xlabel, ylabel=ylabel, title=title, noise=noise, df=self, **kwargs)\n",
    "\n",
    "    def plot_boundary(self, predict):\n",
    "        self._df.evaluate().plot_boundary(predict)\n",
    "        \n",
    "    def plot_contour(self, predict):\n",
    "        self._df.evaluate().plot_contour(predict)\n",
    "\n",
    "class DSet(pd.DataFrame, _DSet):\n",
    "    _metadata = _DSet._metadata\n",
    "    _internal_names = _DSet._internal_names\n",
    "    _internal_names_set = _DSet._internal_names_set\n",
    "    \n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return DSet\n",
    "           \n",
    "    @classmethod\n",
    "    def from_dframe(cls, data, df, dfindices, transforms):\n",
    "        r = cls(data)\n",
    "        r._df = df\n",
    "        r._dfindices = dfindices\n",
    "        r._pt_categoryx = df._categoryx\n",
    "        r._pt_categoryy = df._categoryy\n",
    "        r._pt_dummiesx = df._dummiesx\n",
    "        r._pt_dummiesy = df._dummiesy\n",
    "        r._pt_columny = df._columny\n",
    "        r._pt_columnx = df._columnx\n",
    "        r._pt_dataset = df._pt_dataset\n",
    "        r._pt_transforms = transforms\n",
    "        r._pt_transposey = df._pt_transposey\n",
    "        r._pt_polynomials = df._pt_polynomials\n",
    "        r._pt_bias = df._pt_bias\n",
    "        r._pt_dtype = df._pt_dtype\n",
    "        r._pt_is_test = False\n",
    "        r._pt_sequence_window = df._pt_sequence_window\n",
    "        r._pt_sequence_shift_y = df._pt_sequence_shift_y\n",
    "        return r\n",
    "    \n",
    "    @classmethod\n",
    "    def df_to_testset(cls, data, df, dfindices, transforms):\n",
    "        r = cls.from_dframe(data, df, dfindices, transforms)\n",
    "        r._pt_is_test = True\n",
    "        return r\n",
    "    \n",
    "    def groupby(self, by, axis=0, level=None, as_index=True, sort=True, group_keys=True, observed=False, dropna=True):\n",
    "        r = super().groupby(by, axis=axis, level=level, as_index=as_index, sort=sort, group_keys=group_keys, observed=observed, dropna=dropna)\n",
    "        return self._copy_meta( GroupedDSet(r) )\n",
    "\n",
    "class NumpyDataset(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "class GroupedDSetSeries(SeriesGroupBy, _DSet):\n",
    "    _metadata = _DSet._metadata\n",
    "    #_internal_names = PTDS._internal_names\n",
    "    #_internal_names_set = PTDS._internal_names_set\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDSetSeries\n",
    "    \n",
    "    @property\n",
    "    def _constructor_expanddim(self):\n",
    "        return GroupedDFrame\n",
    "    \n",
    "class GroupedDSet(DataFrameGroupBy, _DSet):\n",
    "    _metadata = _DSet._metadata\n",
    "    #_internal_names = PTDS._internal_names\n",
    "    #_internal_names_set = PTDS._internal_names_set\n",
    "\n",
    "    def __init__(self, data=None):\n",
    "        super().__init__(obj=data.obj, keys=data.keys, axis=data.axis, level=data.level, grouper=data.grouper, exclusions=data.exclusions,\n",
    "                selection=data._selection, as_index=data.as_index, sort=data.sort, group_keys=data.group_keys,\n",
    "                observed=data.observed, mutated=data.mutated, dropna=data.dropna)\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return GroupedDSet\n",
    "    \n",
    "    @property\n",
    "    def _constructor_sliced(self):\n",
    "        return GroupedDSetSeries\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for group, subset in super().__iter__():\n",
    "            yield group, selfl._copy_meta(subset)\n",
    "            \n",
    "    \n",
    "    def astype(self, dtype, copy=True, errors='raise'):\n",
    "        DSet.astype(self, dtype, copy=copy, errors=errors)\n",
    "\n",
    "    def get_group(self, name, obj=None):\n",
    "        return self._dset( super().get_group(name, obj=obj) )\n",
    "        \n",
    "    def to_dataset(self):\n",
    "        from torch.utils.data import ConcatDataset\n",
    "        dss = []\n",
    "        for key, group in self:\n",
    "            dss.append( group.to_dataset())\n",
    "\n",
    "        return ConcatDataset(dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
