{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36456b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting study.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile study.py\n",
    "\n",
    "import optuna\n",
    "import joblib\n",
    "from optuna.storages._cached_storage import _CachedStorage\n",
    "from optuna.storages._heartbeat import is_heartbeat_enabled\n",
    "from optuna.trial._state import TrialState\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import RANSACRegressor, HuberRegressor, TheilSenRegressor, LinearRegression\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import inspect\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from joblib import parallel_backend\n",
    "import copy\n",
    "import random\n",
    "from optuna.exceptions import ExperimentalWarning\n",
    "import warnings\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning, module=\"optuna\")\n",
    "\n",
    "class Study(optuna.study.Study):\n",
    "    \"\"\"\n",
    "    Extension to an optuna Study. This extension caches the target functions and plot_hyperparameters\n",
    "    provides a good side-by-side overvew of the hyperparameters over the targets.\n",
    "    \n",
    "    For more information, check out Optuna Study.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, study, *target, grid=None, prune_none=True):\n",
    "        \"\"\"\n",
    "        Call create_study to instantiate a study\n",
    "        \"\"\"\n",
    "        super().__init__(study.study_name, study._storage, study.sampler, study.pruner)\n",
    "        assert len(target) > 0, 'You need to define at least one target'\n",
    "        for t in target:\n",
    "            assert type(t) == str, 'Only str names for targets are currently supported'\n",
    "        self.target = target\n",
    "        self.grid = grid\n",
    "        self._filter_sd = None\n",
    "        self._filter_upper = None\n",
    "        self._filter_lower = None\n",
    "        self._prune_none = prune_none\n",
    "        \n",
    "    @classmethod\n",
    "    def from_study(cls, study, *target, trials=None):\n",
    "        if len(target) == 0:\n",
    "            target = study.target\n",
    "        r = cls.create_study(*target, \n",
    "                             storage=study._storage, \n",
    "                             sampler=study.sampler,\n",
    "                             pruner=study.pruner)\n",
    "        trials = trials or study.trials\n",
    "        r.add_trials(trials)\n",
    "        return r\n",
    "        \n",
    "    @classmethod\n",
    "    def create_study(cls, *target, storage=None, \n",
    "                     sampler=None, multivariate=True, pruner=None, \n",
    "                     study_name=None, direction=None, \n",
    "                     directions=None, load_if_exists=False, grid=None):\n",
    "        \"\"\"\n",
    "        Uses optuna.create_study to create a Study. This extension registers the target metrics for inspection.\n",
    "        \n",
    "        Arguments:\n",
    "            *target: 'loss', str, callable\n",
    "                When called with no targets, this is set to 'loss'\n",
    "                When omitted the targets can be set by returning a dictionary of values\n",
    "                from the trial functions, and then the keys will be used.\n",
    "                When direction is omitted, a target 'loss' is set to minimize \n",
    "                and all other directions to maximize by default.\n",
    "            directions: list['minimize'|'maximize'] (None)\n",
    "                Behavior of directions is slightly different from Optuna. When None, and a\n",
    "                list of targets is provided, directions are set to minimize for 'loss' and\n",
    "                'maximize' for other metrics. \n",
    "            sampler: optuna Sampler (None)\n",
    "                By default the multivariate TPE sampler is used. You can override this by passing an\n",
    "                instantiated Optuna Sampler.\n",
    "            grid: dict (None)\n",
    "                If not None, a Grid Search is performed for all possible parameter combinations. This also\n",
    "                means that you do not have to specify n_trials in optimize, since it will only sample every\n",
    "                combination once.\n",
    "                e.g. grid={'lr':[1e-2, 1e-3], 'hidden':range(100, 1000, 100)}\n",
    "                You cannot combine this with a sampler (since this used GridSampler). \n",
    "            other arguments: check optuna\n",
    "        \"\"\"\n",
    "        if grid is not None:\n",
    "            assert type(grid) == dict, 'You have to pass a dict to grid'\n",
    "            assert sampler is None, 'You cannot use grid together with a custom sampler'\n",
    "            sampler = optuna.samplers.GridSampler(grid)\n",
    "        elif sampler is None:\n",
    "            sampler = optuna.samplers.TPESampler(multivariate=True)\n",
    "        if direction is None and directions is None:\n",
    "            if len(target) > 1:\n",
    "                directions = [ 'minimize' if t == 'loss' else 'maximize' for t in target ]\n",
    "            elif len(target) == 1:\n",
    "                direction = 'minimize' if target[0] == 'loss' else 'maximize'\n",
    "            else:\n",
    "                direction = 'minimize'\n",
    "        elif type(directions) == int:\n",
    "            directions = ['minimize'] + (directions - 1) * ['maximize']\n",
    "        study = optuna.create_study(storage=storage, sampler=sampler, pruner=pruner,\n",
    "                                    study_name=study_name, direction=direction, directions=directions, \n",
    "                                    load_if_exists=load_if_exists)\n",
    "        return cls(study, *target, grid=grid)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        return joblib.load(file)\n",
    "    \n",
    "    def save(self, file):\n",
    "        joblib.dump(self, file)\n",
    "\n",
    "    def __getitem__(self, slice):\n",
    "        return self.from_study(self, trials=self.trials[slice])\n",
    "    \n",
    "    def sample(self, n):\n",
    "        \"\"\"\n",
    "        Added for checking optimization robustness, by returning\n",
    "        a study with a random sample of performed trials.\n",
    "        \n",
    "        Args:\n",
    "            n: float or int\n",
    "                fraction or number of samples to be taken\n",
    "        \n",
    "        Returns: Study\n",
    "            with a sample of the performed trials.\n",
    "        \"\"\"\n",
    "        if type(n) == float:\n",
    "            n = int(round(len(self.trials) * n))\n",
    "        with self.quiet_mode():\n",
    "            r = self.from_study(self, trials=random.sample(self.trials, n))\n",
    "        return r\n",
    "    \n",
    "    def constraint(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Return a copy of the study that only contain the trails for which the\n",
    "        hyperparameters are within the given ranges. \n",
    "        \n",
    "        Example:\n",
    "            constraint(lr=[0.01, 0.1], hidden=[100, 400])\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: dict\n",
    "                In kwargs, pass the parameter names with a range [min, max]. \n",
    "                The min and max boundaries are inclusive. \n",
    "        \n",
    "        Returns: Study\n",
    "            with only the trials that meet the given constraints\n",
    "        \"\"\"\n",
    "        r = self.results\n",
    "        for p, (minx, maxx) in kwargs.items():\n",
    "            if p in set(r.parameter.unique()):\n",
    "                rp = r[(r.parameter == p) & (r.parametersetting >= minx) \n",
    "                                          & (r.parametersetting <= maxx)]\n",
    "                trials = set(rp.trial.unique())\n",
    "                r = r[r.trial.isin(trials)]\n",
    "            elif p in set(r.target.unique()):\n",
    "                rp = r[(r.target == p) & (r.targetvalue >= minx) \n",
    "                                       & (r.targetvalue <= maxx)]\n",
    "                trials = set(rp.trial.unique())\n",
    "                r = r[r.trial.isin(trials)]\n",
    "        trials = [ t for t in self.trials if t.number in trials ]\n",
    "        return Study.from_study(self, trials=trials)\n",
    "        \n",
    "    def quiet_mode(self, quiet=True):\n",
    "        \"\"\"\n",
    "        A ContextManager to silence optuna\n",
    "        \"\"\"\n",
    "        class CM(object):\n",
    "            def __enter__(self):\n",
    "                if quiet:\n",
    "                    self.old_verbosity = optuna.logging.get_verbosity()\n",
    "                    optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "            \n",
    "            def __exit__(self, type, value, traceback):\n",
    "                if quiet:\n",
    "                    optuna.logging.set_verbosity(self.old_verbosity)\n",
    "\n",
    "        return CM()\n",
    "    \n",
    "    def optimize(self, func, n_trials=None, timeout=None, catch=(), callbacks=None, \n",
    "                 gc_after_trial=True, show_progress_bar=False, n_jobs=1, loadout=()):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loadout: tuple ()\n",
    "                a tuple of objects that are passed to the trial function. The number\n",
    "                of positional arguments of the function must match the number of\n",
    "                values in the loadout.\n",
    "        \n",
    "        See Optuna's optimize, this extensions adds the possibility of a loadout\n",
    "        and automatically registers the target names that are returned by a trial\n",
    "        function.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.grid is not None and n_trials is None:\n",
    "            n_trials = np.prod( [ len(v) for v in self.grid.values() ])\n",
    "        args = len(inspect.getfullargspec(func)[0])\n",
    "        assert args == len(loadout) + 1, \"The number of arguments of your trial function has to match the number of objects that is passed as a loadout + 1\"\n",
    "        for l in loadout:\n",
    "            func = partial(func, l)\n",
    "        try:\n",
    "            del self._rules\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._results\n",
    "        except: pass\n",
    "        with self.quiet_mode(show_progress_bar):\n",
    "            super().optimize(func, n_trials=n_trials, \n",
    "                             timeout=timeout, catch=catch, \n",
    "                             callbacks=callbacks,\n",
    "                             gc_after_trial=gc_after_trial, \n",
    "                             show_progress_bar=show_progress_bar, n_jobs=n_jobs)        \n",
    "\n",
    "    def add_trial(self, trial):\n",
    "        if trial.state.name == 'COMPLETE':\n",
    "            if type(trial.params) == dict:\n",
    "                targets = list(trial.params.keys())\n",
    "                trial.params = list(trial.params.values())\n",
    "            if len(self.target) == 0:\n",
    "                try:\n",
    "                    self.target = targets\n",
    "                except: pass\n",
    "            super().add_trial(trial)\n",
    "        try:\n",
    "            del self._results\n",
    "        except: pass\n",
    "        try:\n",
    "            del self._rules\n",
    "        except: pass\n",
    "\n",
    "    @property\n",
    "    def trials(self):\n",
    "        return super().get_trials(deepcopy=True, states=[ TrialState.COMPLETE ] )\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.trials[0].params.keys()\n",
    "\n",
    "    @property\n",
    "    def results(self):\n",
    "        try:\n",
    "            return self._results\n",
    "        except:\n",
    "            table = []\n",
    "            for t in self.trials:\n",
    "                for param, paramv in t.params.items():\n",
    "                    for target, value in zip(self.target, t.values):\n",
    "                        table.append((t.number, param, paramv, target, value))\n",
    "            if len(table) > 0:\n",
    "                return pd.DataFrame(table, columns=['trial', 'parameter', 'parametersetting', \n",
    "                                                    'target', 'targetvalue'])\n",
    "            for t in self.trials:\n",
    "                for target, value in zip(self.target, t.values):\n",
    "                    table.append((t.number, target, value))\n",
    "            self._results = pd.DataFrame(table, columns=['trial', 'target', 'targetvalue'])\n",
    "            return self._results\n",
    "   \n",
    "    def get_target(self, target):\n",
    "        assert target in self.target, f'Target {target} is not in the studies targets'\n",
    "        i = self.target.index(target)\n",
    "        trials = []\n",
    "        for t in self.trials:\n",
    "            t = copy.copy(t)\n",
    "            t.values = [ t.values[i] ]\n",
    "            trials.append(t)\n",
    "        return Study.from_study(self, target, trials=trials)\n",
    "\n",
    "    def target_direction(self, target='loss'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target: str ('loss')\n",
    "                The target to return the direction for\n",
    "        \n",
    "        Return: bool\n",
    "            True if minimize, else False\n",
    "        \"\"\"\n",
    "        targeti = self.target.index(target)\n",
    "        return self.directions[targeti].value == 1\n",
    "\n",
    "    def selected_results(self, parameter, target):\n",
    "        return self.results[(self.results.parameter == parameter) & (self.results.target == target)]\n",
    "\n",
    "    def _filtered_target(self):\n",
    "        r = set(self.results.trial)\n",
    "        df = self.results[self.results.trial.isin(self.filtered_trials())]\n",
    "        if self._filter_sd is not None:\n",
    "            firstp = list(self.parameters)[0]\n",
    "            s = (df.parameter == firstp) \n",
    "            s = s & (df.target == self._filter_target)\n",
    "            t = df[s].targetvalue\n",
    "            mean = np.mean(t)\n",
    "            sd = np.std(t)\n",
    "            s = set(df[(df.targetvalue > mean - self._filter_sd * sd) & (df.targetvalue < mean + self._filter_sd * sd)].trial)\n",
    "            r = r.intersection(s)\n",
    "        if self._filter_lower is not None:\n",
    "            s = (df.targetvalue > self._filter_lower) \n",
    "            s = s & (df.target == self._filter_target)\n",
    "            s = set(df[s].trial)\n",
    "            r = r.intersection(s)\n",
    "        if self._filter_upper is not None:\n",
    "            s = (df.targetvalue < self._filter_upper) \n",
    "            s = s & (df.target == self._filter_target)\n",
    "            s = set(df[s].trial)\n",
    "            r = r.intersection(s)\n",
    "        return r\n",
    "    \n",
    "    def filter_target(self, sd=None, lower=None, upper=None, target=None):\n",
    "        \"\"\"\n",
    "        Filter the results so that points are excluded based on the following rules:\n",
    "        \n",
    "        Args:\n",
    "            sd: float (None)\n",
    "                when set, compute the mean and std for the target and exclude \n",
    "                all results outside \n",
    "                [ mean(target) - sd * std(target), mean(target) + sd * std(target)]\n",
    "                \n",
    "            lower: float (None)\n",
    "                when set, exclude points with a targetvalue lower that the given threshold\n",
    "                \n",
    "            higher: float (None)\n",
    "                when set, exclude points with a targetvalue higher that the given threshold\n",
    "        \"\"\"\n",
    "        if target is None:\n",
    "            target = self.target[1] if len(self.target) > 1 else self.target[0]\n",
    "        self._filter_sd = sd\n",
    "        self._filter_lower = lower\n",
    "        self._filter_upper = upper\n",
    "        self._filter_target = target\n",
    "    \n",
    "    def filter_upper(self, target=None):\n",
    "        if target is None:\n",
    "            target = self.target[1] if len(self.target) > 1 else self.target[0]\n",
    "        firstp = list(self.parameters)[0]\n",
    "        s = (self.results.parameter == firstp) \n",
    "        s = s & (self.results.target == target)\n",
    "        t = self.results[s].targetvalue\n",
    "        mean = np.mean(t)\n",
    "        self.filter_target(upper=mean, target=target)\n",
    "    \n",
    "    def filter_lower(self, target=None):\n",
    "        if target is None:\n",
    "            target = self.target[1] if len(self.target) > 1 else self.target[0]\n",
    "        firstp = list(self.parameters)[0]\n",
    "        s = (self.results.parameter == firstp) \n",
    "        s = s & (self.results.target == target)\n",
    "        t = self.results[s].targetvalue\n",
    "        mean = np.mean(t)\n",
    "        self.filter_target(lower=mean, target=target)\n",
    "    \n",
    "    @property\n",
    "    def rules(self):\n",
    "        try:\n",
    "            return self._rules\n",
    "        except:\n",
    "            if len(self.trials) > 0:\n",
    "                #t = self._filtered_target()\n",
    "                self._rules = pd.DataFrame(columns=['parameter', 'low', 'high'])\n",
    "                t = self.trials[0]\n",
    "                for parameter, dist in t.distributions.items():\n",
    "                    try:\n",
    "                        self._rules.loc[len(self._rules)] = (parameter, None, None)\n",
    "                    except: pass\n",
    "                return self._rules\n",
    "            \n",
    "    def filtered_trials(self):\n",
    "        df = self.results\n",
    "        s = set(df.trial)\n",
    "        for i, r in self.rules.iterrows():\n",
    "            f = (df.parameter == r.parameter)\n",
    "            if r.low is not None:\n",
    "                f = f & (df.parametersetting  > r.low)\n",
    "            if r.high is not None:\n",
    "                f = f & (df.parametersetting  < r.high)\n",
    "            s = s.intersection(set(df[f].trial))\n",
    "        return s\n",
    "    \n",
    "    def filtered_results(self):\n",
    "        df = self.results\n",
    "        df = df[df.trial.isin(self._filtered_target())]\n",
    "        df = df[df.trial.isin(self.filtered_trials())]  \n",
    "        return df\n",
    "\n",
    "    def pivotted_results(self, target=None):\n",
    "        from pipetorch.data import DFrame\n",
    "        target = target or self.default_target()\n",
    "        targetvalues = self.results[self.results.target == target]\n",
    "        targetvalues = targetvalues[['trial', 'targetvalue']].drop_duplicates().set_index('trial')\n",
    "        trials = self.results.pivot_table(index='trial', columns='parameter', values='parametersetting', aggfunc = np.mean)\n",
    "        data = trials.join(targetvalues)\n",
    "        return DFrame(data)\n",
    "    \n",
    "    def rule(self, parameter, low=None, high=None):\n",
    "        if low is not None:\n",
    "            self.rules.loc[self.rules.parameter == parameter, 'low'] = low\n",
    "        if high is not None:\n",
    "            self.rules.loc[self.rules.parameter == parameter, 'high'] = high   \n",
    "\n",
    "    def reset_rules(self):\n",
    "        try:\n",
    "            del self._rules\n",
    "        except: pass\n",
    "            \n",
    "    def distribution(self, param):\n",
    "        dist = self.trials[0].distributions[param]\n",
    "        return dist\n",
    "    \n",
    "    def is_log_distribution(self, param):\n",
    "        try:\n",
    "            return self.distribution(param).log\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def tune_r2_repeated(self, n=10, sample=0.8, **kwargs):\n",
    "        results = []\n",
    "        for i in range(n):\n",
    "            s = self.sample(sample)\n",
    "            r = s.tune_r2(**kwargs)\n",
    "            r = r.pivot_table(columns='parameter').reset_index(drop=True)\n",
    "            results.append(r)\n",
    "        return pd.concat(results)\n",
    "        \n",
    "    def sort2(self, a, b):\n",
    "        return (a, b) if a < b else (b, a)\n",
    "\n",
    "    def default_target(self):\n",
    "        if len(self.target) > 1 and self.target[0] == 'loss':\n",
    "            return self.target[1]\n",
    "        return self.target[0]\n",
    "\n",
    "    def tune_ransac(self, *parameters, degree=3, target=None, best=20, minimum=5, **kwargs):\n",
    "        results = self._tune_ransac(*parameters, degree=degree, target=target, best=best, minimum=minimum, **kwargs)\n",
    "        return self.tune(results=results, target=target, parameters=parameters, best=best, minimum=minimum)\n",
    "    \n",
    "    def _tune_ransac(self, *parameters, degree=3, target=None, best=20, minimum=5, **kwargs):\n",
    "        parameters = parameters if len(parameters) > 0 else self.parameters\n",
    "        target = target or self.default_target()\n",
    "        r = self.pivotted_results(target)\n",
    "        r = r.columnx(*parameters).columny('targetvalue')\n",
    "        r = r.polynomials(degree=degree)\n",
    "        model = RANSACRegressor(**kwargs)\n",
    "        for c in self.parameters:\n",
    "            if self.is_log_distribution(c):\n",
    "                r = r.log(c)\n",
    "        model.fit(r.train_X, r.train_y)\n",
    "        r['targetvalue'] = model.predict(r.train_X)\n",
    "        r = r.reset_index()\n",
    "        r = pd.melt(r, value_vars=self.parameters, id_vars=['targetvalue', 'trial'], \n",
    "                    var_name='parameter', value_name='parametersetting')\n",
    "        r['target'] = target\n",
    "        return pd.DataFrame(r)\n",
    " \n",
    "    def tunej(self, *parameters, degree=3, target=None, best=20, minimum=5, factor=5, **kwargs):\n",
    "        results = self._tunej(*parameters, degree=degree, target=target, best=best, minimum=minimum, factor=factor, **kwargs)\n",
    "        return self.tune(results=results, target=target, parameters=parameters, best=best, minimum=minimum)\n",
    "    \n",
    "    def _tunej(self, *parameters, degree=3, target=None, best=20, minimum=5, factor=5, **kwargs):\n",
    "        parameters = parameters if len(parameters) > 0 else self.parameters\n",
    "        target = target or self.default_target()\n",
    "        r = self.pivotted_results(target)\n",
    "        r = r.columnx(*parameters).columny('targetvalue')\n",
    "        r = r.polynomials(degree=degree)\n",
    "        for c in self.parameters:\n",
    "            if self.is_log_distribution(c):\n",
    "                r = r.log(c)\n",
    "        weights = None\n",
    "        for i in range(10):\n",
    "            model = LinearRegression()\n",
    "            model.fit(r.train_X, r.train_y, sample_weight=weights)\n",
    "            y_pred = model.predict(r.train_X)\n",
    "            mean_error = np.mean(r.train_y - y_pred)\n",
    "            std_error = np.std(r.train_y - y_pred) / factor\n",
    "            z_error = (abs(r.train_y - y_pred) - mean_error) / std_error\n",
    "            weights = 1 - st.norm.cdf(z_error)\n",
    "\n",
    "        r['targetvalue'] = model.predict(r.train_X)\n",
    "        r = r.reset_index()\n",
    "        r = pd.melt(r, value_vars=self.parameters, id_vars=['targetvalue', 'trial'], \n",
    "                    var_name='parameter', value_name='parametersetting')\n",
    "        r['target'] = target\n",
    "        return pd.DataFrame(r)\n",
    "\n",
    "    def tune(self, results=None, target=None, parameters=None, best=20, minimum=5):\n",
    "        \"\"\"\n",
    "        Finds the optimal hyperparameter settings by sampling intervals\n",
    "        between combinations of the #best trials.\n",
    "        \n",
    "        The best hyperparameter settings are found by sampling combinations of\n",
    "        relatively good trials. Every combination sets the boundaries for the\n",
    "        hyperparameters by the minimum and maximum values of these points, thus\n",
    "        selecting all trials that fall in between these boundaries. Combinations \n",
    "        that fail to include at least the set 'minimum' of points are dismissed.\n",
    "        \n",
    "        Arguments:\n",
    "            target: str (None)\n",
    "                the target (objective) that is used for the optimization. When None,\n",
    "                by default the first objective of the study after the loss is used. \n",
    "                If there is no first objective, then loss is used.\n",
    "            parameters: [ str ] (None)\n",
    "                the hyperparameters to be tuned. If None, all are tuned simultaneously.\n",
    "                This can be used to focus on the most sensitive parameters only.\n",
    "            best: int (10)\n",
    "                uses the #best trials to sample combinations from\n",
    "            minimum: int(5)\n",
    "                only combinations that include at least #minimum trials are considered.\n",
    "                when no combination satisfied the minimum constraint (which sometimes\n",
    "                happens), the combination with the highest number of trials are used.\n",
    "        \n",
    "        Returns: DataFrame\n",
    "            the columns in the DataFrame correspond to this studies' hyperparameters\n",
    "            and the row contains an estimation of the optimal values for each.\n",
    "        \"\"\"\n",
    "        target = target or self.default_target()\n",
    "        parameters = parameters or self.parameters\n",
    "        results = results if results is not None else self.results\n",
    "        assert all([ (p in self.parameters) for p in parameters ]), \\\n",
    "            \"all parameters should be parameters that are tuned in this study\"\n",
    "        assert target in self.target, 'target should be an objective in this study'\n",
    "        \n",
    "        direction = self.target_direction(target)\n",
    "        r = results[results.target == target]\n",
    "\n",
    "        paramsetting = {}\n",
    "        paramtrial = {}\n",
    "        paramindex = {}\n",
    "        for p in parameters:\n",
    "            rp = r[r.parameter == p].sort_values(by='parametersetting')\n",
    "            paramsetting[p] = rp.parametersetting.to_list()\n",
    "            paramtrial[p] = rp.trial.to_list()\n",
    "            paramindex[p] = { t:i for i, t in enumerate(paramtrial[p]) }\n",
    "        trial2targetvalue = { t.trial:t.targetvalue for t in rp.itertuples() }\n",
    "        pivots = rp.sort_values(by='targetvalue', ascending=(direction==1))\n",
    "        alltrials = set(rp.trial.unique())       \n",
    "        maxtargetvalue = None\n",
    "        maxtrials = None\n",
    "        maxminimum = 0\n",
    "        best = min(best, len(pivots))\n",
    "        while maxtrials is None:\n",
    "            for i in range(best-1):\n",
    "                for j in range(i+1, best):\n",
    "                    currenttrials = set(alltrials)\n",
    "                    triali = pivots.iloc[i].trial\n",
    "                    trialj = pivots.iloc[j].trial\n",
    "                    for p in parameters:\n",
    "                        mini, maxi = self.sort2(paramindex[p][triali], paramindex[p][trialj])\n",
    "                        currenttrials = currenttrials.intersection(set(paramtrial[p][mini:maxi+1]))\n",
    "                    if len(currenttrials) >= minimum:\n",
    "                        targetvalue = sum([ trial2targetvalue[t] for t in currenttrials ])/len(currenttrials)\n",
    "                        if maxtargetvalue is None or \\\n",
    "                           (direction == 1 and targetvalue < maxtargetvalue) or \\\n",
    "                           (direction == 0 and targetvalue > maxtargetvalue):\n",
    "                            maxtargetvalue = targetvalue\n",
    "                            maxtrials = currenttrials\n",
    "                    else:\n",
    "                        maxminimum = max(len(currenttrials), maxminimum)\n",
    "            if maxtrials is None:\n",
    "                minimum = maxminimum\n",
    "\n",
    "        r = r[r.trial.isin(maxtrials)]\n",
    "        trials = r.pivot_table(index='trial', columns='parameter', values='parametersetting')\n",
    "        result = self.median_parametersettings(trials)\n",
    "        for t in self.target:\n",
    "            r = results[(self.results.target == t)]\n",
    "            r = r[r.trial.isin(maxtrials)]\n",
    "            result[t] = r.targetvalue.mean()\n",
    "        return result\n",
    "\n",
    "    def tune_repeated(self, n=10, sample=0.8, **kwargs):\n",
    "        \"\"\"\n",
    "        Tunes the hyperparameters repeatedly to inspect the sensitivity of the\n",
    "        hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            n: int (10)\n",
    "                number of times to repeat the tuning\n",
    "            sample: float (0.8)\n",
    "                a new random sample of this fraction is used for each iteration\n",
    "            **kwargs: arguments passed to tune()\n",
    "            \n",
    "        Returns: DataFrame\n",
    "            The columns correspond to this studies hyperparameters and in the rows\n",
    "            are the results for the repeated tests\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i in tqdm(range(n)):\n",
    "            s = self.sample(sample)\n",
    "            r = s.tune(**kwargs)\n",
    "            results.append(r)\n",
    "        return pd.concat(results)\n",
    "    \n",
    "    def median_parametersettings(self, df):\n",
    "        \"\"\"\n",
    "        Correctly computes the parameter median from a DataFrame with\n",
    "        parameters settings, taking into account that some parameters\n",
    "        may be on a log scale.\n",
    "        \n",
    "        Args: DataFrame\n",
    "            The columns should correspond to this studies hyperparameters\n",
    "            and the rows are the values to be averaged.\n",
    "            \n",
    "        Returns: DataFrame\n",
    "            With the parameter medians.\n",
    "        \"\"\"\n",
    "        m = {}\n",
    "        for p in df.columns:\n",
    "            if p in self.parameters and self.is_log_distribution(p):\n",
    "                m[p] = np.log(np.median(np.exp(df[p])))\n",
    "            else:\n",
    "                m[p] = df[p].median()\n",
    "        return pd.DataFrame([m])\n",
    "\n",
    "    def tune_bootstrap(self, n=10, sample=0.8, **kwargs):\n",
    "        \"\"\"\n",
    "        Performs a bootstrap estimation of the hyperparameter tuning.\n",
    "        \n",
    "        Args:\n",
    "            n: int (10)\n",
    "                number of times to repeat the tuning\n",
    "            sample: float (0.8)\n",
    "                a new random sample of this fraction is used for each iteration\n",
    "            **kwargs: arguments passed to tune()\n",
    "            \n",
    "        Returns: DataFrame\n",
    "            the columns in the DataFrame correspond to this studies' hyperparameters\n",
    "            and the row contains an estimation of the optimal values for each.\n",
    "        \"\"\"\n",
    "\n",
    "        r = self.tune_repeated(n=n, sample=sample, **kwargs)\n",
    "        return self.median_parametersettings(r)\n",
    "        \n",
    "    def tune_r2(self, keep=0.3, target=None, min_r2=0, degree=2, alpha=1e-4):\n",
    "        target = target or self.default_target()\n",
    "        total = len(self.filtered_results())/len(self.parameters)/len(self.target)\n",
    "        if type(keep) == float:\n",
    "            keep = keep * total\n",
    "        pbar = tqdm(total=total - keep)\n",
    "        while total > keep:\n",
    "            opt = [ Optimum(self, p, target, degree=degree, alpha=alpha) \n",
    "                    for p in self.parameters ]\n",
    "            opt = [ (o, *o.predict_left_right()) for o in opt ]\n",
    "            left = sorted([ (l, o) for o, l, r in opt ])\n",
    "            right = sorted([ (r, o) for o, l, r in opt ])\n",
    "            old_total = total\n",
    "            if self.target_direction(target):\n",
    "                lr, l = left[-1]\n",
    "                rr, r = right[-1]\n",
    "                if lr > rr:\n",
    "                    self.rule(l.parameter, low=l.df.parametersetting.min())\n",
    "                else:\n",
    "                    self.rule(r.parameter, high=r.df.parametersetting.max())\n",
    "            else:\n",
    "                lr, l = left[0]\n",
    "                rr, r = right[0]\n",
    "                if rr > lr:\n",
    "                    self.rule(l.parameter, low=l.df.parametersetting.min())\n",
    "                else:\n",
    "                    self.rule(r.parameter, high=r.df.parametersetting.max())\n",
    "            total = len(self.filtered_results())/len(self.parameters)/len(self.target)\n",
    "            pbar.update(old_total - total)\n",
    "        pbar.close()\n",
    "        r = self.filtered_results()\n",
    "        r = r[r.target == target]\n",
    "        return r.groupby(by='parameter').parametersetting.mean().to_frame()\n",
    "\n",
    "    def plot_hyperparameter(self, parameter, target=None, ax=plt, fit=None, show='out', **modelparameters):\n",
    "        \"\"\"\n",
    "        Plot a scatter graph for a specific parameter and target\n",
    "        \n",
    "        Args:\n",
    "            parameter: str or int\n",
    "                specifies the parameter by name or position in the parameter list\n",
    "            target: str or int\n",
    "                specifies the target (objective function) by name or\n",
    "                position in the target list\n",
    "            ax: plt Axis (plt)\n",
    "                when specified this should be the axis object of a subfigure to\n",
    "                create side-by-side plots\n",
    "            fit: bool (None)\n",
    "                if True, a polynomial regression line will be fitted and shown on the data\n",
    "            degree: int (2)\n",
    "                the degree of the polynomial regression function to be fitted\n",
    "            alpha: float (1e-4)\n",
    "                the regularization parameter of the regression function to be fitted\n",
    "            show: str\n",
    "                'in' will show only the datapoints that are filtered in\n",
    "                'out' will show the datapoints that were filtered out as tiny dots\n",
    "                'all' will disregard the current filter and show all points\n",
    "        \"\"\"\n",
    "        self.scatter_hyperparameter(parameter, target, ax, fit, show, **modelparameters)\n",
    "    \n",
    "    def scatter_hyperparameter(self, parameter=0, target=None, ax=plt, fit=None, show='out', figsize=(3,3), **modelparameters):\n",
    "        \"\"\"\n",
    "        Plot a scatter graph for a specific parameter and target\n",
    "        \n",
    "        Args:\n",
    "            parameter: str or int (0)\n",
    "                specifies the parameter by name or position in the parameter list\n",
    "            target: str or int (None)\n",
    "                specifies the target (objective function) by name or\n",
    "                position in the target list, default is the first objective\n",
    "                besides the loss function.\n",
    "            ax: plt Axis (plt)\n",
    "                when specified this should be the axis object of a subfigure to\n",
    "                create side-by-side plots\n",
    "            fit: bool (None)\n",
    "                if True, a polynomial regression line will be fitted and shown on the data\n",
    "            degree: int (2)\n",
    "                the degree of the polynomial regression function to be fitted\n",
    "            alpha: float (1e-4)\n",
    "                the regularization parameter of the regression function to be fitted\n",
    "            show: str\n",
    "                'in' will show only the datapoints that are filtered in\n",
    "                'out' will show the datapoints that were filtered out as tiny dots\n",
    "                'all' will disregard the current filter and show all points\n",
    "        \"\"\"\n",
    "        if type(parameter) == int:\n",
    "            parameter = self.parameters[parami]\n",
    "        if ax == plt:\n",
    "            plt.figure(figsize=figsize)\n",
    "        if target is None:\n",
    "            target = self.default_target()\n",
    "        elif type(target) == int:\n",
    "            target = self.target[targeti]\n",
    "        if fit:\n",
    "            if fit=='RANSAC':\n",
    "                o = OptimumRANSAC(self, parameter, target, **modelparameters)\n",
    "            elif fit=='Huber':\n",
    "                o = OptimumHuber(self, parameter, target, **modelparameters)\n",
    "            elif fit=='TheilSen':\n",
    "                o = OptimumTheilSen(self, parameter, target, **modelparameters)\n",
    "            elif fit=='downweight':\n",
    "                o = OptimumDownweight(self, parameter, target, **modelparameters)\n",
    "            else:\n",
    "                o = Optimum(self, parameter, target, **modelparameters)\n",
    "            #if o.r2 > 0:\n",
    "            o.plot(ax)\n",
    "        ylim = plt.ylim if ax == plt else ax.set_ylim\n",
    "        title = plt.title if ax == plt else ax.set_title\n",
    "        xscale = plt.xscale if ax == plt else ax.set_xscale\n",
    "        ylabel = plt.ylabel if ax == plt else ax.set_ylabel\n",
    "        xlabel = plt.xlabel if ax == plt else ax.set_xlabel\n",
    "        \n",
    "        subset = self.selected_results(parameter, target)\n",
    "        if show == 'in' or show == 'out':\n",
    "            trials = self.filtered_trials().intersection(self._filtered_target())\n",
    "            subset_in = subset[subset.trial.isin(trials)]\n",
    "            self._scatter(ax, subset_in)\n",
    "            if show == 'out':\n",
    "                self._scatter_hidden(ax, subset[~subset.trial.isin(trials)])\n",
    "            ylim(subset_in.targetvalue.min(), subset_in.targetvalue.max())\n",
    "        else:\n",
    "            self._scatter(ax, subset)\n",
    "            ylim(subset.targetvalue.min(), subset.targetvalue.max())\n",
    "            \n",
    "        title(parameter)\n",
    "        if self.is_log_distribution(parameter):\n",
    "            xscale('log')\n",
    "        ylabel(target)\n",
    "    \n",
    "    def scatter_hyperparameters(self, figsize=None, logscale=['loss'], fit=True, show='out', **modelparameters):\n",
    "        \"\"\"\n",
    "        Plots scatter graps of each hyperparameter over each recorded metric to \n",
    "        allow manual optimization and provide insight to the sensitivity.\n",
    "        \n",
    "        Arguments:\n",
    "            figsize: (width, height) None\n",
    "                controls the size of the figure displayed\n",
    "            logscale: ['loss']\n",
    "                list of metrics whose y-axis is shown as a log scale. By default this is done for the loss\n",
    "                because the learning rate is often sampled from a log distribution and this makes it easier\n",
    "                to estimate the optimum.\n",
    "            fit, degree, alpha, show: see scatter_hyperparameter\n",
    "        \"\"\"\n",
    "        results = self.results\n",
    "        parameters = self.parameters\n",
    "        if figsize is None:\n",
    "            figsize = (4 * len(parameters), 4 * len(self.target))\n",
    "        \n",
    "        fig, axs = plt.subplots(len(self.target), len(parameters), sharex='col', sharey='row', figsize=figsize)\n",
    "        \n",
    "        if len(parameters) == 1:\n",
    "            if len(self.target) == 1:\n",
    "                axs = np.array([[axs]])\n",
    "            else:\n",
    "                axs = np.expand_dims(axs, axis=1)\n",
    "        elif len(self.target) == 1:\n",
    "            axs = np.expand_dims(axs, axis=0)\n",
    "        for parami, param in enumerate(parameters):\n",
    "            for targeti, target in enumerate(self.target):\n",
    "                ax = axs[targeti, parami]\n",
    "                self.scatter_hyperparameter(param, target, ax=ax, fit=fit, show=show, **modelparameters)\n",
    "                if target in logscale:\n",
    "                    ax.set_yscale('log')\n",
    "\n",
    "    def plot_hyperparameters(self, figsize=None, logscale=['loss'], fit=False, **modelparameters):\n",
    "        \"\"\"\n",
    "        Plots scatter graps of each hyperparameter over each recorded metric to \n",
    "        allow manual optimization and provide insight to the sensitivity.\n",
    "        \n",
    "        Arguments:\n",
    "            figsize: (width, height) None\n",
    "                controls the size of the figure displayed\n",
    "            logscale: ['loss']\n",
    "                list of metrics whose y-axis is shown as a log scale. By default this is done for the loss\n",
    "                because the learning rate is often sampled from a log distribution and this makes it easier\n",
    "                to estimate the optimum.\n",
    "            fit and **kwargs: see scatter_hyperparameter.\n",
    "        \"\"\"\n",
    "        self.scatter_hyperparameters(figsize=figsize, logscale=logscale, fit=fit, **modelparameters)\n",
    "                    \n",
    "    def plot_targets(self, *targets, parameter=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Plots the target results in a single figure\n",
    "        \n",
    "        Arguments:\n",
    "            *targets: str (None)\n",
    "                the targets to plot\n",
    "            parameter: str (None)\n",
    "                the parameter to plot, or None to plot all data which is fine when there is only one parameter\n",
    "            **kwargs: dict\n",
    "                arguments for Pandas DataFrame.plot\n",
    "                \n",
    "        Returns: matplotlib.axes.Axes\n",
    "            of the plotted figure, which can be used to extend or modify the figure\n",
    "        \"\"\"\n",
    "        r = self.results\n",
    "        if parameter is not None:\n",
    "            r = r[r.parameter == parameter]\n",
    "        targets = targets if len(targets) > 0 else self.target\n",
    "        \n",
    "        \n",
    "        curves = [ r[r.target==t].sort_values(by='parametersetting') for t in targets ]\n",
    "        for t, c in zip(self.target, curves):\n",
    "            try:\n",
    "                c.plot(x='parametersetting', y='targetvalue', label=t, ax=ax)\n",
    "            except:\n",
    "                ax = c.plot(x='parametersetting', y='targetvalue', label=t, **kwargs)\n",
    "        plt.legend()\n",
    "        return ax\n",
    "                    \n",
    "    def trial_targets(self):\n",
    "        \"\"\"\n",
    "        lists to metrics over the trials.\n",
    "        \"\"\"\n",
    "        l = defaultdict(list)\n",
    "        for t in self.trials:\n",
    "            for target, value in zip(self.target, t.values):\n",
    "                l[target].append(value)\n",
    "        return pd.DataFrame.from_dict(l)        \n",
    "       \n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Reports the mean and variance for each metric over the trials, providing more stable outcomes using\n",
    "        n-fold cross validation.\n",
    "        \"\"\"\n",
    "        l = defaultdict(list)\n",
    "        for t in self.trials:\n",
    "            for target, value in zip(self.target, t.values):\n",
    "                l[target].append(value)\n",
    "        mean = []\n",
    "        std = []\n",
    "        for target, values in l.items():\n",
    "            mean.append(np.mean(values))\n",
    "            std.append(np.std(values))\n",
    "        return pd.DataFrame({'target':self.target, 'mean':mean, 'std':std})        \n",
    "        \n",
    "    def _scatter(self, ax, subset):\n",
    "        x = subset.parametersetting.astype(np.float64)\n",
    "        y = subset.targetvalue.astype(np.float64)\n",
    "        z = subset.trial\n",
    "        ax.scatter(x, y, c=z, cmap='plasma')\n",
    "        \n",
    "    def _scatter_hidden(self, ax, subset):\n",
    "        x = subset.parametersetting.astype(np.float64)\n",
    "        y = subset.targetvalue.astype(np.float64)\n",
    "        ax.scatter(x, y, s=1)\n",
    "\n",
    "class Ridge:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        leftmat = np.linalg.pinv(X.T @ X + self.alpha * np.identity(X.shape[1]))\n",
    "        self.betas = leftmat @ X.T @ y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.betas\n",
    "    \n",
    "    def optima(self):\n",
    "        if len(self.betas) == 3:\n",
    "            _, b, a = [ i * b for i, b in enumerate(self.betas) ]\n",
    "            return [ -b / (2 * a) ]\n",
    "        elif len(self.betas) == 4:\n",
    "            _, c, b, a = [ i * b for i, b in enumerate(self.betas) ]\n",
    "            if a != 0:\n",
    "                D = b * b - 4 * a * c\n",
    "                if D == 0:\n",
    "                    return [ -b / (2 * a) ]\n",
    "                elif D > 0:\n",
    "                    return [ (-b - math.sqrt(D)) / (2 * a), (-b + math.sqrt(D)) / (2 * a) ]\n",
    "            else:\n",
    "                return [ -c / (2 * b) ]\n",
    "        return []\n",
    "    \n",
    "class Optimum:\n",
    "    def __init__(self, study, parameter, target, fold=None, degree=2, alpha=1e-3):\n",
    "        self.study = study\n",
    "        self.parameter = parameter\n",
    "        self.target = target\n",
    "        self.degree = degree\n",
    "        self.alpha = alpha\n",
    "        self.fold = fold\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.parameter} {self.target} {self.r2}'\n",
    "        \n",
    "    @classmethod\n",
    "    def loo(cls, study, parameter, target, **kwargs):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        base = cls(study, parameter, target, **kwargs)\n",
    "        for i in range(1, len(base.df)-1):\n",
    "            print(parameter, i)\n",
    "            s = cls(study, parameter, target, fold=i, **kwargs)\n",
    "            y_pred.append(s.pred_y.item())\n",
    "            y_true.append(s.valid_y.item())\n",
    "        return r2_score(y_true, y_pred)\n",
    "        \n",
    "    @property\n",
    "    def is_log_distribution(self):\n",
    "        return self.study.is_log_distribution(self.parameter)\n",
    "    \n",
    "    @property\n",
    "    def df(self):\n",
    "        try:\n",
    "            return self._df\n",
    "        except:\n",
    "            self._df = copy.copy(self.study.filtered_results())\n",
    "            self._df = self._df[(self._df.parameter == self.parameter) & (self._df.target == self.target)]\n",
    "            self._df = self._df[['parametersetting', 'targetvalue']]\n",
    "            self._df = self._df.sort_values(by='parametersetting').reset_index(drop=True)\n",
    "            return self._df\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.fold:\n",
    "            return self.df.drop(self.fold)\n",
    "        return self.df\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        if self.fold:\n",
    "            return self.df.iloc[self.fold:self.fold+1]\n",
    "        return self.df\n",
    "    \n",
    "    def transform_X(self, X):\n",
    "        if self.is_log_distribution:\n",
    "            X = np.log( X )\n",
    "        p = PolynomialFeatures(degree=self.degree, include_bias=True)\n",
    "        return p.fit_transform(X)\n",
    "    \n",
    "    @property\n",
    "    def train_X(self):\n",
    "        return self.transform_X(self.train.loc[:, ['parametersetting']])\n",
    "    \n",
    "    @property\n",
    "    def valid_X(self):\n",
    "        return self.transform_X(self.valid.loc[:, ['parametersetting']])\n",
    "    \n",
    "    @property\n",
    "    def train_y(self):\n",
    "        return self.train['targetvalue']\n",
    "    \n",
    "    @property\n",
    "    def valid_y(self):\n",
    "        return self.valid['targetvalue']\n",
    "    \n",
    "    @property\n",
    "    def minx(self):\n",
    "        try:\n",
    "            return self._minx\n",
    "        except:\n",
    "            self._minx = self.df.parametersetting.min()\n",
    "            return self._minx\n",
    "\n",
    "    @property\n",
    "    def maxx(self):\n",
    "        try:\n",
    "            return self._maxx\n",
    "        except:\n",
    "            self._maxx = self.df.parametersetting.max()\n",
    "            return self._maxx\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return self._model\n",
    "        except:\n",
    "            self._model = Ridge(alpha=self.alpha)\n",
    "            self._model.fit(self.train_X, self.train_y)\n",
    "            return self._model\n",
    "        \n",
    "    @property\n",
    "    def pred_y(self):\n",
    "        return self.model.predict(self.valid_X)\n",
    "\n",
    "    def plot(self, ax):\n",
    "        if self.is_log_distribution:\n",
    "            X = np.logspace(np.log(self.minx)/np.log(10), np.log(self.maxx)/np.log(10))\n",
    "        else:\n",
    "            X = np.linspace(self.minx, self.maxx)\n",
    "        pred_y = self.model.predict(self.transform_X(X.reshape(-1,1)))\n",
    "        ax.plot(X, pred_y)\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self.study.target_direction(self.target)\n",
    "        \n",
    "    def predict_left_right(self):\n",
    "        x = np.array([self.minx, self.maxx])\n",
    "        y = self.model.predict(self.transform_X(x.reshape(-1,1)))\n",
    "        y = y.reshape(-1)\n",
    "        return y\n",
    "        \n",
    "    def optima(self):\n",
    "        try:\n",
    "            return self._optima\n",
    "        except:\n",
    "            inrange = lambda x: x >= self.minx and x <= self.maxx\n",
    "            x = np.array([self.minx, self.maxx, \n",
    "                          *[ x for x in self.model.optima() if inrange(x) ]])\n",
    "            y = self.model.predict(self.transform_X(x.reshape(-1,1)))\n",
    "            y = y.reshape(-1)\n",
    "            if self.direction == 1:\n",
    "                i = np.argmin(y)\n",
    "                self._optima = x[i], y[i]\n",
    "            else:\n",
    "                i = np.argmax(y)\n",
    "                self._optima = x[i], y[i]\n",
    "            return self._optima\n",
    "\n",
    "    def least_optimal(self):\n",
    "        try:\n",
    "            return self._least_optimal\n",
    "        except:\n",
    "            inrange = lambda x: x >= self.minx and x <= self.maxx\n",
    "            x = np.array([self.minx, self.maxx, \n",
    "                          *[ x for x in self.model.optima() if inrange(x) ]])\n",
    "            y = self.model.predict(self.transform_X(x.reshape(-1,1)))\n",
    "            y = y.reshape(-1)\n",
    "            if self.direction == 1:\n",
    "                i = np.argmax(y)\n",
    "                self._least_optimal = x[i], y[i]\n",
    "            else:\n",
    "                i = np.argmin(y)\n",
    "                self._least_optimal = x[i], y[i]\n",
    "            return self._least_optimal\n",
    "        \n",
    "    def opt(self):\n",
    "        a = 3 * self.model.betas[3]\n",
    "        b = 2 * self.model.betas[2]\n",
    "        c = self.model.betas[1]\n",
    "        D = b * b - 4 * a * c\n",
    "        if D == 0:\n",
    "            if c < 0 and self.direction == 1:\n",
    "                return [ -b / (2 * a) ]\n",
    "            if c > 0 and self.direction == 0:\n",
    "                return [ -b / (2 * a) ]\n",
    "        else:\n",
    "            x = [ (-b - math.sqrt(D))/ (2 * a), (-b + math.sqrt(D))/ (2 * a) ]\n",
    "            y = self.model.predict(self.transform_X(x))\n",
    "            if self.direction == 1:\n",
    "                if y[0] < y[1]:\n",
    "                    return x[0]\n",
    "                return x[1]\n",
    "            else:\n",
    "                if y[1] < y[0]:\n",
    "                    return x[1]\n",
    "                return x[0]\n",
    "\n",
    "    @property\n",
    "    def r2(self):\n",
    "        return r2_score(self.valid_y, self.pred_y)\n",
    "\n",
    "class OptimumRANSAC(Optimum):\n",
    "    def __init__(self, study, parameter, target, fold=None, degree=2, min_samples=None,\n",
    "                residual_threshold=None):\n",
    "        super().__init__(study, parameter, target, fold=fold, degree=degree)\n",
    "        self.min_samples = min_samples\n",
    "        self.residual_threshold = residual_threshold\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return self._model\n",
    "        except:\n",
    "            self._model = RANSACRegressor(min_samples=self.min_samples, residual_threshold=self.residual_threshold)\n",
    "            self._model.fit(self.train_X, self.train_y)\n",
    "            return self._model\n",
    "        \n",
    "class OptimumHuber(Optimum):\n",
    "    def __init__(self, study, parameter, target, fold=None, degree=2, epsilon=1.35,\n",
    "                alpha=1e-4, tol=1e-5):\n",
    "        super().__init__(study, parameter, target, fold=fold, degree=degree)\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return self._model\n",
    "        except:\n",
    "            self._model = HuberRegressor(epsilon=self.epsilon, alpha=self.alpha, tol=self.tol)\n",
    "            self._model.fit(self.train_X, self.train_y)\n",
    "            return self._model\n",
    "        \n",
    "class OptimumTheilSen(Optimum):\n",
    "    def __init__(self, study, parameter, target, fold=None, degree=2):\n",
    "        super().__init__(study, parameter, target, fold=fold, degree=degree)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return self._model\n",
    "        except:\n",
    "            self._model = TheilSenRegressor()\n",
    "            self._model.fit(self.train_X, self.train_y)\n",
    "            return self._model\n",
    "\n",
    "class OptimumDownweight(Optimum):\n",
    "    def __init__(self, study, parameter, target, fold=None, degree=2, factor=1.0, window=5):\n",
    "        super().__init__(study, parameter, target, fold=fold, degree=degree)\n",
    "        self.factor = factor\n",
    "        self.window = window\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        try:\n",
    "            return self._model\n",
    "        except:\n",
    "            weights = np.zeros(len(self.df))\n",
    "            for i in range(len(self.df)):\n",
    "                mini = max(0, i - self.window // 2)\n",
    "                maxi = min(len(self.df), i + 1 + self.window // 2)\n",
    "                mean = np.mean(self.df.iloc[mini:maxi].targetvalue)\n",
    "                std = np.std(self.df.iloc[mini:maxi].targetvalue)\n",
    "                if self.direction:\n",
    "                    y = np.clip(mean - self.df.iloc[i].targetvalue, None, 0) / std\n",
    "                else:\n",
    "                    y = np.clip(self.df.iloc[i].targetvalue - mean, None, 0) / std\n",
    "                weights[i] = st.norm.cdf(y * self.factor)\n",
    "            #print(np.concatenate([weights.reshape(-1,1, self.df.targetvalue.values], axis=1))\n",
    "            self._model = LinearRegression()\n",
    "            self._model.fit(self.train_X, self.train_y, sample_weight=weights)\n",
    "            return self._model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08371e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
